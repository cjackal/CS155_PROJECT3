{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 40, 5)             16030     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 200)               164800    \n",
      "_________________________________________________________________\n",
      "lambda_22 (Lambda)           (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3206)              644406    \n",
      "=================================================================\n",
      "Total params: 825,236\n",
      "Trainable params: 825,236\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 4s 274us/step\n",
      "[0.08522063263796159, 0.9887309670448303]\n",
      "perplexity:  1.088957299717466\n",
      "accuracy:  0.9887309670448303\n",
      "unit  200 embedding dimension 5\n",
      "1.088957299717466\n",
      "0.9887309670448303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 40, 5)             16030     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 300)               367200    \n",
      "_________________________________________________________________\n",
      "lambda_23 (Lambda)           (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3206)              965006    \n",
      "=================================================================\n",
      "Total params: 1,348,236\n",
      "Trainable params: 1,348,236\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 9s 640us/step\n",
      "[0.13030574954357477, 0.9753995537757874]\n",
      "perplexity:  1.1391766328188824\n",
      "accuracy:  0.9753995537757874\n",
      "unit  300 embedding dimension 5\n",
      "1.1391766328188824\n",
      "0.9753995537757874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_44\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 40, 5)             16030     \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 400)               649600    \n",
      "_________________________________________________________________\n",
      "lambda_24 (Lambda)           (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 1,951,236\n",
      "Trainable params: 1,951,236\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 17s 1ms/step\n",
      "[0.006680879098548909, 1.0]\n",
      "perplexity:  1.006703245953655\n",
      "accuracy:  1.0\n",
      "unit  400 embedding dimension 5\n",
      "1.006703245953655\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_46\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 10)            32060     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               168800    \n",
      "_________________________________________________________________\n",
      "lambda_25 (Lambda)           (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3206)              644406    \n",
      "=================================================================\n",
      "Total params: 845,266\n",
      "Trainable params: 845,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 9s 651us/step\n",
      "[0.030180931709149237, 0.99985271692276]\n",
      "perplexity:  1.0306409927211888\n",
      "accuracy:  0.99985271692276\n",
      "unit  200 embedding dimension 10\n",
      "1.0306409927211888\n",
      "0.99985271692276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_48\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 40, 10)            32060     \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 300)               373200    \n",
      "_________________________________________________________________\n",
      "lambda_26 (Lambda)           (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3206)              965006    \n",
      "=================================================================\n",
      "Total params: 1,370,266\n",
      "Trainable params: 1,370,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 19s 1ms/step\n",
      "[0.006670516200853134, 1.0]\n",
      "perplexity:  1.0066928136449618\n",
      "accuracy:  1.0\n",
      "unit  300 embedding dimension 10\n",
      "1.0066928136449618\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_50\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 40, 10)            32060     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 400)               657600    \n",
      "_________________________________________________________________\n",
      "lambda_27 (Lambda)           (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 1,975,266\n",
      "Trainable params: 1,975,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 35s 3ms/step\n",
      "[4.1456098875768, 0.19901303946971893]\n",
      "perplexity:  63.156128296312325\n",
      "accuracy:  0.19901303946971893\n",
      "unit  400 embedding dimension 10\n",
      "63.156128296312325\n",
      "0.19901303946971893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_52\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 25)            80150     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               180800    \n",
      "_________________________________________________________________\n",
      "lambda_28 (Lambda)           (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3206)              644406    \n",
      "=================================================================\n",
      "Total params: 905,356\n",
      "Trainable params: 905,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 12s 904us/step\n",
      "[0.00485831474412172, 1.0]\n",
      "perplexity:  1.0048701354904142\n",
      "accuracy:  1.0\n",
      "unit  200 embedding dimension 25\n",
      "1.0048701354904142\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_54\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 40, 25)            80150     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 300)               391200    \n",
      "_________________________________________________________________\n",
      "lambda_29 (Lambda)           (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3206)              965006    \n",
      "=================================================================\n",
      "Total params: 1,436,356\n",
      "Trainable params: 1,436,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 25s 2ms/step\n",
      "[0.005628054120854381, 1.0]\n",
      "perplexity:  1.0056439213707289\n",
      "accuracy:  1.0\n",
      "unit  300 embedding dimension 25\n",
      "1.0056439213707289\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_56\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 40, 25)            80150     \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 400)               681600    \n",
      "_________________________________________________________________\n",
      "lambda_30 (Lambda)           (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 2,047,356\n",
      "Trainable params: 2,047,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 35s 3ms/step\n",
      "[0.005212324941954059, 1.0]\n",
      "perplexity:  1.0052259327400865\n",
      "accuracy:  1.0\n",
      "unit  400 embedding dimension 25\n",
      "1.0052259327400865\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_58\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 50)            160300    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               200800    \n",
      "_________________________________________________________________\n",
      "lambda_31 (Lambda)           (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3206)              644406    \n",
      "=================================================================\n",
      "Total params: 1,005,506\n",
      "Trainable params: 1,005,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 13s 926us/step\n",
      "[0.0040191000219275495, 1.0]\n",
      "perplexity:  1.004027187435499\n",
      "accuracy:  1.0\n",
      "unit  200 embedding dimension 50\n",
      "1.004027187435499\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_60\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 40, 50)            160300    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 300)               421200    \n",
      "_________________________________________________________________\n",
      "lambda_32 (Lambda)           (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3206)              965006    \n",
      "=================================================================\n",
      "Total params: 1,546,506\n",
      "Trainable params: 1,546,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 25s 2ms/step\n",
      "[0.0057392145023507195, 1.0]\n",
      "perplexity:  1.0057557153460932\n",
      "accuracy:  1.0\n",
      "unit  300 embedding dimension 50\n",
      "1.0057557153460932\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_62\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 40, 50)            160300    \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 400)               721600    \n",
      "_________________________________________________________________\n",
      "lambda_33 (Lambda)           (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 2,167,506\n",
      "Trainable params: 2,167,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 35s 3ms/step\n",
      "[0.0030533260945841225, 1.0]\n",
      "perplexity:  1.003057992242586\n",
      "accuracy:  1.0\n",
      "unit  400 embedding dimension 50\n",
      "1.003057992242586\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_64\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 100)           320600    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "lambda_34 (Lambda)           (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3206)              644406    \n",
      "=================================================================\n",
      "Total params: 1,205,806\n",
      "Trainable params: 1,205,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 13s 981us/step\n",
      "[0.0011050433296964366, 1.0]\n",
      "perplexity:  1.0011056541150374\n",
      "accuracy:  1.0\n",
      "unit  200 embedding dimension 100\n",
      "1.0011056541150374\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_66\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 100)           320600    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 300)               481200    \n",
      "_________________________________________________________________\n",
      "lambda_35 (Lambda)           (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3206)              965006    \n",
      "=================================================================\n",
      "Total params: 1,766,806\n",
      "Trainable params: 1,766,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 25s 2ms/step\n",
      "[0.0030762814959062407, 1.0]\n",
      "perplexity:  1.0030810181056302\n",
      "accuracy:  1.0\n",
      "unit  300 embedding dimension 100\n",
      "1.0030810181056302\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_68\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 40, 100)           320600    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 400)               801600    \n",
      "_________________________________________________________________\n",
      "lambda_36 (Lambda)           (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 2,407,806\n",
      "Trainable params: 2,407,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 35s 3ms/step\n",
      "[0.002920005690586158, 1.0]\n",
      "perplexity:  1.0029242730597725\n",
      "accuracy:  1.0\n",
      "unit  400 embedding dimension 100\n",
      "1.0029242730597725\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_70\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 200)           641200    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "lambda_37 (Lambda)           (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3206)              644406    \n",
      "=================================================================\n",
      "Total params: 1,606,406\n",
      "Trainable params: 1,606,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 13s 988us/step\n",
      "[0.001998497339620212, 1.0]\n",
      "perplexity:  1.0020004956664237\n",
      "accuracy:  1.0\n",
      "unit  200 embedding dimension 200\n",
      "1.0020004956664237\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_72\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 200)           641200    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 300)               601200    \n",
      "_________________________________________________________________\n",
      "lambda_38 (Lambda)           (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3206)              965006    \n",
      "=================================================================\n",
      "Total params: 2,207,406\n",
      "Trainable params: 2,207,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 31s 2ms/step\n",
      "[0.0016207307741556852, 1.0]\n",
      "perplexity:  1.0016220448681117\n",
      "accuracy:  1.0\n",
      "unit  300 embedding dimension 200\n",
      "1.0016220448681117\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_74\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 40, 200)           641200    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "lambda_39 (Lambda)           (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 2,888,406\n",
      "Trainable params: 2,888,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 41s 3ms/step\n",
      "[0.0013842865990280626, 1.0]\n",
      "perplexity:  1.0013852451659817\n",
      "accuracy:  1.0\n",
      "unit  400 embedding dimension 200\n",
      "1.0013852451659817\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "## Look into perplexity\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "useWordEmbedding = True\n",
    "embedSize = [5, 10, 25, 50, 100, 200]\n",
    "numUnit = [200, 300, 400]\n",
    "data = 'shakespeare'\n",
    "\n",
    "if data == 'shakespeare':\n",
    "    fileName_data = ''\n",
    "elif data == 'spenser':\n",
    "    fileName_data = 'Spenser_'\n",
    "elif data == 'both':\n",
    "    fileName_data = 'Both_'\n",
    "perplexity = np.zeros((len(numUnit), len(embedSize)))\n",
    "accuracy = np.zeros((len(numUnit), len(embedSize)))\n",
    "\n",
    "for i, dim in enumerate(embedSize):\n",
    "    for j, un in enumerate(numUnit):\n",
    "        if useWordEmbedding:\n",
    "            modelName = \"model_%sunit%d_withWordEmbedding %d.h5\" % (fileName_data, un, dim)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWordEmbedding %d.pk1\" % (fileName_data, un, dim)\n",
    "        test2 = LSTM_word()\n",
    "        if data == 'shakespeare':\n",
    "            test2.SonnetLoader('shakespeare')\n",
    "        elif data == 'spenser':\n",
    "            test2.SonnetLoader('Spenser_v2')\n",
    "        elif data == 'both':\n",
    "            test2.SonnetLoader(['shakespeare', 'Spenser_v2'])\n",
    "        \n",
    "        test2.getTrainSeq()\n",
    "        test2.getMapping()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "        [perplexity[j, i], accuracy[j, i]] = test2.perplexity_train(useWordEmbedding=True)\n",
    "        print(\"unit \", un, \"embedding dimension\", dim)\n",
    "        print(perplexity[j, i])\n",
    "        print(accuracy[j, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.0889573   1.03064099  1.00487014  1.00402719  1.00110565  1.0020005 ]\n",
      " [ 1.13917663  1.00669281  1.00564392  1.00575572  1.00308102  1.00162204]\n",
      " [ 1.00670325 63.1561283   1.00522593  1.00305799  1.00292427  1.00138525]]\n"
     ]
    }
   ],
   "source": [
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature:  1.5 , embedding size:  100 \n",
      " shall i compare thee to a summer's day \n",
      " for my heart thou i that i am it see \n",
      " by self-example mayst have be or mine eye \n",
      " no more i never am but for my name \n",
      " let that and the world may stain you live in love \n",
      " they by his beauty being false but die \n",
      " but mutual render only me there \n",
      " for beauty's pattern to succeeding men \n",
      " yet do i spur though mounted on the wind \n",
      " in winged speed no motion shall i know \n",
      " then can no horse with my desire keep pace \n",
      " therefore desire of perfect'st love being made \n",
      " shall neigh no dull flesh in his fiery race \n",
      " but love for love thus shall excuse my jade \n",
      " since from thee going he went wilful-slow \n",
      " towards thee i'll run and give him leave to go \n",
      " in me thou seest the glowing of such fire \n",
      "\n",
      "temperature:  1 , embedding size:  100 \n",
      " shall i compare thee to a summer's day \n",
      " for my heart thou i that i am it see \n",
      " by self-example mayst have be or mine eye \n",
      " no more i never i may true her pleasure i part \n",
      " the soil is this that thou dost common \n",
      " how i return rebuked to my content \n",
      " and gain by ills thrice more than i have spent \n",
      " by self-example mayst thou be denied \n",
      " thou art thy mother's glass and such mayst to thee \n",
      " if thou turn back and all give back again \n",
      " yet will be true before thy beauty do \n",
      " but rising best not let that is not better \n",
      " to say him this and love i am not so \n",
      " to put fair truth upon thy foul age \n",
      " that thou wilt so preposterously be stained \n",
      " to leave for nothing all thy sum of good \n",
      " for nothing this wide universe \n",
      "\n",
      "temperature:  0.75 , embedding size:  100 \n",
      " shall i compare thee to a summer's day \n",
      " for my heart thou i that i am it see \n",
      " by self-example mayst have be or mine untrue \n",
      " thy self thou gav'st thy own worth then not knowing \n",
      " or me to whom thou gav'st it else mistaking \n",
      " so thy great gift upon misprision growing \n",
      " comes home again on thine judgement making \n",
      " thus have i had thee as a dream doth flatter \n",
      " in sleep a king but waking no such matter \n",
      " o what a mansion have those vices got \n",
      " which for their habitation chose out thee \n",
      " where beauty's veil doth cover every blot \n",
      " and all things turns to fair that eyes can see \n",
      " take heed dear heart of this large privilege \n",
      " the hardest knife ill-used doth lose his edge \n",
      " they own sweet hue which methinks from you \n",
      " no art of others and all all you \n",
      " \n",
      "\n",
      "temperature:  0.25 , embedding size:  100 \n",
      " shall i compare thee to a summer's day \n",
      " for my heart thou i that i am it see \n",
      " by self-example mayst have thy or thy fair \n",
      " and my heart knows the wide world's common place \n",
      " or mine eyes seeing this say this is not \n",
      " to put fair truth upon so foul a face \n",
      " in things right true my heart and eyes have erred \n",
      " and to this false plague are they now transferred \n",
      " o carve be fair and thine shall know \n",
      " for thee against my self i'll vow debate \n",
      " for i must ne'er love him whom thou dost hate \n",
      " but that to hope is with my verse can will \n",
      " all all the breathers of this world are dead \n",
      " you still shall live such virtue hath my mind \n",
      " and you must live drawn by such let not \n",
      " that i in thy abundance am sufficed \n",
      " and \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## model with minimum perplexity\n",
    "useWordEmbedding = True\n",
    "embedSize = [100]\n",
    "numUnit = [200]\n",
    "data = 'shakespeare'\n",
    "\n",
    "if data == 'shakespeare':\n",
    "    fileName_data = ''\n",
    "elif data == 'spenser':\n",
    "    fileName_data = 'Spenser_'\n",
    "elif data == 'both':\n",
    "    fileName_data = 'Both_'\n",
    "\n",
    "for i, dim in enumerate(embedSize):\n",
    "    for j, un in enumerate(numUnit):\n",
    "        if useWordEmbedding:\n",
    "            modelName = \"model_%sunit%d_withWordEmbedding %d.h5\" % (fileName_data, un, dim)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWordEmbedding %d.pk1\" % (fileName_data, un, dim)\n",
    "        test2 = LSTM_word()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "        if data == 'shakespeare':\n",
    "            test2.SonnetLoader('shakespeare')\n",
    "        elif data == 'spenser':\n",
    "            test2.SonnetLoader('Spenser_v2')\n",
    "        elif data == 'both':\n",
    "            test2.SonnetLoader(['shakespeare', 'Spenser_v2'])\n",
    "        \n",
    "        tempList = [1.5, 1, 0.75, 0.25]\n",
    "        predicted = [test2.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=150, temperature = x, checkPentameter = False, useWordEmbedding = True) for x in tempList]\n",
    "        for i, x in enumerate(predicted):\n",
    "            print('temperature: ', tempList[i], ', embedding size: ', dim, '\\n', x, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 40, 200)           641200    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "lambda_5 (Lambda)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 2,888,406\n",
      "Trainable params: 2,888,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 40, 200)           641200    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "lambda_6 (Lambda)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 2,888,406\n",
      "Trainable params: 2,888,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 40, 200)           641200    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 2,888,406\n",
      "Trainable params: 2,888,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 40, 200)           641200    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "lambda_8 (Lambda)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 2,888,406\n",
      "Trainable params: 2,888,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "temperature:  1.5 , embedding size:  200 \n",
      " shall i compare thee to a summer's day \n",
      " my heart doth heart's praise and her eye \n",
      " in heart that to flatterer stopped are \n",
      " mark how with my neglect i do dispense \n",
      " you are so strongly in my purpose bred \n",
      " that all the world besides methinks are dead \n",
      " and that they see what which which in my will \n",
      " i will acquaintance strangle and look strange \n",
      " be absent from thy walks and in my tongue \n",
      " thy sweet beloved name no more shall dwell \n",
      " lest i too much profane should do it wronk \n",
      " and haply of our old acquaintance tell \n",
      " for thee against my self i'll vow debate \n",
      " for i must ne'er love him whom thou dost hate \n",
      " but thou not with me thou hast my state \n",
      " and chide the beauty of thy lusty respect \n",
      " to say within thine own deep sunken eyes \n",
      " were an \n",
      "\n",
      "temperature:  1 , embedding size:  200 \n",
      " shall i compare thee to a summer's day \n",
      " my heart doth heart's praise and her eye \n",
      " in heart that to flatterer stopped are \n",
      " mark how with my neglect i do dispense \n",
      " you are so strongly in my purpose bred \n",
      " that all the world besides methinks are dead \n",
      " and that they see what which which in my will \n",
      " i will acquaintance strangle and look strange \n",
      " be absent from thy walks and in my tongue \n",
      " thy sweet beloved name no more shall dwell \n",
      " lest i too much profane should do it wronk \n",
      " and haply of our old acquaintance tell \n",
      " for thee against my self i'll vow debate \n",
      " for i must ne'er love him whom thou dost hate \n",
      " by self-example mayst thou be denied \n",
      " and taught me much to my love i do \n",
      " if i may thee my loss is so \n",
      " and i am still \n",
      "\n",
      "temperature:  0.75 , embedding size:  200 \n",
      " shall i compare thee to a summer's day \n",
      " my heart doth heart's praise and her eye \n",
      " in heart that to flatterer stopped are \n",
      " mark how with my neglect i do dispense \n",
      " you are so strongly in my purpose bred \n",
      " that all the world besides methinks are dead \n",
      " and that they see what which which in my will \n",
      " i will acquaintance strangle and look strange \n",
      " be absent from thy walks and in my tongue \n",
      " thy sweet beloved name no more shall dwell \n",
      " lest i too much profane should do it wronk \n",
      " and haply of our old acquaintance tell \n",
      " for thee against my self i'll vow debate \n",
      " for i must ne'er love him whom thou dost hate \n",
      " by self-example mayst thou be denied \n",
      " and by and by clean starved for a look \n",
      " possessing or pursuing no delight \n",
      " save what is had or must from \n",
      "\n",
      "temperature:  0.25 , embedding size:  200 \n",
      " shall i compare thee to a summer's day \n",
      " my heart doth heart's praise and her eye \n",
      " in thee to love to give till nothing a even \n",
      " when i was wont to greet it with my lays \n",
      " as philomel in summer's front doth sing \n",
      " and stops her pipe in growth of riper days \n",
      " not that the summer is less pleasant now \n",
      " than when her mournful hymns did hush the night \n",
      " but that wild music burthens every bough \n",
      " and sweets grown common lose their dear delight \n",
      " therefore like her i sometime hold my tongue \n",
      " because i would not dull you with my song \n",
      " and do not my heart think so but one \n",
      " that proud of many lives upon thy gains \n",
      " o him she stores to show what wealth she had \n",
      " in days long since before these last so bad \n",
      " when i was certain o'er incertainty \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## model with minimum perplexity\n",
    "useWordEmbedding = True\n",
    "embedSize = [200]\n",
    "numUnit = [400]\n",
    "data = 'shakespeare'\n",
    "\n",
    "if data == 'shakespeare':\n",
    "    fileName_data = ''\n",
    "elif data == 'spenser':\n",
    "    fileName_data = 'Spenser_'\n",
    "elif data == 'both':\n",
    "    fileName_data = 'Both_'\n",
    "\n",
    "for i, dim in enumerate(embedSize):\n",
    "    for j, un in enumerate(numUnit):\n",
    "        if useWordEmbedding:\n",
    "            modelName = \"model_%sunit%d_withWordEmbedding %d.h5\" % (fileName_data, un, dim)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWordEmbedding %d.pk1\" % (fileName_data, un, dim)\n",
    "        test2 = LSTM_word()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "        if data == 'shakespeare':\n",
    "            test2.SonnetLoader('shakespeare')\n",
    "        elif data == 'spenser':\n",
    "            test2.SonnetLoader('Spenser_v2')\n",
    "        elif data == 'both':\n",
    "            test2.SonnetLoader(['shakespeare', 'Spenser_v2'])\n",
    "        \n",
    "        tempList = [1.5, 1, 0.75, 0.25]\n",
    "        predicted = [test2.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=150, temperature = x, checkPentameter = False, useWordEmbedding = True) for x in tempList]\n",
    "        for i, x in enumerate(predicted):\n",
    "            print('temperature: ', tempList[i], ', embedding size: ', dim, '\\n', x, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to C:\\Users\\HyeongChan\n",
      "[nltk_data]     Jo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 40, 200)           874600    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4373)              1753573   \n",
      "=================================================================\n",
      "Total params: 3,589,773\n",
      "Trainable params: 3,589,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 40, 200)           874600    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4373)              1753573   \n",
      "=================================================================\n",
      "Total params: 3,589,773\n",
      "Trainable params: 3,589,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 40, 200)           874600    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4373)              1753573   \n",
      "=================================================================\n",
      "Total params: 3,589,773\n",
      "Trainable params: 3,589,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 40, 200)           874600    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4373)              1753573   \n",
      "=================================================================\n",
      "Total params: 3,589,773\n",
      "Trainable params: 3,589,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "temperature:  1.5 , embedding size:  200 \n",
      " shall i compare thee to a summer's day \n",
      " when others gaze upon their shadows vain \n",
      " but th'only image of that heavenly ray \n",
      " whereof some glance doth in mine eye remain \n",
      " of which beholding the idea plain \n",
      " through contemplation of my purest part \n",
      " with light thereof i do myself sustain \n",
      " and thereon feed my love-afamished heart \n",
      " but with such brightness whilst i fill my mind \n",
      " i starve my body and mine eyes do blind \n",
      " nor thou art covetous and he is kind \n",
      " he learned but surety-like to write for me \n",
      " under that bond that him as fist doth bind \n",
      " the statute of thy beauty thou wilt take \n",
      " thou usurer that put'st forth all to use \n",
      " and sue a friend came debtor for my sake \n",
      " so him i lose through my unkind abuse \n",
      " him have i lost thou hast both him and me \n",
      " \n",
      "\n",
      "temperature:  1 , embedding size:  200 \n",
      " shall i compare thee to a summer's day \n",
      " when others gaze upon their shadows vain \n",
      " but th'only image of that heavenly ray \n",
      " whereof some glance doth in mine eye remain \n",
      " of which beholding the idea plain \n",
      " through contemplation of my purest part \n",
      " with light thereof i do myself sustain \n",
      " and thereon feed my love-afamished heart \n",
      " but with such brightness whilst i fill my mind \n",
      " i starve my body and mine eyes do blind \n",
      " nor thou art covetous and he is kind \n",
      " he learned but surety-like to write for me \n",
      " under that bond that him as fist doth bind \n",
      " the statute of thy beauty thou wilt take \n",
      " thou usurer that put'st forth all to use \n",
      " and sue a friend came debtor for my sake \n",
      " so him i lose through my unkind abuse \n",
      " him have i lost thou hast both him and me \n",
      " \n",
      "\n",
      "temperature:  0.75 , embedding size:  200 \n",
      " shall i compare thee to a summer's day \n",
      " when others gaze upon their shadows vain \n",
      " but th'only image of that heavenly ray \n",
      " whereof some glance doth in mine eye remain \n",
      " of which beholding the idea plain \n",
      " through contemplation of my purest part \n",
      " with light thereof i do myself sustain \n",
      " and thereon feed my love-afamished heart \n",
      " but with such brightness whilst i fill my mind \n",
      " i starve my body and mine eyes do blind \n",
      " nor thou art covetous and he is kind \n",
      " he learned but surety-like to write for me \n",
      " under that bond that him as fist doth bind \n",
      " the statute of thy beauty thou wilt take \n",
      " thou usurer that put'st forth all to use \n",
      " and sue a friend came debtor for my sake \n",
      " so him i lose through my unkind abuse \n",
      " him have i lost thou hast both him and me \n",
      " \n",
      "\n",
      "temperature:  0.25 , embedding size:  200 \n",
      " shall i compare thee to a summer's day \n",
      " when others gaze upon their shadows vain \n",
      " but th'only image of that heavenly ray \n",
      " whereof some glance doth in mine eye remain \n",
      " of which beholding the idea plain \n",
      " through contemplation of my purest part \n",
      " with light thereof i do myself sustain \n",
      " and thereon feed my love-afamished heart \n",
      " but with such brightness whilst i fill my mind \n",
      " i starve my body and mine eyes do blind \n",
      " nor thou art covetous and he is kind \n",
      " he learned but surety-like to write for me \n",
      " under that bond that him as fist doth bind \n",
      " the statute of thy beauty thou wilt take \n",
      " thou usurer that put'st forth all to use \n",
      " and sue a friend came debtor for my sake \n",
      " so him i lose through my unkind abuse \n",
      " him have i lost thou hast both him and me \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "## model with minimum perplexity\n",
    "useWordEmbedding = True\n",
    "embedSize = [200]\n",
    "numUnit = [400]\n",
    "data = 'both'\n",
    "\n",
    "if data == 'shakespeare':\n",
    "    fileName_data = ''\n",
    "elif data == 'spenser':\n",
    "    fileName_data = 'Spenser_'\n",
    "elif data == 'both':\n",
    "    fileName_data = 'Both_'\n",
    "\n",
    "for i, dim in enumerate(embedSize):\n",
    "    for j, un in enumerate(numUnit):\n",
    "        if useWordEmbedding:\n",
    "            modelName = \"model_%sunit%d_withWordEmbedding %d.h5\" % (fileName_data, un, dim)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWordEmbedding %d.pk1\" % (fileName_data, un, dim)\n",
    "        test2 = LSTM_word()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "        if data == 'shakespeare':\n",
    "            test2.SonnetLoader('shakespeare')\n",
    "        elif data == 'spenser':\n",
    "            test2.SonnetLoader('Spenser_v2')\n",
    "        elif data == 'both':\n",
    "            test2.SonnetLoader(['shakespeare', 'Spenser_v2'])\n",
    "        \n",
    "        tempList = [1.5, 1, 0.75, 0.25]\n",
    "        predicted = [test2.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=150, temperature = x, checkPentameter = False, useWordEmbedding = True) for x in tempList]\n",
    "        for i, x in enumerate(predicted):\n",
    "            print('temperature: ', tempList[i], ', embedding size: ', dim, '\\n', x, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_step1_patience5_maxEpoch400_char.h5\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "lambda_6 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "87876/87876 [==============================] - 49s 562us/step\n",
      "[0.3821563599080511, 0.879307210445404]\n",
      "perplexity:  1.465441203471861\n",
      "accuracy:  0.879307210445404\n",
      "step  1 patience 5\n",
      "1.465441203471861\n",
      "0.879307210445404\n",
      "model_step2_patience5_maxEpoch400_char.h5\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_10 (LSTM)               (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "87876/87876 [==============================] - 50s 565us/step\n",
      "[2.359622018822031, 0.6915881633758545]\n",
      "perplexity:  10.586949028596067\n",
      "accuracy:  0.6915881633758545\n",
      "step  2 patience 5\n",
      "10.586949028596067\n",
      "0.879307210445404\n",
      "model_step4_patience5_maxEpoch400_char.h5\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "lambda_8 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "87876/87876 [==============================] - 51s 577us/step\n",
      "[3.1108362220536, 0.5339797139167786]\n",
      "perplexity:  22.439801213876386\n",
      "accuracy:  0.5339797139167786\n",
      "step  4 patience 5\n",
      "22.439801213876386\n",
      "0.879307210445404\n",
      "model_step8_patience5_maxEpoch400_char.h5\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "lambda_9 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "87876/87876 [==============================] - 51s 575us/step\n",
      "[3.2265244518068017, 0.43278029561042786]\n",
      "perplexity:  25.191948810111516\n",
      "accuracy:  0.43278029561042786\n",
      "step  8 patience 5\n",
      "25.191948810111516\n",
      "0.879307210445404\n"
     ]
    }
   ],
   "source": [
    "# Look into perplexity - with characters\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "stepList = [1, 2, 4, 8]\n",
    "patienceList = [5]\n",
    "perplexity = np.zeros((len(stepList), len(patienceList)))\n",
    "accuracy = np.zeros((len(stepList), len(patienceList)))\n",
    "\n",
    "for i, step in enumerate(stepList):\n",
    "    for j, patience in enumerate(patienceList):\n",
    "        maxEpoch = '_maxEpoch400'   \n",
    "        modelName = \"model_step%d_patience%d%s_char.h5\" % (step, patience, maxEpoch)\n",
    "        print(modelName)\n",
    "        mappingName = \"model_step%d_patience%d%s_char.pkl\" % (step, patience, maxEpoch)\n",
    "        test2 = LSTM_char(step = 1) # for the purpose of testing; therefore, it is different from the settings used in each model\n",
    "        test2.SonnetLoader('shakespeare')\n",
    "        test2.getTrainSeq()\n",
    "        test2.getMapping()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "        [perplexity[i, j], accuracy[i, j]] = test2.perplexity_train()\n",
    "        print(\"step \", step, \"patience\", patience)\n",
    "        print(perplexity[i, j])\n",
    "        print(accuracy[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_step1_patience15_maxEpoch400_char.h5\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "88435/88435 [==============================] - 21s 236us/step\n",
      "[0.28058014103925105, 0.9265788197517395]\n",
      "perplexity:  1.3238976369435778\n",
      "accuracy:  0.9265788197517395\n",
      "step  1 patience 15\n",
      "1.3238976369435778\n",
      "0.9265788197517395\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "lambda_8 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "88435/88435 [==============================] - 21s 241us/step\n",
      "[0.23484841205161577, 0.9277661442756653]\n",
      "perplexity:  1.2647170383401218\n",
      "accuracy:  0.9277661442756653\n",
      "step  1 patience 15\n",
      "1.2647170383401218\n",
      "0.9277661442756653\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "lambda_9 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "88435/88435 [==============================] - 22s 253us/step\n",
      "[0.24598189936536402, 0.9269633293151855]\n",
      "perplexity:  1.278876424857325\n",
      "accuracy:  0.9269633293151855\n",
      "step  1 patience 15\n",
      "1.278876424857325\n",
      "0.9269633293151855\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "lambda_10 (Lambda)           (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "88435/88435 [==============================] - 20s 229us/step\n",
      "[0.5835255859986767, 0.9219878911972046]\n",
      "perplexity:  1.7923463759023284\n",
      "accuracy:  0.9219878911972046\n",
      "step  1 patience 15\n",
      "1.7923463759023284\n",
      "0.9219878911972046\n"
     ]
    }
   ],
   "source": [
    "# Look into perplexity - with characters & different temperatures\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "stepList = [1]\n",
    "tempList = [1.5, 1, 0.75, 0.25]\n",
    "patienceList = [15]\n",
    "perplexity = np.zeros((len(stepList), len(tempList)))\n",
    "accuracy = np.zeros((len(stepList), len(tempList)))\n",
    "\n",
    "for i, step in enumerate(stepList):\n",
    "    for j, patience in enumerate(patienceList):\n",
    "        maxEpoch = '_maxEpoch400'   \n",
    "        modelName = \"model_step%d_patience%d%s_char.h5\" % (step, patience, maxEpoch)\n",
    "        print(modelName)\n",
    "        mappingName = \"model_step%d_patience%d%s_char.pkl\" % (step, patience, maxEpoch)\n",
    "        test2 = LSTM_char(step = 1) # for the purpose of testing; therefore, it is different from the settings used in each model\n",
    "        test2.SonnetLoader('shakespeare')\n",
    "        test2.getTrainSeq()\n",
    "        test2.getMapping()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "        for k, temp in enumerate(tempList):\n",
    "            [perplexity[i, k], accuracy[i, k]] = test2.perplexity_train(temperature = temp)\n",
    "            print(\"step \", step, \"patience\", patience)\n",
    "            print(perplexity[i, k])\n",
    "            print(accuracy[i, k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.4654412 ]\n",
      " [10.58694903]\n",
      " [22.43980121]\n",
      " [25.19194881]]\n",
      "[[0.87930721]\n",
      " [0.69158816]\n",
      " [0.53397971]\n",
      " [0.4327803 ]]\n"
     ]
    }
   ],
   "source": [
    "print(perplexity)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_step1_patience5_maxEpoch400_char.h5\n",
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "lambda_12 (Lambda)           (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "87876/87876 [==============================] - 23s 265us/step\n",
      "[0.3821563599080511, 0.879307210445404]\n",
      "perplexity:  1.465441203471861\n",
      "accuracy:  0.879307210445404\n",
      "step  1 patience 5\n",
      "1.465441203471861\n",
      "0.879307210445404\n",
      "model_step1_patience10_maxEpoch400_char.h5\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_15 (LSTM)               (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "lambda_13 (Lambda)           (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "87876/87876 [==============================] - 23s 259us/step\n",
      "[0.30785916300303207, 0.8985502123832703]\n",
      "perplexity:  1.360509365390256\n",
      "accuracy:  0.8985502123832703\n",
      "step  1 patience 10\n",
      "1.360509365390256\n",
      "0.8985502123832703\n",
      "model_step1_patience15_maxEpoch400_char.h5\n",
      "Model: \"sequential_30\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "lambda_14 (Lambda)           (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "87876/87876 [==============================] - 24s 270us/step\n",
      "[0.2047204081235861, 0.9310733079910278]\n",
      "perplexity:  1.2271819069012582\n",
      "accuracy:  0.9310733079910278\n",
      "step  1 patience 15\n",
      "1.2271819069012582\n",
      "0.9310733079910278\n"
     ]
    }
   ],
   "source": [
    "# Look into perplexity - with characters\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "stepList = [1]\n",
    "patienceList = [5, 10, 15]\n",
    "perplexity = np.zeros((len(stepList), len(patienceList)))\n",
    "accuracy = np.zeros((len(stepList), len(patienceList)))\n",
    "\n",
    "for i, step in enumerate(stepList):\n",
    "    for j, patience in enumerate(patienceList):\n",
    "        maxEpoch = '_maxEpoch400'   \n",
    "        modelName = \"model_step%d_patience%d%s_char.h5\" % (step, patience, maxEpoch)\n",
    "        print(modelName)\n",
    "        mappingName = \"model_step%d_patience%d%s_char.pkl\" % (step, patience, maxEpoch)\n",
    "        test2 = LSTM_char(step = 1) # for the purpose of testing; therefore, it is different from the settings used in each model\n",
    "        test2.SonnetLoader('shakespeare')\n",
    "        test2.getTrainSeq()\n",
    "        test2.getMapping()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "        [perplexity[i, j], accuracy[i, j]] = test2.perplexity_train()\n",
    "        print(\"step \", step, \"patience\", patience)\n",
    "        print(perplexity[i, j])\n",
    "        print(accuracy[i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.4654412  1.36050937 1.22718191]]\n",
      "[[0.87930721 0.89855021 0.93107331]]\n"
     ]
    }
   ],
   "source": [
    "print(perplexity)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               1322800   \n",
      "_________________________________________________________________\n",
      "lambda_16 (Lambda)           (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3206)              323806    \n",
      "=================================================================\n",
      "Total params: 1,646,606\n",
      "Trainable params: 1,646,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 14s 1ms/step\n",
      "[0.03988895964333189, 0.9988215565681458]\n",
      "perplexity:  1.0406952086091383\n",
      "accuracy:  0.9988215565681458\n",
      "unit  100 embedding dimension 1\n",
      "1.0406952086091383\n",
      "0.9988215565681458\n"
     ]
    }
   ],
   "source": [
    "## Look into perplexity - word-based model, without wordEmbedding\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "useWordEmbedding = False\n",
    "embedSize = [1]\n",
    "numUnit = [100]\n",
    "data = 'shakespeare'\n",
    "\n",
    "if data == 'shakespeare':\n",
    "    fileName_data = ''\n",
    "elif data == 'spenser':\n",
    "    fileName_data = 'Spenser_'\n",
    "elif data == 'both':\n",
    "    fileName_data = 'Both_'\n",
    "perplexity = np.zeros((len(numUnit), len(embedSize)))\n",
    "accuracy = np.zeros((len(numUnit), len(embedSize)))\n",
    "\n",
    "for i, dim in enumerate(embedSize):\n",
    "    for j, un in enumerate(numUnit):\n",
    "        if useWordEmbedding:\n",
    "            modelName = \"model_%sunit%d_withWordEmbedding %d.h5\" % (fileName_data, un, dim)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWordEmbedding %d.pkl\" % (fileName_data, un, dim)\n",
    "        else:\n",
    "            modelName = \"model_%sunit%d_withWords_noEmbedding.h5\" % (fileName_data, un)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWords_noEmbedding.pkl\" % (fileName_data, un)\n",
    "        test2 = LSTM_word()\n",
    "        if data == 'shakespeare':\n",
    "            test2.SonnetLoader('shakespeare')\n",
    "        elif data == 'spenser':\n",
    "            test2.SonnetLoader('Spenser_v2')\n",
    "        elif data == 'both':\n",
    "            test2.SonnetLoader(['shakespeare', 'Spenser_v2'])\n",
    "        \n",
    "        test2.getTrainSeq()\n",
    "        test2.getMapping()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "        [perplexity[j, i], accuracy[j, i]] = test2.perplexity_train(useWordEmbedding=useWordEmbedding)\n",
    "        print(\"unit \", un, \"embedding dimension\", dim)\n",
    "        print(perplexity[j, i])\n",
    "        print(accuracy[j, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               1322800   \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3206)              323806    \n",
      "=================================================================\n",
      "Total params: 1,646,606\n",
      "Trainable params: 1,646,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "temperature:  1 , embedding size:  1 \n",
      " shall i compare thee to a summer's day \n",
      " beauteous thou thou seek to that which common near \n",
      " such eyes of trial sweet image frame \n",
      " whether time you are and your sweet \n",
      " where you in you of holy hours \n",
      " shall all their lines had time there eyes \n",
      " but ah with you when my love sweet \n",
      " then like a sad slave time \n",
      " therefore them so my poor sweet no find \n",
      " but beauty he is such as he kind to thee \n",
      " thou art of great of abundance oaths there \n",
      " if thy same form be praise \n",
      " when self sweet thief doth almost taught \n",
      " then i return rebuked my love respect \n",
      " that then have seem thy friend and she tongue \n",
      " but let my self than happy do more \n",
      " then then i dare to whom thou shouldst depart more \n",
      " and hang her toil she striving to die \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## generate the poem from the loaded settings\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "useWordEmbedding = False\n",
    "embedSize = [1]\n",
    "numUnit = [100]\n",
    "data = 'shakespeare'\n",
    "\n",
    "if data == 'shakespeare':\n",
    "    fileName_data = ''\n",
    "elif data == 'spenser':\n",
    "    fileName_data = 'Spenser_'\n",
    "elif data == 'both':\n",
    "    fileName_data = 'Both_'\n",
    "for i, dim in enumerate(embedSize):\n",
    "    for j, un in enumerate(numUnit):\n",
    "        if useWordEmbedding:\n",
    "            modelName = \"model_%sunit%d_withWordEmbedding %d.h5\" % (fileName_data, un, dim)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWordEmbedding %d.pkl\" % (fileName_data, un, dim)\n",
    "        else:\n",
    "            modelName = \"model_%sunit%d_withWords_noEmbedding.h5\" % (fileName_data, un)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWords_noEmbedding.pkl\" % (fileName_data, un)\n",
    "        test2 = LSTM_word()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "\n",
    "        if data == 'shakespeare':\n",
    "            test2.SonnetLoader('shakespeare')\n",
    "        elif data == 'spenser':\n",
    "            test2.SonnetLoader('Spenser_v2')\n",
    "        elif data == 'both':\n",
    "            test2.SonnetLoader(['shakespeare', 'Spenser_v2'])\n",
    "        \n",
    "        tempList = [1]\n",
    "        predicted = [test2.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=150, temperature = x, checkPentameter = False, useWordEmbedding = True) for x in tempList]\n",
    "        for i, x in enumerate(predicted):\n",
    "            print('temperature: ', tempList[i], ', embedding size: ', dim, '\\n', x, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 100)           320600    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3206)              644406    \n",
      "=================================================================\n",
      "Total params: 1,205,806\n",
      "Trainable params: 1,205,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "temperature:  1 , embedding size:  100 \n",
      " shall i compare thee to a summer's day \n",
      " for my heart thou i that i am it see \n",
      " by self-example mayst have be or mine eye \n",
      " no more i never i may true her pleasure i part \n",
      " the soil is this that thou dost common \n",
      " how i return rebuked to my content \n",
      " and gain by ills thrice more than i have spent \n",
      " by self-example mayst thou be denied \n",
      " thou art thy mother's glass and such mayst to thee \n",
      " if thou turn back and all give back again \n",
      " yet will be true before thy beauty do \n",
      " but rising best not let that is not better \n",
      " to say him this and love i am not so \n",
      " to put fair truth upon thy foul age \n",
      " that thou wilt so preposterously be stained \n",
      " to leave for nothing all thy sum of good \n",
      " for nothing this wide universe i call \n",
      " save thou my rose in it thou art my all \n",
      " since that beauty still thou shouldst not abhor \n",
      " with all they foul that thy complexion lack \n",
      " past reason hated as a swallowed bait \n",
      " on purpose laid to make the taker mad \n",
      " mad in pursuit and in possession so \n",
      " had having and in quest to have extreme \n",
      " a bliss in proof and proved a very woe \n",
      " before a joy proposed behind a dream \n",
      " all this the world well knows yet none knows well \n",
      " to shun the heaven that leads men to this hell \n",
      " therefore i swear it with my self alone \n",
      " and to be sure that is not thee i swear \n",
      " a thousand groans but thinking on thy face \n",
      " one on another's neck do witness bear \n",
      " thy black is fairest in my judgment's \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## generate the poem with word embedding\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "useWordEmbedding = True\n",
    "embedSize = [100]\n",
    "numUnit = [200]\n",
    "# embedSize = [200]\n",
    "# numUnit = [400]\n",
    "data = 'shakespeare'\n",
    "\n",
    "if data == 'shakespeare':\n",
    "    fileName_data = ''\n",
    "elif data == 'spenser':\n",
    "    fileName_data = 'Spenser_'\n",
    "elif data == 'both':\n",
    "    fileName_data = 'Both_'\n",
    "for i, dim in enumerate(embedSize):\n",
    "    for j, un in enumerate(numUnit):\n",
    "        if useWordEmbedding:\n",
    "            modelName = \"model_%sunit%d_withWordEmbedding %d.h5\" % (fileName_data, un, dim)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWordEmbedding %d.pk1\" % (fileName_data, un, dim)\n",
    "        else:\n",
    "            modelName = \"model_%sunit%d_withWords_noEmbedding.h5\" % (fileName_data, un)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWords_noEmbedding.pk1\" % (fileName_data, un)\n",
    "        test2 = LSTM_word()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "\n",
    "        if data == 'shakespeare':\n",
    "            test2.SonnetLoader('shakespeare')\n",
    "        elif data == 'spenser':\n",
    "            test2.SonnetLoader('Spenser_v2')\n",
    "        elif data == 'both':\n",
    "            test2.SonnetLoader(['shakespeare', 'Spenser_v2'])\n",
    "        \n",
    "        tempList = [1]\n",
    "        predicted = [test2.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=300, temperature = x, checkPentameter = False, useWordEmbedding = useWordEmbedding) for x in tempList]\n",
    "        for i, x in enumerate(predicted):\n",
    "            print('temperature: ', tempList[i], ', embedding size: ', dim, '\\n', x, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 40, 200)           444200    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "lambda_6 (Lambda)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2221)              890621    \n",
      "=================================================================\n",
      "Total params: 2,296,421\n",
      "Trainable params: 2,296,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "temperature:  1 , embedding size:  200 \n",
      " shall i compare thee to a summer's day \n",
      " that her fair pride with eyes herself to see \n",
      " and let her love like doth argue you \n",
      " to be divine and born of heavenly seed \n",
      " deriv'd from that fair spirit from whom all true \n",
      " and perfect beauty did at first proceed \n",
      " he only fair and what he fair hath made \n",
      " all other fair like flowers untimely fade \n",
      " mote have your life in honour long maintained \n",
      " but by his death which some perhaps will moan \n",
      " ye shall condemned be of many a one \n",
      " and think they die with pleasure live with pain \n",
      " my helice the lodestar of my life \n",
      " will shine again and look on me at last \n",
      " with lovely light to clear my cloudy grief \n",
      " till then i wander carefull comfortless \n",
      " in secret sorrow and sad pensiveness \n",
      " and my proud heart that to his den \n",
      " whose sweet aspect both god and man can move \n",
      " in her unspotted pleasauns to delight \n",
      " dark is my day while her fair light i miss \n",
      " and dead my life that wants such lively bliss \n",
      " but as she will whose will my life doth sway \n",
      " my lower heaven so it perforce must be \n",
      " but ye high heavens that all this sorrow see \n",
      " sith all your tempests cannot hold me back \n",
      " aswage your storms or else both you and she \n",
      " will both together me too sorely wrack \n",
      " enough it is for one man to sustain \n",
      " the storms which she alone on me doth rain \n",
      " ne joy i well that when this storm is past \n",
      " my helice the lodestar of my life \n",
      " will shine again and look on me at last \n",
      " with lovely light to clear \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## generate the poem with word embedding - using Spenser's amoretti\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "useWordEmbedding = True\n",
    "embedSize = [100]\n",
    "numUnit = [200]\n",
    "embedSize = [200]\n",
    "numUnit = [400]\n",
    "data = 'spenser'\n",
    "\n",
    "if data == 'shakespeare':\n",
    "    fileName_data = ''\n",
    "elif data == 'spenser':\n",
    "    fileName_data = 'Spenser_'\n",
    "elif data == 'both':\n",
    "    fileName_data = 'Both_'\n",
    "for i, dim in enumerate(embedSize):\n",
    "    for j, un in enumerate(numUnit):\n",
    "        if useWordEmbedding:\n",
    "            modelName = \"model_%sunit%d_withWordEmbedding %d.h5\" % (fileName_data, un, dim)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWordEmbedding %d.pk1\" % (fileName_data, un, dim)\n",
    "        else:\n",
    "            modelName = \"model_%sunit%d_withWords_noEmbedding.h5\" % (fileName_data, un)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWords_noEmbedding.pk1\" % (fileName_data, un)\n",
    "        test2 = LSTM_word()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "\n",
    "        if data == 'shakespeare':\n",
    "            test2.SonnetLoader('shakespeare')\n",
    "        elif data == 'spenser':\n",
    "            test2.SonnetLoader('Spenser_v2')\n",
    "        elif data == 'both':\n",
    "            test2.SonnetLoader(['shakespeare', 'Spenser_v2'])\n",
    "        \n",
    "        tempList = [1]\n",
    "        predicted = [test2.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=300, temperature = x, checkPentameter = False, useWordEmbedding = useWordEmbedding) for x in tempList]\n",
    "        for i, x in enumerate(predicted):\n",
    "            print('temperature: ', tempList[i], ', embedding size: ', dim, '\\n', x, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_26\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 40, 100)           437300    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "lambda_13 (Lambda)           (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4373)              878973    \n",
      "=================================================================\n",
      "Total params: 1,557,073\n",
      "Trainable params: 1,557,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "temperature:  1 , embedding size:  100 \n",
      " when others gaze upon their shadows vain \n",
      " and to be praised of ages yet to be \n",
      " then do thy office muse i teach thee how \n",
      " to make him seem long hence as he shows now \n",
      " that thou being mayst mine truth thy will \n",
      " if if thou lour'st on me do i not spend \n",
      " revenge upon my self with present moan \n",
      " what merit do i in my self respect \n",
      " that is so proud thy service to despise \n",
      " when all my best doth worship thy defect \n",
      " commanded by the motion of thine eyes \n",
      " but love hate on for now i know thy mind \n",
      " those that can see thou lov'st and i am blind \n",
      " love's eye is not so true as all men's no \n",
      " how can it o how can love's eye be true \n",
      " that is so vexed with watching and with tears \n",
      " no marvel then though i mistake my view \n",
      " the sun it self sees not till heaven clears \n",
      " o cunning love with tears thou keep'st me blind \n",
      " lest eyes well-seeing thy foul faults should find \n",
      " whose speechless song being many seeming one \n",
      " sings this to thee 'thou single wilt prove none' \n",
      " and in my madness might speak ill of thee \n",
      " now this ill-wresting world is grown so bad \n",
      " mad slanderers by mad ears believed be \n",
      " that i may not be so nor thou belied \n",
      " bear thine eyes straight though thy proud heart go wide \n",
      " and from thy beauty still my lover's life \n",
      " so will i well shall make the rest subdue \n",
      " i do i hope her stubborn heart to me \n",
      " and all that night which they do leave \n",
      " thy registers and your own will beguiled \n",
      " no word \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## generate the poem with word embedding - using Spenser's amoretti\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "useWordEmbedding = True\n",
    "embedSize = [100]\n",
    "numUnit = [200]\n",
    "# embedSize = [200]\n",
    "# numUnit = [400]\n",
    "data = 'both'\n",
    "\n",
    "if data == 'shakespeare':\n",
    "    fileName_data = ''\n",
    "elif data == 'spenser':\n",
    "    fileName_data = 'Spenser_'\n",
    "elif data == 'both':\n",
    "    fileName_data = 'Both_'\n",
    "for i, dim in enumerate(embedSize):\n",
    "    for j, un in enumerate(numUnit):\n",
    "        if useWordEmbedding:\n",
    "            modelName = \"model_%sunit%d_withWordEmbedding %d.h5\" % (fileName_data, un, dim)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWordEmbedding %d.pk1\" % (fileName_data, un, dim)\n",
    "        else:\n",
    "            modelName = \"model_%sunit%d_withWords_noEmbedding.h5\" % (fileName_data, un)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWords_noEmbedding.pk1\" % (fileName_data, un)\n",
    "        test2 = LSTM_word()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "\n",
    "        if data == 'shakespeare':\n",
    "            test2.SonnetLoader('shakespeare')\n",
    "        elif data == 'spenser':\n",
    "            test2.SonnetLoader('Spenser_v2')\n",
    "        elif data == 'both':\n",
    "            test2.SonnetLoader(['shakespeare', 'Spenser_v2'])\n",
    "        \n",
    "        tempList = [1]\n",
    "        #predicted = [test2.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=300, temperature = x, checkPentameter = False, useWordEmbedding = useWordEmbedding) for x in tempList]\n",
    "        predicted = [test2.Predict(\"when others gaze upon their shadows vain \\n\", outputText_len=300, temperature = x, checkPentameter = False, useWordEmbedding = useWordEmbedding) for x in tempList]\n",
    "        #predicted = [test2.Predict(\"decrease suffer for burthens charge my drained\\n\", outputText_len=300, temperature = x, checkPentameter = False, useWordEmbedding = useWordEmbedding) for x in tempList]\n",
    "        for i, x in enumerate(predicted):\n",
    "            print('temperature: ', tempList[i], ', embedding size: ', dim, '\\n', x, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Look into perplexity\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "useWordEmbedding = True\n",
    "embedSize = [5, 10, 25, 50, 100, 200]\n",
    "numUnit = [200, 300, 400]\n",
    "data = 'shakespeare'\n",
    "\n",
    "if data == 'shakespeare':\n",
    "    fileName_data = ''\n",
    "elif data == 'spenser':\n",
    "    fileName_data = 'Spenser_'\n",
    "elif data == 'both':\n",
    "    fileName_data = 'Both_'\n",
    "perplexity = np.zeros((len(numUnit), len(embedSize)))\n",
    "accuracy = np.zeros((len(numUnit), len(embedSize)))\n",
    "\n",
    "for i, dim in enumerate(embedSize):\n",
    "    for j, un in enumerate(numUnit):\n",
    "        if useWordEmbedding:\n",
    "            modelName = \"model_%sunit%d_withWordEmbedding %d.h5\" % (fileName_data, un, dim)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWordEmbedding %d.pk1\" % (fileName_data, un, dim)\n",
    "        test2 = LSTM_word()\n",
    "        if data == 'shakespeare':\n",
    "            test2.SonnetLoader('shakespeare')\n",
    "        elif data == 'spenser':\n",
    "            test2.SonnetLoader('Spenser_v2')\n",
    "        elif data == 'both':\n",
    "            test2.SonnetLoader(['shakespeare', 'Spenser_v2'])\n",
    "        \n",
    "        test2.getTrainSeq()\n",
    "        test2.getMapping()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "        [perplexity[j, i], accuracy[j, i]] = test2.perplexity_train(useWordEmbedding=True)\n",
    "        print(\"unit \", un, \"embedding dimension\", dim)\n",
    "        print(perplexity[j, i])\n",
    "        print(accuracy[j, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "useWordEmbedding = True\n",
    "embedSize = [5, 10, 25, 50, 100, 200]\n",
    "numUnit = [200, 300, 400]\n",
    "data = 'shakespeare'\n",
    "\n",
    "if data == 'shakespeare':\n",
    "    fileName_data = ''\n",
    "elif data == 'spenser':\n",
    "    fileName_data = 'Spenser_'\n",
    "elif data == 'both':\n",
    "    fileName_data = 'Both_'\n",
    "perplexity = np.zeros((len(numUnit), len(embedSize)))\n",
    "accuracy = np.zeros((len(numUnit), len(embedSize)))\n",
    "\n",
    "for i, dim in enumerate(embedSize):\n",
    "    for j, un in enumerate(numUnit):\n",
    "        if useWordEmbedding:\n",
    "            modelName = \"model_%sunit%d_withWordEmbedding %d.h5\" % (fileName_data, un, dim)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWordEmbedding %d.pk1\" % (fileName_data, un, dim)\n",
    "        test2 = LSTM_word()\n",
    "        if data == 'shakespeare':\n",
    "            test2.SonnetLoader('shakespeare')\n",
    "        elif data == 'spenser':\n",
    "            test2.SonnetLoader('Spenser_v2')\n",
    "        elif data == 'both':\n",
    "            test2.SonnetLoader(['shakespeare', 'Spenser_v2'])\n",
    "        \n",
    "        test2.getTrainSeq()\n",
    "        test2.getMapping()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "        [perplexity[j, i], accuracy[j, i]] = test2.perplexity_train(useWordEmbedding=True)\n",
    "        print(\"unit \", un, \"embedding dimension\", dim)\n",
    "        print(perplexity[j, i])\n",
    "        print(accuracy[j, i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
