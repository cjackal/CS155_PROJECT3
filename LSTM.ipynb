{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training - can be skipped, if you use saved files\n",
    "from LSTMforSonnet import LSTM_char\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "test = LSTM_char()\n",
    "test.SonnetLoader('shakespeare')\n",
    "test.getTrainSeq()\n",
    "test.getMapping()\n",
    "test.Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_step1_patience15_maxEpoch400_char.h5\n",
      "model_step1_patience15_maxEpoch400_char.pkl\n",
      "with temperature  10 \n",
      " shall i compare thee to a summer's day?\n",
      "The pearst is from their remily reigh die,\n",
      "Mike,\n",
      "And trees with ver, ard this for comfore,\n",
      "The ridef'l's though in your preifule,\n",
      "Thesefore mine in my hatl his gilder's giet,\n",
      "Wille mine eark infrear's love's live,\n",
      "Which purd's your preauly in proody's pripeing,\n",
      "Make hinc, will cansuiest in thee, 'glikerquen me,\n",
      "As, would I an aming presiouny quies thee,\n",
      "Askins my love's limethed ard calf me,\n",
      "Thall quine, whillingingle right bedimide,\n",
      "Seeds' all triess, and kindlest, and kidlf\n",
      "I willf'ly nither love, that from lime,\n",
      "Then grow'st thou lory detied, in theire,\n",
      "Whouck thou love, 'stild conter precious kind,\n",
      "Seeling illow'rarifull from lime that glow,\n",
      "Whilly griffle reffiel'st,\n",
      "And ligute, and true lifule y'sequere,\n",
      "When I apperies willing, his love,\n",
      "Which plricking love's love'low, in is sull's\n",
      "with temperature  0.01 \n",
      " shall i compare thee to a summer's day?\n",
      "The merit his spilf belies which thou art no thence,\n",
      "The beauty to be, beriedess some desing,\n",
      "Than thou art the willing thy foul to be:\n",
      "In to men to thee that works it or than sholl mend\n",
      "Sevarter cales be bedmest here bur to can.\n",
      "\n",
      "\n",
      "                                        sw d dembeb dit and all hair to love,\n",
      "  That acont thou lose can liey to me must,\n",
      "She till to thee a sweet werd can lieys Whend are sweet doth she swee,\n",
      "The pearest of belinn thinking olly in lie,\n",
      "To  yee, be thy demond in than by shines be.\n",
      "\n",
      "\n",
      "                                        sw d dembeb dit and all hair to love,\n",
      "  That acont thou lose can liey to me must,\n",
      "She till to thee a sweet werd can lieys Whend are sweet doth she swee,\n",
      "The pearest of belinn thinking olly in lie,\n",
      "To  yee, be thy demond in than by shines be.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make predictions after loading trained model from saved files\n",
    "from LSTMforSonnet import LSTM_char\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "test = LSTM_char()\n",
    "step = 1\n",
    "patience = 15\n",
    "maxEpoch = \"_maxEpoch400\"\n",
    "modelName = \"model_step%d_patience%d%s_char.h5\" % (step, patience, maxEpoch)\n",
    "mappingName = \"model_step%d_patience%d%s_char.pkl\" % (step, patience, maxEpoch)\n",
    "print(modelName)\n",
    "print(mappingName)\n",
    "#test.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "test.LoadModel()\n",
    "#tempList = [1.5, 1, 0.75, 0.25]\n",
    "tempList = [10, 0.01]\n",
    "predicted = [test.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=800, temperature = x) for x in tempList]\n",
    "for i, x in enumerate(predicted):\n",
    "    print('with temperature ', tempList[i], '\\n', x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_step1_patience15_maxEpoch400_char.h5\n",
      "model_step1_patience15_maxEpoch400_char.pkl\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5c4467024e6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetTrainSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLoadModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodelName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappingName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmappingName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - California Institute of Technology\\class\\Machine Learning and Data Mining\\Project 3\\git\\CS155_PROJECT3\\LSTMforSonnet.py\u001b[0m in \u001b[0;36mgetMapping\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainSeq_encoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainSeq_encoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoca_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainSeq_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainSeq_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoca_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoca_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "# make predictions after loading trained model from saved files\n",
    "from LSTMforSonnet import LSTM_char\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "test = LSTM_char()\n",
    "step = 1\n",
    "patience = 15\n",
    "maxEpoch = \"_maxEpoch400\"\n",
    "modelName = \"model_step%d_patience%d%s_char.h5\" % (step, patience, maxEpoch)\n",
    "mappingName = \"model_step%d_patience%d%s_char.pkl\" % (step, patience, maxEpoch)\n",
    "print(modelName)\n",
    "print(mappingName)\n",
    "test.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "\n",
    "tempList = [1.5, 1, 0.75, 0.25]\n",
    "perplexity = np.zeros((4,1))\n",
    "#tempList = [2, 0.1]\n",
    "# if data == 'shakespeare':\n",
    "#     test.SonnetLoader('shakespeare')\n",
    "# elif data == 'spenser':\n",
    "#     test.SonnetLoader('Spenser_v2')\n",
    "# elif data == 'both':\n",
    "#     test.SonnetLoader(['shakespeare', 'Spenser_v2'])\n",
    "\n",
    "test.getTrainSeq()\n",
    "test.getMapping()\n",
    "test.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "\n",
    "for i, temp in enumerate(tempList):\n",
    "    [perplexity[i], accuracy[i]] = test.perplexity_train(temperature = i)\n",
    "\n",
    "\n",
    "# predicted = [test.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=800, temperature = x) for x in tempList]\n",
    "# for i, x in enumerate(predicted):\n",
    "#     print('with temperature ', tempList[i], '\\n', x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find perplexity after loading trained model from saved files\n",
    "from LSTMforSonnet import LSTM_char\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "test = LSTM_char()\n",
    "step = 1\n",
    "patience = 15\n",
    "maxEpoch = \"_maxEpoch400\"\n",
    "modelName = \"model_step%d_patience%d%s_char.h5\" % (step, patience, maxEpoch)\n",
    "mappingName = \"model_step%d_patience%d%s_char.pkl\" % (step, patience, maxEpoch)\n",
    "print(modelName)\n",
    "print(mappingName)\n",
    "#test.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "test.LoadModel()\n",
    "tempList = [1.5, 1, 0.75, 0.25]\n",
    "[perplexity[j, i], accuracy[j, i]] = test2.perplexity_train(useWordEmbedding=True)\n",
    "predicted = [test.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=800, temperature = x) for x in tempList]\n",
    "for i, x in enumerate(predicted):\n",
    "    print('with temperature ', tempList[i], '\\n', x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summers day?\n",
      "The merit his spilf belies with flowns Wheres dost see,\n",
      "That me this dobe befuiele tirds showe speaks.\n",
      "  Mush givens with a pood the bebold of thee,\n",
      "  Thos to be, belueder thee I so love that wore,\n",
      "The repett by badse have sholl and all his growtred,\n",
      "  All that hid tost in thee, and then bess,\n",
      " hol I an your new In thaines bearth,\n",
      "Of burs my heart me bul are of as freas still:\n",
      "  For twill thy be then thee to bllovere soos,\n",
      "And dember bourupe to the elesust to creaken.\n",
      "  For thou as desticiness seap-for eres in,\n",
      "Ciming to tovens of thee I and mist, brace,\n",
      "And so see so to giads not so ence sed,\n",
      "Sead in theire,  I wos, these of elisust so efe?\n",
      "The then thou thy freat remolded do of ye,\n",
      "The tell hes, and thoughts conour these partelles sholl men mine,\n",
      "Then broul all his bown and beauty to \n",
      "11\n",
      "13\n",
      "13\n",
      "13\n",
      "16\n",
      "14\n",
      "11\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "12\n",
      "11\n",
      "16\n",
      "11\n",
      "16\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import SyllableTokenizer # use syllable tokenizer only when syllabel&stress info is not found in cmudict\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "SSP = SyllableTokenizer()\n",
    "text = re.sub(r\"'\", \"\", predicted[1])\n",
    "print(text)\n",
    "lines = text.split('\\n')\n",
    "for x in lines:\n",
    "    words = x.split(' ')\n",
    "    temp = [len(SSP.tokenize(eachWord)) for eachWord in words]\n",
    "    print(sum(temp))\n",
    "\n",
    "        \n",
    "    \n",
    "#     for word in predicted[0]:  # for each of the words not found in the dictionary, put the number of syllables based on SSP, and add an empty entry in stressList\n",
    "#     syl = set()\n",
    "#     if len(dict_syl)!=0 and len([x for x in dict_syl.index if x==word])!=0:\n",
    "#         for x in [dict_syl.loc[word][0], dict_syl.loc[word][1]]:\n",
    "#             syl.add(x)\n",
    "#         if len(syl)>1:\n",
    "#             syl.discard(0)\n",
    "#         syl_num.append(syl)\n",
    "#     else:\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to C:\\Users\\HyeongChan\n",
      "[nltk_data]     Jo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-700a471c0956>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtest2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSonnetLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'shakespeare'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mtest2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetTrainSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtest2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - California Institute of Technology\\class\\Machine Learning and Data Mining\\Project 3\\git\\CS155_PROJECT3\\LSTMforSonnet.py\u001b[0m in \u001b[0;36mgetMapping\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoca_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_orig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainSeq_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainSeq_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoca_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_orig\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoca_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\OneDrive - California Institute of Technology\\class\\Machine Learning and Data Mining\\Project 3\\git\\CS155_PROJECT3\\LSTMforSonnet.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    163\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoca_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_orig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainSeq_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainSeq_encoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoca_size\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_orig\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoca_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\np_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     \u001b[0mcategorical\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m     \u001b[0mcategorical\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0moutput_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# for training with words, not characters - can be skipped, if you use saved files\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "test2 = LSTM_word()\n",
    "test2.SonnetLoader('shakespeare')\n",
    "test2.getTrainSeq()\n",
    "test2.getMapping()\n",
    "print(test2.mapping)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 200)               2725600   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3206)              644406    \n",
      "=================================================================\n",
      "Total params: 3,370,006\n",
      "Trainable params: 3,370,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      " - 114s - loss: 6.4065 - accuracy: 0.1125\n",
      "Epoch 2/100\n",
      " - 110s - loss: 6.0112 - accuracy: 0.1134\n",
      "Epoch 3/100\n",
      " - 109s - loss: 5.8406 - accuracy: 0.1234\n",
      "Epoch 4/100\n",
      " - 108s - loss: 5.6014 - accuracy: 0.1460\n",
      "Epoch 5/100\n",
      " - 109s - loss: 5.3762 - accuracy: 0.1591\n",
      "Epoch 6/100\n",
      " - 108s - loss: 5.1459 - accuracy: 0.1711\n",
      "Epoch 7/100\n",
      " - 108s - loss: 5.1116 - accuracy: 0.1710\n",
      "Epoch 8/100\n",
      " - 109s - loss: 4.7195 - accuracy: 0.1921\n",
      "Epoch 9/100\n",
      " - 108s - loss: 4.4820 - accuracy: 0.2056\n",
      "Epoch 10/100\n",
      " - 108s - loss: 4.2360 - accuracy: 0.2245\n",
      "Epoch 11/100\n",
      " - 109s - loss: 3.9808 - accuracy: 0.2459\n",
      "Epoch 12/100\n",
      " - 108s - loss: 3.7092 - accuracy: 0.2763\n",
      "Epoch 13/100\n",
      " - 110s - loss: 3.4339 - accuracy: 0.3147\n",
      "Epoch 14/100\n",
      " - 109s - loss: 3.1526 - accuracy: 0.3651\n",
      "Epoch 15/100\n",
      " - 111s - loss: 2.8741 - accuracy: 0.4226\n",
      "Epoch 16/100\n",
      " - 108s - loss: 2.5931 - accuracy: 0.4861\n",
      "Epoch 17/100\n",
      " - 108s - loss: 2.3145 - accuracy: 0.5514\n",
      "Epoch 18/100\n",
      " - 109s - loss: 2.0567 - accuracy: 0.6115\n",
      "Epoch 19/100\n",
      " - 108s - loss: 1.8009 - accuracy: 0.6672\n",
      "Epoch 20/100\n",
      " - 109s - loss: 1.5685 - accuracy: 0.7144\n",
      "Epoch 21/100\n",
      " - 108s - loss: 1.3619 - accuracy: 0.7624\n",
      "Epoch 22/100\n",
      " - 108s - loss: 1.1653 - accuracy: 0.8029\n",
      "Epoch 23/100\n",
      " - 108s - loss: 0.9948 - accuracy: 0.8374\n",
      "Epoch 24/100\n",
      " - 108s - loss: 0.8517 - accuracy: 0.8686\n",
      "Epoch 25/100\n",
      " - 109s - loss: 0.7040 - accuracy: 0.9004\n",
      "Epoch 26/100\n",
      " - 109s - loss: 0.5979 - accuracy: 0.9206\n",
      "Epoch 27/100\n",
      " - 109s - loss: 0.4925 - accuracy: 0.9406\n",
      "Epoch 28/100\n",
      " - 108s - loss: 0.4094 - accuracy: 0.9569\n",
      "Epoch 29/100\n",
      " - 108s - loss: 0.3476 - accuracy: 0.9662\n",
      "Epoch 30/100\n",
      " - 109s - loss: 0.2790 - accuracy: 0.9782\n",
      "Epoch 31/100\n",
      " - 109s - loss: 0.2108 - accuracy: 0.9867\n",
      "Epoch 32/100\n",
      " - 109s - loss: 0.2085 - accuracy: 0.9865\n",
      "Epoch 33/100\n",
      " - 109s - loss: 0.1658 - accuracy: 0.9907\n",
      "Epoch 34/100\n",
      " - 109s - loss: 0.1938 - accuracy: 0.9825\n",
      "Epoch 35/100\n",
      " - 108s - loss: 0.0999 - accuracy: 0.9970\n",
      "Epoch 36/100\n",
      " - 108s - loss: 0.0642 - accuracy: 0.9993\n",
      "Epoch 37/100\n",
      " - 108s - loss: 0.0433 - accuracy: 0.9995\n",
      "Epoch 38/100\n",
      " - 109s - loss: 0.0471 - accuracy: 0.9990\n",
      "Epoch 39/100\n",
      " - 109s - loss: 0.3779 - accuracy: 0.9300\n",
      "Epoch 40/100\n",
      " - 109s - loss: 0.0983 - accuracy: 0.9932\n",
      "Epoch 41/100\n",
      " - 109s - loss: 0.0366 - accuracy: 0.9994\n",
      "Epoch 42/100\n",
      " - 108s - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      " - 108s - loss: 0.0143 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      " - 109s - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      " - 109s - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      " - 109s - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      " - 109s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      " - 110s - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      " - 110s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      " - 108s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      " - 109s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      " - 108s - loss: 0.6734 - accuracy: 0.8391\n",
      "Epoch 53/100\n",
      " - 109s - loss: 0.3781 - accuracy: 0.9235\n",
      "Epoch 54/100\n",
      " - 109s - loss: 0.0735 - accuracy: 0.9950\n",
      "Epoch 55/100\n",
      " - 109s - loss: 0.0254 - accuracy: 0.9999\n",
      "Epoch 56/100\n",
      " - 110s - loss: 0.0150 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      " - 110s - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      " - 109s - loss: 0.0089 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      " - 108s - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      " - 109s - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      " - 109s - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 00061: early stopping\n"
     ]
    }
   ],
   "source": [
    "test2.Train(useWordEmbedding = False, embeddingSize = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['shall', 'i', 'compare', 'thee', 'to', 'a', \"summer's\", 'day', '\\n', 'that', 'i', 'see', 'on', 'thee', 'which', 'beauty', 'bring', '\\n', 'and', 'in', 'our', 'faults', 'by', 'lies', 'we', 'flattered', 'be', '\\n', 'and', 'make', 'their', 'proud', 'lives', 'of', 'his', 'time', '\\n', 'now', 'love', 'is', 'thine', 'as', 'he', 'takes', 'from', 'you', '\\n', 'then', 'can', 'i', 'see', 'what', 'you', 'with', 'your', 'true', '\\n', 'to', 'love', 'your', 'self', 'with', 'keeps', \"love's\", 'still', '\\n', 'sweet', 'life', 'so', 'much', 'sweet', 'self', 'should', 'from', 'hate', '\\n', 'that', 'all', 'i', 'swear', 'yet', 'to', 'death', 'a', 'alone', '\\n', 'but', 'him', 'for', 'her', 'and', 'by', 'well', 'i', 'lose', 'thee', '\\n', 'those', 'long', \"i'll\", 'child', 'for', 'my', 'love', 'not', '\\n', 'though', 'i', 'in', 'love', 'loves', 'gracious', 'a', 'common', 'kind', '\\n', 'be', 'as', 'thy', 'proud', 'thy', 'self', 'thy', 'constancy', '\\n', 'to', 'live', 'thy', 'self', 'thy', 'self', 'i', 'will', '\\n', 'make', 'thee', 'a', 'heart', 'even', 'when', 'thy', 'graces', '\\n', 'the', 'perfect', 'of', 'my', 'will', '\\n', 'a', 'most', 'time', 'jewel', 'out', 'out', '\\n', 'now', 'in', 'thee']\n",
      "['shall', 'i', 'compare', 'thee', 'to', 'a', \"summer's\", 'day', '\\n', 'that', 'i', 'see', 'on', 'thee', 'which', 'beauty', 'bring', '\\n', 'and', 'in', 'our', 'faults', 'by', 'lies', 'we', 'flattered', 'be', '\\n', 'and', 'make', 'their', 'proud', 'lives', 'of', 'his', 'time', '\\n', 'now', 'his', 'beauty', 'shall', 'still', 'you', 'for', 'so', '\\n', 'and', 'all', 'my', 'best', 'is', 'to', 'this', 'all', 'one', '\\n', 'and', 'i', 'an', 'see', 'them', 'with', 'my', 'pen', '\\n', 'therefore', 'all', 'my', 'love', 'and', 'from', 'my', 'life', 'hath', 'end', '\\n', 'i', 'find', 'sweet', 'love', 'though', 'my', 'love', 'is', 'not', '\\n', 'so', 'thou', 'no', \"mother's\", 'that', 'i', 'am', 'both', '\\n', 'as', 'high', 'to', 'lose', 'my', 'purpose', 'disgrace', '\\n', 'till', 'my', 'heart', 'knows', 'all', 'present', \"eye's\", \"eye's\", '\\n', 'and', 'for', 'my', 'love', 'for', 'thee', 'is', 'so', 'fair', '\\n', 'that', 'i', 'am', 'not', 'with', 'present', 'nor', 'to', 'be', '\\n', 'and', 'he', 'that', 'might', 'i', 'not', 'my', 'love', 'be', '\\n', 'as', 'if', 'thou', 'not', 'becoming', 'this', 'nor', 'be', '\\n', 'not', 'thee', 'i', 'not', 'yet', 'i', 'am', 'not']\n",
      "['shall', 'i', 'compare', 'thee', 'to', 'a', \"summer's\", 'day', '\\n', 'that', 'i', 'see', 'on', 'thee', 'which', 'beauty', 'bring', '\\n', 'and', 'in', 'our', 'faults', 'by', 'lies', 'we', 'flattered', 'be', '\\n', 'and', 'make', 'their', 'proud', 'lives', 'of', 'his', 'time', '\\n', 'now', 'his', 'beauty', 'shall', 'still', 'you', 'for', 'so', '\\n', 'and', 'all', 'my', 'best', 'is', 'to', 'this', 'all', 'one', '\\n', 'and', 'i', 'an', 'accessary', 'needs', 'must', 'be', '\\n', 'that', 'i', 'will', 'not', 'to', 'be', 'with', 'thee', '\\n', 'of', 'him', 'thy', 'self', 'that', 'i', 'do', 'thee', 'more', '\\n', 'then', 'worthy', 'i', 'love', 'my', 'self', 'thou', 'dost', 'be', '\\n', 'the', 'ten', 'of', 'thine', 'for', 'happier', 'be', 'i', 'whilst', '\\n', 'thou', \"gav'st\", 'the', \"mother's\", 'part', 'i', 'am', 'love', '\\n', 'as', 'those', 'gold', 'candles', 'fixed', 'in', 'thee', 'air', '\\n', 'lose', 'all', 'of', 'more', 'that', 'i', 'may', 'see', '\\n', 'and', 'this', 'true', 'love', 'is', 'love', 'still', 'thee', 'and', 'so', 'so', '\\n', 'for', 'love', 'that', 'love', 'i', 'will', 'in', 'thee', '\\n', 'by', 'mine', 'eyes', 'thy', 'fair', 'truth', 'wouldst', 'to']\n",
      "with temperature  1.5 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now love is thine as he takes from you \n",
      " then can i see what you with your true \n",
      " to love your self with keeps love's still \n",
      " sweet life so much sweet self should from hate \n",
      " that all i swear yet to death a alone \n",
      " but him for her and by well i lose thee \n",
      " those long i'll child for my love not \n",
      " though i in love loves gracious a common kind \n",
      " be as thy proud thy self thy constancy \n",
      " to live thy self thy self i will \n",
      " make thee a heart even when thy graces \n",
      " the perfect of my will \n",
      " a most time jewel out out \n",
      " now in thee\n",
      "with temperature  0.75 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now his beauty shall still you for so \n",
      " and all my best is to this all one \n",
      " and i an see them with my pen \n",
      " therefore all my love and from my life hath end \n",
      " i find sweet love though my love is not \n",
      " so thou no mother's that i am both \n",
      " as high to lose my purpose disgrace \n",
      " till my heart knows all present eye's eye's \n",
      " and for my love for thee is so fair \n",
      " that i am not with present nor to be \n",
      " and he that might i not my love be \n",
      " as if thou not becoming this nor be \n",
      " not thee i not yet i am not\n",
      "with temperature  0.25 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now his beauty shall still you for so \n",
      " and all my best is to this all one \n",
      " and i an accessary needs must be \n",
      " that i will not to be with thee \n",
      " of him thy self that i do thee more \n",
      " then worthy i love my self thou dost be \n",
      " the ten of thine for happier be i whilst \n",
      " thou gav'st the mother's part i am love \n",
      " as those gold candles fixed in thee air \n",
      " lose all of more that i may see \n",
      " and this true love is love still thee and so so \n",
      " for love that love i will in thee \n",
      " by mine eyes thy fair truth wouldst to\n"
     ]
    }
   ],
   "source": [
    "# from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "\n",
    "# test2 = LSTM_word()\n",
    "# test2.LoadModel()\n",
    "tempList = [1.5, 0.75, 0.25]\n",
    "predicted = [test2.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=150, temperature = x) for x in tempList]\n",
    "for i, x in enumerate(predicted):\n",
    "    print('with temperature ', tempList[i], '\\n', x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128240\n",
      "3206\n",
      "43527862\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 40, 5)             16030     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 200)               164800    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3206)              644406    \n",
      "=================================================================\n",
      "Total params: 825,236\n",
      "Trainable params: 825,236\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 13s - loss: 6.4036 - accuracy: 0.1131\n",
      "Epoch 2/200\n",
      " - 12s - loss: 5.9879 - accuracy: 0.1134\n",
      "Epoch 3/200\n",
      " - 12s - loss: 5.8300 - accuracy: 0.1164\n",
      "Epoch 4/200\n",
      " - 12s - loss: 5.6294 - accuracy: 0.1365\n",
      "Epoch 5/200\n",
      " - 12s - loss: 5.4526 - accuracy: 0.1475\n",
      "Epoch 6/200\n",
      " - 13s - loss: 5.2986 - accuracy: 0.1518\n",
      "Epoch 7/200\n",
      " - 14s - loss: 5.1591 - accuracy: 0.1531\n",
      "Epoch 8/200\n",
      " - 14s - loss: 5.0263 - accuracy: 0.1562\n",
      "Epoch 9/200\n",
      " - 13s - loss: 4.8917 - accuracy: 0.1587\n",
      "Epoch 10/200\n",
      " - 12s - loss: 4.7540 - accuracy: 0.1619\n",
      "Epoch 11/200\n",
      " - 12s - loss: 4.6114 - accuracy: 0.1660\n",
      "Epoch 12/200\n",
      " - 12s - loss: 4.4637 - accuracy: 0.1691\n",
      "Epoch 13/200\n",
      " - 12s - loss: 4.3048 - accuracy: 0.1757\n",
      "Epoch 14/200\n",
      " - 12s - loss: 4.1470 - accuracy: 0.1836\n",
      "Epoch 15/200\n",
      " - 13s - loss: 3.9915 - accuracy: 0.2022\n",
      "Epoch 16/200\n",
      " - 14s - loss: 3.8281 - accuracy: 0.2202\n",
      "Epoch 17/200\n",
      " - 12s - loss: 3.6655 - accuracy: 0.2478\n",
      "Epoch 18/200\n",
      " - 15s - loss: 3.5077 - accuracy: 0.2760\n",
      "Epoch 19/200\n",
      " - 15s - loss: 3.3490 - accuracy: 0.3053\n",
      "Epoch 20/200\n",
      " - 12s - loss: 3.1957 - accuracy: 0.3334\n",
      "Epoch 21/200\n",
      " - 13s - loss: 3.0431 - accuracy: 0.3593\n",
      "Epoch 22/200\n",
      " - 12s - loss: 2.9023 - accuracy: 0.3851\n",
      "Epoch 23/200\n",
      " - 12s - loss: 2.7640 - accuracy: 0.4082\n",
      "Epoch 24/200\n",
      " - 13s - loss: 2.6398 - accuracy: 0.4360\n",
      "Epoch 25/200\n",
      " - 13s - loss: 2.5187 - accuracy: 0.4599\n",
      "Epoch 26/200\n",
      " - 13s - loss: 2.3937 - accuracy: 0.4834\n",
      "Epoch 27/200\n",
      " - 13s - loss: 2.2944 - accuracy: 0.5006\n",
      "Epoch 28/200\n",
      " - 13s - loss: 2.1828 - accuracy: 0.5263\n",
      "Epoch 29/200\n",
      " - 13s - loss: 2.0823 - accuracy: 0.5467\n",
      "Epoch 30/200\n",
      " - 12s - loss: 1.9915 - accuracy: 0.5651\n",
      "Epoch 31/200\n",
      " - 13s - loss: 1.9021 - accuracy: 0.5835\n",
      "Epoch 32/200\n",
      " - 13s - loss: 1.8196 - accuracy: 0.5987\n",
      "Epoch 33/200\n",
      " - 13s - loss: 1.7364 - accuracy: 0.6197\n",
      "Epoch 34/200\n",
      " - 13s - loss: 1.6581 - accuracy: 0.6387\n",
      "Epoch 35/200\n",
      " - 12s - loss: 1.5930 - accuracy: 0.6498\n",
      "Epoch 36/200\n",
      " - 13s - loss: 1.5202 - accuracy: 0.6685\n",
      "Epoch 37/200\n",
      " - 13s - loss: 1.4490 - accuracy: 0.6850\n",
      "Epoch 38/200\n",
      " - 13s - loss: 1.3887 - accuracy: 0.6957\n",
      "Epoch 39/200\n",
      " - 12s - loss: 1.3296 - accuracy: 0.7104\n",
      "Epoch 40/200\n",
      " - 12s - loss: 1.2602 - accuracy: 0.7301\n",
      "Epoch 41/200\n",
      " - 12s - loss: 1.2150 - accuracy: 0.7384\n",
      "Epoch 42/200\n",
      " - 12s - loss: 1.1709 - accuracy: 0.7481\n",
      "Epoch 43/200\n",
      " - 13s - loss: 1.1032 - accuracy: 0.7644\n",
      "Epoch 44/200\n",
      " - 12s - loss: 1.0518 - accuracy: 0.7770\n",
      "Epoch 45/200\n",
      " - 13s - loss: 1.0082 - accuracy: 0.7907\n",
      "Epoch 46/200\n",
      " - 12s - loss: 0.9701 - accuracy: 0.7952\n",
      "Epoch 47/200\n",
      " - 12s - loss: 0.9213 - accuracy: 0.8120\n",
      "Epoch 48/200\n",
      " - 13s - loss: 0.8742 - accuracy: 0.8249\n",
      "Epoch 49/200\n",
      " - 12s - loss: 0.8465 - accuracy: 0.8251\n",
      "Epoch 50/200\n",
      " - 13s - loss: 0.8108 - accuracy: 0.8393\n",
      "Epoch 51/200\n",
      " - 12s - loss: 0.7680 - accuracy: 0.8490\n",
      "Epoch 52/200\n",
      " - 12s - loss: 0.7157 - accuracy: 0.8639\n",
      "Epoch 53/200\n",
      " - 12s - loss: 0.7005 - accuracy: 0.8671\n",
      "Epoch 54/200\n",
      " - 12s - loss: 0.6658 - accuracy: 0.8740\n",
      "Epoch 55/200\n",
      " - 13s - loss: 0.6784 - accuracy: 0.8680\n",
      "Epoch 56/200\n",
      " - 12s - loss: 0.6052 - accuracy: 0.8898\n",
      "Epoch 57/200\n",
      " - 12s - loss: 0.5811 - accuracy: 0.8931\n",
      "Epoch 58/200\n",
      " - 12s - loss: 0.5632 - accuracy: 0.8964\n",
      "Epoch 59/200\n",
      " - 12s - loss: 0.5214 - accuracy: 0.9082\n",
      "Epoch 60/200\n",
      " - 12s - loss: 0.4942 - accuracy: 0.9154\n",
      "Epoch 61/200\n",
      " - 12s - loss: 0.4702 - accuracy: 0.9218\n",
      "Epoch 62/200\n",
      " - 12s - loss: 0.4795 - accuracy: 0.9157\n",
      "Epoch 63/200\n",
      " - 12s - loss: 0.4595 - accuracy: 0.9213\n",
      "Epoch 64/200\n",
      " - 12s - loss: 0.4196 - accuracy: 0.9327\n",
      "Epoch 65/200\n",
      " - 12s - loss: 0.4013 - accuracy: 0.9370\n",
      "Epoch 66/200\n",
      " - 12s - loss: 0.4051 - accuracy: 0.9311\n",
      "Epoch 67/200\n",
      " - 12s - loss: 0.3800 - accuracy: 0.9381\n",
      "Epoch 68/200\n",
      " - 13s - loss: 0.4064 - accuracy: 0.9290\n",
      "Epoch 69/200\n",
      " - 13s - loss: 0.3641 - accuracy: 0.9425\n",
      "Epoch 70/200\n",
      " - 12s - loss: 0.3228 - accuracy: 0.9540\n",
      "Epoch 71/200\n",
      " - 12s - loss: 0.3486 - accuracy: 0.9437\n",
      "Epoch 72/200\n",
      " - 13s - loss: 0.3679 - accuracy: 0.9375\n",
      "Epoch 73/200\n",
      " - 13s - loss: 0.3151 - accuracy: 0.9521\n",
      "Epoch 74/200\n",
      " - 13s - loss: 0.2452 - accuracy: 0.9712\n",
      "Epoch 75/200\n",
      " - 13s - loss: 0.2458 - accuracy: 0.9688\n",
      "Epoch 76/200\n",
      " - 13s - loss: 0.2373 - accuracy: 0.9733\n",
      "Epoch 77/200\n",
      " - 13s - loss: 0.2554 - accuracy: 0.9642\n",
      "Epoch 78/200\n",
      " - 13s - loss: 0.2428 - accuracy: 0.9684\n",
      "Epoch 79/200\n",
      " - 13s - loss: 0.2742 - accuracy: 0.9582\n",
      "Epoch 80/200\n",
      " - 14s - loss: 0.2235 - accuracy: 0.9697\n",
      "Epoch 81/200\n",
      " - 14s - loss: 0.3008 - accuracy: 0.9469\n",
      "Epoch 82/200\n",
      " - 16s - loss: 0.2637 - accuracy: 0.9564\n",
      "Epoch 83/200\n",
      " - 14s - loss: 0.1879 - accuracy: 0.9792\n",
      "Epoch 84/200\n",
      " - 13s - loss: 0.1538 - accuracy: 0.9886\n",
      "Epoch 85/200\n",
      " - 13s - loss: 0.1541 - accuracy: 0.9861\n",
      "Epoch 86/200\n",
      " - 13s - loss: 0.2357 - accuracy: 0.9636\n",
      "Epoch 87/200\n",
      " - 12s - loss: 0.1747 - accuracy: 0.9801\n",
      "Epoch 88/200\n",
      " - 13s - loss: 0.1896 - accuracy: 0.9741\n",
      "Epoch 89/200\n",
      " - 15s - loss: 0.1370 - accuracy: 0.9890\n",
      "Epoch 90/200\n",
      " - 13s - loss: 0.1385 - accuracy: 0.9851\n",
      "Epoch 91/200\n",
      " - 13s - loss: 0.2362 - accuracy: 0.9576\n",
      "Epoch 92/200\n",
      " - 13s - loss: 0.2105 - accuracy: 0.9655\n",
      "Epoch 93/200\n",
      " - 15s - loss: 0.1518 - accuracy: 0.9825\n",
      "Epoch 94/200\n",
      " - 17s - loss: 0.1073 - accuracy: 0.9933\n",
      "Epoch 95/200\n",
      " - 15s - loss: 0.1060 - accuracy: 0.9925\n",
      "Epoch 96/200\n",
      " - 12s - loss: 0.1327 - accuracy: 0.9851\n",
      "Epoch 97/200\n",
      " - 12s - loss: 0.2122 - accuracy: 0.9612\n",
      "Epoch 98/200\n",
      " - 12s - loss: 0.1452 - accuracy: 0.9816\n",
      "Epoch 99/200\n",
      " - 13s - loss: 0.1503 - accuracy: 0.9797\n",
      "Epoch 100/200\n",
      " - 13s - loss: 0.1267 - accuracy: 0.9852\n",
      "Epoch 101/200\n",
      " - 13s - loss: 0.1008 - accuracy: 0.9920\n",
      "Epoch 102/200\n",
      " - 13s - loss: 0.0822 - accuracy: 0.9951\n",
      "Epoch 103/200\n",
      " - 13s - loss: 0.0730 - accuracy: 0.9959\n",
      "Epoch 104/200\n",
      " - 12s - loss: 0.1476 - accuracy: 0.9775\n",
      "Epoch 105/200\n",
      " - 12s - loss: 0.2030 - accuracy: 0.9610\n",
      "Epoch 106/200\n",
      " - 12s - loss: 0.2109 - accuracy: 0.9566\n",
      "Epoch 107/200\n",
      " - 12s - loss: 0.1489 - accuracy: 0.9764\n",
      "Epoch 108/200\n",
      " - 12s - loss: 0.0730 - accuracy: 0.9955\n",
      "Epoch 109/200\n",
      " - 13s - loss: 0.0560 - accuracy: 0.9976\n",
      "Epoch 110/200\n",
      " - 13s - loss: 0.0441 - accuracy: 0.9992\n",
      "Epoch 111/200\n",
      " - 12s - loss: 0.0319 - accuracy: 0.9999\n",
      "Epoch 112/200\n",
      " - 12s - loss: 0.0289 - accuracy: 0.9997\n",
      "Epoch 113/200\n",
      " - 12s - loss: 0.3953 - accuracy: 0.8939\n",
      "Epoch 114/200\n",
      " - 12s - loss: 0.3257 - accuracy: 0.9171\n",
      "Epoch 115/200\n",
      " - 12s - loss: 0.1306 - accuracy: 0.9807\n",
      "Epoch 116/200\n",
      " - 12s - loss: 0.0770 - accuracy: 0.9945\n",
      "Epoch 117/200\n",
      " - 12s - loss: 0.0510 - accuracy: 0.9977\n",
      "Epoch 118/200\n",
      " - 12s - loss: 0.0389 - accuracy: 0.9991\n",
      "Epoch 119/200\n",
      " - 12s - loss: 0.0266 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      " - 12s - loss: 0.0210 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      " - 12s - loss: 0.0222 - accuracy: 0.9997\n",
      "Epoch 122/200\n",
      " - 12s - loss: 0.7109 - accuracy: 0.8095\n",
      "Epoch 123/200\n",
      " - 12s - loss: 0.2647 - accuracy: 0.9362\n",
      "Epoch 124/200\n",
      " - 12s - loss: 0.1445 - accuracy: 0.9752\n",
      "Epoch 125/200\n",
      " - 12s - loss: 0.0861 - accuracy: 0.9911\n",
      "Epoch 126/200\n",
      " - 12s - loss: 0.0618 - accuracy: 0.9957\n",
      "Epoch 127/200\n",
      " - 13s - loss: 0.0563 - accuracy: 0.9965\n",
      "Epoch 128/200\n",
      " - 13s - loss: 0.1069 - accuracy: 0.9821\n",
      "Epoch 129/200\n",
      " - 13s - loss: 0.2013 - accuracy: 0.9535\n",
      "Epoch 130/200\n",
      " - 13s - loss: 0.1125 - accuracy: 0.9834\n",
      "Epoch 00130: early stopping\n"
     ]
    }
   ],
   "source": [
    "# LSTM with word embedding - dimension: 50, 100, 200\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "embedSize = [5]\n",
    "for dim in embedSize:\n",
    "    test2 = LSTM_word()\n",
    "    test2.SonnetLoader('shakespeare')\n",
    "    test2.getTrainSeq()\n",
    "    test2.getMapping()\n",
    "    test2.Train(useWordEmbedding = True, embeddingSize = dim, numEpoch = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature:  1.5 , embedding size:  5 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now love is thine as he takes from you \n",
      " then can i see what you with your true \n",
      " to love your self with keeps love's still \n",
      " sweet life so much sweet self should from hate \n",
      " that all i swear yet to death a alone \n",
      " but him for her and by well i lose thee \n",
      " those long i'll child for my love not \n",
      " though i in love loves gracious a common kind \n",
      " be as thy proud thy self thy constancy \n",
      " to live thy self thy self i will \n",
      " make thee a heart even when thy graces \n",
      " the perfect of my will \n",
      " a most time jewel out out \n",
      " now in thee \n",
      "\n",
      "temperature:  1 , embedding size:  5 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now love is thine as he takes from you \n",
      " then can i see what you with your true \n",
      " to love your self with keeps love's still \n",
      " sweet life though this nature use doth grow \n",
      " and yet to be it being with be \n",
      " that you thy love even keeps to give \n",
      " that is to outlive long date \n",
      " when to have eyes of upon \n",
      " or time doth have a satire of praise \n",
      " and make make time's spoils despised everywhere \n",
      " give my love fame faster than time wastes life \n",
      " so thou prevent'st black save and my part \n",
      " \n",
      " that i have though not must be hide \n",
      " like tender old to be i \n",
      "\n",
      "temperature:  0.75 , embedding size:  5 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now his beauty shall still you for so \n",
      " and all my best is to this all one \n",
      " and i an see them with my pen \n",
      " therefore all my love and from my life hath end \n",
      " i find sweet love though my love is not \n",
      " so thou no mother's that i am both \n",
      " as high to lose my purpose disgrace \n",
      " till my heart knows all present eye's eye's \n",
      " and for my love for thee is so fair \n",
      " that i am not with present nor to be \n",
      " and he that might i not my love be \n",
      " as if thou not becoming this nor be \n",
      " not thee i not yet i am not \n",
      "\n",
      "temperature:  0.25 , embedding size:  5 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now his beauty shall still you for so \n",
      " and all my best is to this all one \n",
      " and i an accessary needs must be \n",
      " that i will not to be with thee \n",
      " of him thy self that i do thee more \n",
      " then worthy i love my self thou dost be \n",
      " the ten of thine for happier be i whilst \n",
      " thou gav'st the mother's part i am love \n",
      " as those gold candles fixed in thee air \n",
      " lose all of more that i may see \n",
      " and this true love is love still thee and so so \n",
      " for love that love i will in thee \n",
      " by mine eyes thy fair truth wouldst to \n",
      "\n",
      "temperature:  1.5 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now love is thine as he takes from you \n",
      " then can i see what you with your true \n",
      " to love your self with keeps love's still \n",
      " sweet life so much sweet self should from hate \n",
      " that all i swear yet to death a alone \n",
      " but him for her and by well i lose thee \n",
      " those long i'll child for my love not \n",
      " though i in love loves gracious a common kind \n",
      " be as thy proud thy self thy constancy \n",
      " to live thy self thy self i will \n",
      " make thee a heart even when thy graces \n",
      " the perfect of my will \n",
      " a most time jewel out out \n",
      " now in thee \n",
      "\n",
      "temperature:  1 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now love is thine as he takes from you \n",
      " then can i see what you with your true \n",
      " to love your self with keeps love's still \n",
      " sweet life though this nature use doth grow \n",
      " and yet to be it being with be \n",
      " that you thy love even keeps to give \n",
      " that is to outlive long date \n",
      " when to have eyes of upon \n",
      " or time doth have a satire of praise \n",
      " and make make time's spoils despised everywhere \n",
      " give my love fame faster than time wastes life \n",
      " so thou prevent'st black save and my part \n",
      " \n",
      " that i have though not must be hide \n",
      " like tender old to be i \n",
      "\n",
      "temperature:  0.75 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now his beauty shall still you for so \n",
      " and all my best is to this all one \n",
      " and i an see them with my pen \n",
      " therefore all my love and from my life hath end \n",
      " i find sweet love though my love is not \n",
      " so thou no mother's that i am both \n",
      " as high to lose my purpose disgrace \n",
      " till my heart knows all present eye's eye's \n",
      " and for my love for thee is so fair \n",
      " that i am not with present nor to be \n",
      " and he that might i not my love be \n",
      " as if thou not becoming this nor be \n",
      " not thee i not yet i am not \n",
      "\n",
      "temperature:  0.25 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now his beauty shall still you for so \n",
      " and all my best is to this all one \n",
      " and i an accessary needs must be \n",
      " that i will not to be with thee \n",
      " of him thy self that i do thee more \n",
      " then worthy i love my self thou dost be \n",
      " the ten of thine for happier be i whilst \n",
      " thou gav'st the mother's part i am love \n",
      " as those gold candles fixed in thee air \n",
      " lose all of more that i may see \n",
      " and this true love is love still thee and so so \n",
      " for love that love i will in thee \n",
      " by mine eyes thy fair truth wouldst to \n",
      "\n",
      "temperature:  1.5 , embedding size:  25 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now love is thine as he takes from you \n",
      " then can i see what you with your true \n",
      " to love your self with keeps love's still \n",
      " sweet life so much sweet self should from hate \n",
      " that all i swear yet to death a alone \n",
      " but him for her and by well i lose thee \n",
      " those long i'll child for my love not \n",
      " though i in love loves gracious a common kind \n",
      " be as thy proud thy self thy constancy \n",
      " to live thy self thy self i will \n",
      " make thee a heart even when thy graces \n",
      " the perfect of my will \n",
      " a most time jewel out out \n",
      " now in thee \n",
      "\n",
      "temperature:  1 , embedding size:  25 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now love is thine as he takes from you \n",
      " then can i see what you with your true \n",
      " to love your self with keeps love's still \n",
      " sweet life though this nature use doth grow \n",
      " and yet to be it being with be \n",
      " that you thy love even keeps to give \n",
      " that is to outlive long date \n",
      " when to have eyes of upon \n",
      " or time doth have a satire of praise \n",
      " and make make time's spoils despised everywhere \n",
      " give my love fame faster than time wastes life \n",
      " so thou prevent'st black save and my part \n",
      " \n",
      " that i have though not must be hide \n",
      " like tender old to be i \n",
      "\n",
      "temperature:  0.75 , embedding size:  25 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now his beauty shall still you for so \n",
      " and all my best is to this all one \n",
      " and i an see them with my pen \n",
      " therefore all my love and from my life hath end \n",
      " i find sweet love though my love is not \n",
      " so thou no mother's that i am both \n",
      " as high to lose my purpose disgrace \n",
      " till my heart knows all present eye's eye's \n",
      " and for my love for thee is so fair \n",
      " that i am not with present nor to be \n",
      " and he that might i not my love be \n",
      " as if thou not becoming this nor be \n",
      " not thee i not yet i am not \n",
      "\n",
      "temperature:  0.25 , embedding size:  25 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now his beauty shall still you for so \n",
      " and all my best is to this all one \n",
      " and i an accessary needs must be \n",
      " that i will not to be with thee \n",
      " of him thy self that i do thee more \n",
      " then worthy i love my self thou dost be \n",
      " the ten of thine for happier be i whilst \n",
      " thou gav'st the mother's part i am love \n",
      " as those gold candles fixed in thee air \n",
      " lose all of more that i may see \n",
      " and this true love is love still thee and so so \n",
      " for love that love i will in thee \n",
      " by mine eyes thy fair truth wouldst to \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "embedSize = [5, 10, 25]\n",
    "for dim in embedSize:\n",
    "    modelName = \"model_withWordEmbedding %d.h5\" % dim\n",
    "    mappingName = \"mapping_withWordEmbedding %d.pk1\" % dim\n",
    "    test2 = LSTM_word()\n",
    "    test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "    tempList = [1.5, 1, 0.75, 0.25]\n",
    "    predicted = [test2.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=150, temperature = x) for x in tempList]\n",
    "    for i, x in enumerate(predicted):\n",
    "        print('temperature: ', tempList[i], ', embedding size: ', dim, '\\n', x, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to C:\\Users\\HyeongChan\n",
      "[nltk_data]     Jo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong sentence:  ['your', 'more', 'let', 'you', 'let', 'he', 'look', 'fear']\n",
      "Wrong sentence:  ['your', 'more', 'let', 'you', 'let', 'that', 'look', 'fear']\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'haply', 'judgment', 'judgment', 'watchman', 'judgment']\n",
      "Wrong sentence:  ['happy', 'say', 'look', 'so', 'death', 'look', 'still', 'say', 'now', 'now']\n",
      "Wrong sentence:  ['your', 'more', 'let', 'you', 'let', 'that', 'look', 'fear']\n",
      "Completed sentence:  ['look', 'death', 'death', 'death', 'death', 'death', 'death', 'look', 'look', 'look', 'death']\n",
      "Completed sentence:  ['death', 'death', 'death', 'look', 'death', 'death', 'look', 'death', 'trust', 'look', 'look']\n",
      "Completed sentence:  ['look', 'death', 'death', 'death', 'death', 'death', 'death', 'death', 'death', 'death', 'death']\n",
      "Completed sentence:  ['death', 'death', 'death', 'death', 'death', 'death', 'look', 'death', 'death', 'death', 'death']\n",
      "Completed sentence:  ['death', 'death', 'death', 'death', 'death', 'death', 'death', 'death', 'death', 'look', 'death']\n",
      "Completed sentence:  ['look', 'look', 'death', 'death', 'death', 'look', 'death', 'death', 'death', 'death', 'death']\n",
      "Completed sentence:  ['death', 'death', 'death', 'death', 'death', 'death', 'death', 'death', 'death', 'death', 'death']\n",
      "Completed sentence:  ['death', 'death', 'death', 'death', 'death', 'death', 'death', 'death', 'death', 'death', 'death']\n",
      "Completed sentence:  ['death', 'death', 'death', 'death', 'look', 'look', 'death', 'look', 'death', 'death', 'death']\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'judgment', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['your', 'say', 'death', 'let', 'your', 'death', 'might', 'your', 'show']\n",
      "Wrong sentence:  ['happy', 'say', 'look', 'look', 'death', 'look', 'better', 'trust', 'had', 'had']\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'judgment', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['that', 'i', 'see', 'on', 'now', 'now', 'they', 'see']\n",
      "Wrong sentence:  ['your', 'more', 'let', 'you', 'let', 'that', 'look', 'fear']\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'haply', 'judgment', 'judgment', 'watchman', 'judgment']\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'lawful', 'calls', 'nimble', 'noted', 'any', 'knowledge']\n",
      "Wrong sentence:  ['your', 'more', 'let', 'you', 'let', 'that', 'look', 'fear']\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'judgment', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'haply', 'judgment', 'judgment', 'watchman', 'user']\n",
      "Wrong sentence:  ['your', 'more', 'let', 'your', 'love', 'look', 'look', 'your', 'show']\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'judgment', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'statute', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'judgment', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'judgment', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'judgment', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'statute', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'haply', 'judgment', 'judgment', 'watchman', 'judgment']\n",
      "Wrong sentence:  ['your', 'more', 'let', 'your', 'love', 'look', 'look', 'your', 'show']\n",
      "Wrong sentence:  ['that', 'i', 'see', 'on', 'such', 'now', 'they', 'see']\n",
      "Wrong sentence:  ['your', 'more', 'let', 'your', 'love', 'look', 'look', 'your', 'show']\n",
      "Wrong sentence:  ['that', 'i', 'see', 'on', 'now', 'now', 'they', 'see']\n",
      "Wrong sentence:  ['your', 'more', 'let', 'you', 'let', 'look', 'look', 'be']\n",
      "Wrong sentence:  ['your', 'more', 'let', 'you', 'let', 'now', 'look', 'nature']\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'judgment', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['your', 'more', 'let', 'you', 'let', 'that', 'look', 'fear']\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'judgment', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'haply', 'judgment', 'judgment', 'watchman', 'user']\n",
      "Completed sentence:  ['your', 'say', 'let', 'your', 'death', 'look', 'your', 'show', 'had', 'now', 'who']\n",
      "Completed sentence:  ['all', 'all', 'best', 'best', 'best', 'have', 'death', 'now', 'have', 'seen', 'one']\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'haply', 'judgment', 'judgment', 'watchman', 'judgment']\n",
      "Wrong sentence:  ['that', 'i', 'see', 'on', 'such', 'now', 'they', 'see']\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'statute', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'statute', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'statute', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'statute', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['your', 'more', 'let', 'your', 'love', 'look', 'look', 'your', 'show']\n",
      "Wrong sentence:  ['that', 'i', 'see', 'on', 'such', 'now', 'they', 'see']\n",
      "Wrong sentence:  ['charge', 'knowledge', 'charge', \"shadow's\"]\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'haply', 'judgment', 'judgment', 'watchman', 'user']\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'judgment', 'judgment', 'seeting', \"lour'st\"]\n",
      "Wrong sentence:  ['that', 'i', 'see', 'on', 'thee', 'which', 'beauty', 'bring']\n",
      "Wrong sentence:  ['that', 'i', 'see', 'on', 'thee', 'which', 'beauty', 'bring']\n",
      "Wrong sentence:  ['your', 'more', 'let', 'your', 'love', 'look', 'look', 'your', 'show']\n",
      "Wrong sentence:  ['your', 'more', 'let', 'your', 'love', 'look', 'look', 'your', 'show']\n",
      "Wrong sentence:  ['measure', 'knowledge', 'charge', 'charge', 'sword', 'prisoner', 'judgment', 'judgment', 'seeting', \"lour'st\"]\n",
      "temperature:  1.5 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " look death death death death death death look look look death death death death look death death look death trust look look look death death death death death death death death death death death death death death death death look death death death death death death death death death death death death death look death look look death death death look death death death death death death death death death death death death death death death death death death death death death death death death death death death death death death death look look death look death death death death death death death death death death \n",
      "\n",
      "temperature:  1 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " your more let you let that look fear \n",
      "\n",
      "temperature:  0.75 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " your say let your death look your show had now who all all best best best have death now have seen one \n",
      "\n",
      "temperature:  0.25 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee now they \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with checking petmeter\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "embedSize = [10]\n",
    "for dim in embedSize:\n",
    "    modelName = \"model_withWordEmbedding %d.h5\" % dim\n",
    "    mappingName = \"mapping_withWordEmbedding %d.pk1\" % dim\n",
    "    test2 = LSTM_word()\n",
    "    test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "    test2.SonnetLoader('shakespeare')\n",
    "    tempList = [1.5, 1, 0.75, 0.25]\n",
    "    predicted = [test2.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=150, temperature = x, checkPentameter = True) for x in tempList]\n",
    "    for i, x in enumerate(predicted):\n",
    "        print('temperature: ', tempList[i], ', embedding size: ', dim, '\\n', x, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88840\n",
      "2221\n",
      "16719688\n",
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 10)            22210     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               168800    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 637,431\n",
      "Trainable params: 637,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 8s - loss: 6.3680 - accuracy: 0.1154\n",
      "Epoch 2/200\n",
      " - 8s - loss: 5.8863 - accuracy: 0.1164\n",
      "Epoch 3/200\n",
      " - 7s - loss: 5.7987 - accuracy: 0.1157\n",
      "Epoch 4/200\n",
      " - 8s - loss: 5.7009 - accuracy: 0.1164\n",
      "Epoch 5/200\n",
      " - 8s - loss: 5.5830 - accuracy: 0.1223\n",
      "Epoch 6/200\n",
      " - 8s - loss: 5.4464 - accuracy: 0.1324\n",
      "Epoch 7/200\n",
      " - 7s - loss: 5.3198 - accuracy: 0.1420\n",
      "Epoch 8/200\n",
      " - 8s - loss: 5.2006 - accuracy: 0.1463\n",
      "Epoch 9/200\n",
      " - 7s - loss: 5.0907 - accuracy: 0.1496\n",
      "Epoch 10/200\n",
      " - 8s - loss: 4.9721 - accuracy: 0.1549\n",
      "Epoch 11/200\n",
      " - 7s - loss: 4.8563 - accuracy: 0.1567\n",
      "Epoch 12/200\n",
      " - 7s - loss: 4.7369 - accuracy: 0.1609\n",
      "Epoch 13/200\n",
      " - 8s - loss: 4.6212 - accuracy: 0.1629\n",
      "Epoch 14/200\n",
      " - 8s - loss: 4.5077 - accuracy: 0.1675\n",
      "Epoch 15/200\n",
      " - 7s - loss: 4.3928 - accuracy: 0.1680\n",
      "Epoch 16/200\n",
      " - 7s - loss: 4.2755 - accuracy: 0.1743\n",
      "Epoch 17/200\n",
      " - 7s - loss: 4.1605 - accuracy: 0.1793\n",
      "Epoch 18/200\n",
      " - 7s - loss: 4.0443 - accuracy: 0.1864\n",
      "Epoch 19/200\n",
      " - 7s - loss: 3.9300 - accuracy: 0.1962\n",
      "Epoch 20/200\n",
      " - 7s - loss: 3.8201 - accuracy: 0.2099\n",
      "Epoch 21/200\n",
      " - 8s - loss: 3.7053 - accuracy: 0.2256\n",
      "Epoch 22/200\n",
      " - 7s - loss: 3.5945 - accuracy: 0.2411\n",
      "Epoch 23/200\n",
      " - 7s - loss: 3.4772 - accuracy: 0.2661\n",
      "Epoch 24/200\n",
      " - 7s - loss: 3.3609 - accuracy: 0.2892\n",
      "Epoch 25/200\n",
      " - 7s - loss: 3.2500 - accuracy: 0.3087\n",
      "Epoch 26/200\n",
      " - 7s - loss: 3.1425 - accuracy: 0.3297\n",
      "Epoch 27/200\n",
      " - 7s - loss: 3.0361 - accuracy: 0.3467\n",
      "Epoch 28/200\n",
      " - 7s - loss: 2.9284 - accuracy: 0.3666\n",
      "Epoch 29/200\n",
      " - 7s - loss: 2.8244 - accuracy: 0.3907\n",
      "Epoch 30/200\n",
      " - 7s - loss: 2.7224 - accuracy: 0.4151\n",
      "Epoch 31/200\n",
      " - 7s - loss: 2.6284 - accuracy: 0.4341\n",
      "Epoch 32/200\n",
      " - 7s - loss: 2.5320 - accuracy: 0.4555\n",
      "Epoch 33/200\n",
      " - 7s - loss: 2.4351 - accuracy: 0.4781\n",
      "Epoch 34/200\n",
      " - 7s - loss: 2.3452 - accuracy: 0.4989\n",
      "Epoch 35/200\n",
      " - 7s - loss: 2.2617 - accuracy: 0.5139\n",
      "Epoch 36/200\n",
      " - 7s - loss: 2.1753 - accuracy: 0.5337\n",
      "Epoch 37/200\n",
      " - 8s - loss: 2.0918 - accuracy: 0.5489\n",
      "Epoch 38/200\n",
      " - 7s - loss: 2.0105 - accuracy: 0.5705\n",
      "Epoch 39/200\n",
      " - 7s - loss: 1.9333 - accuracy: 0.5871\n",
      "Epoch 40/200\n",
      " - 7s - loss: 1.8649 - accuracy: 0.6010\n",
      "Epoch 41/200\n",
      " - 7s - loss: 1.7877 - accuracy: 0.6176\n",
      "Epoch 42/200\n",
      " - 7s - loss: 1.7226 - accuracy: 0.6344\n",
      "Epoch 43/200\n",
      " - 7s - loss: 1.6595 - accuracy: 0.6451\n",
      "Epoch 44/200\n",
      " - 7s - loss: 1.5981 - accuracy: 0.6627\n",
      "Epoch 45/200\n",
      " - 7s - loss: 1.5337 - accuracy: 0.6775\n",
      "Epoch 46/200\n",
      " - 7s - loss: 1.4850 - accuracy: 0.6925\n",
      "Epoch 47/200\n",
      " - 7s - loss: 1.4224 - accuracy: 0.7060\n",
      "Epoch 48/200\n",
      " - 7s - loss: 1.3679 - accuracy: 0.7156\n",
      "Epoch 49/200\n",
      " - 7s - loss: 1.3144 - accuracy: 0.7291\n",
      "Epoch 50/200\n",
      " - 7s - loss: 1.2690 - accuracy: 0.7431\n",
      "Epoch 51/200\n",
      " - 7s - loss: 1.2175 - accuracy: 0.7525\n",
      "Epoch 52/200\n",
      " - 7s - loss: 1.1686 - accuracy: 0.7695\n",
      "Epoch 53/200\n",
      " - 7s - loss: 1.1435 - accuracy: 0.7669\n",
      "Epoch 54/200\n",
      " - 7s - loss: 1.0959 - accuracy: 0.7788\n",
      "Epoch 55/200\n",
      " - 7s - loss: 1.0491 - accuracy: 0.7932\n",
      "Epoch 56/200\n",
      " - 7s - loss: 0.9963 - accuracy: 0.8090\n",
      "Epoch 57/200\n",
      " - 7s - loss: 0.9623 - accuracy: 0.8108\n",
      "Epoch 58/200\n",
      " - 7s - loss: 1.3742 - accuracy: 0.7289\n",
      "Epoch 59/200\n",
      " - 7s - loss: 0.9930 - accuracy: 0.8049\n",
      "Epoch 60/200\n",
      " - 8s - loss: 0.8939 - accuracy: 0.8288\n",
      "Epoch 61/200\n",
      " - 7s - loss: 0.8459 - accuracy: 0.8394\n",
      "Epoch 62/200\n",
      " - 8s - loss: 0.8087 - accuracy: 0.8499\n",
      "Epoch 63/200\n",
      " - 7s - loss: 0.7932 - accuracy: 0.8536\n",
      "Epoch 64/200\n",
      " - 7s - loss: 0.7725 - accuracy: 0.8572\n",
      "Epoch 65/200\n",
      " - 7s - loss: 0.7315 - accuracy: 0.8706\n",
      "Epoch 66/200\n",
      " - 7s - loss: 0.6903 - accuracy: 0.8818\n",
      "Epoch 67/200\n",
      " - 7s - loss: 1.3749 - accuracy: 0.7483\n",
      "Epoch 68/200\n",
      " - 7s - loss: 1.3052 - accuracy: 0.6904\n",
      "Epoch 69/200\n",
      " - 7s - loss: 0.8756 - accuracy: 0.8166\n",
      "Epoch 70/200\n",
      " - 7s - loss: 0.7435 - accuracy: 0.8600\n",
      "Epoch 71/200\n",
      " - 7s - loss: 0.6804 - accuracy: 0.8789\n",
      "Epoch 72/200\n",
      " - 7s - loss: 0.6346 - accuracy: 0.8912\n",
      "Epoch 73/200\n",
      " - 6s - loss: 0.5996 - accuracy: 0.8998\n",
      "Epoch 74/200\n",
      " - 6s - loss: 0.5696 - accuracy: 0.9070\n",
      "Epoch 75/200\n",
      " - 7s - loss: 0.5457 - accuracy: 0.9114\n",
      "Epoch 76/200\n",
      " - 7s - loss: 0.5201 - accuracy: 0.9195\n",
      "Epoch 77/200\n",
      " - 7s - loss: 0.5019 - accuracy: 0.9235\n",
      "Epoch 78/200\n",
      " - 7s - loss: 0.4820 - accuracy: 0.9273\n",
      "Epoch 79/200\n",
      " - 7s - loss: 0.4715 - accuracy: 0.9296\n",
      "Epoch 80/200\n",
      " - 7s - loss: 0.4487 - accuracy: 0.9342\n",
      "Epoch 81/200\n",
      " - 7s - loss: 0.4302 - accuracy: 0.9392\n",
      "Epoch 82/200\n",
      " - 7s - loss: 0.4151 - accuracy: 0.9445\n",
      "Epoch 83/200\n",
      " - 7s - loss: 0.3981 - accuracy: 0.9490\n",
      "Epoch 84/200\n",
      " - 7s - loss: 0.3835 - accuracy: 0.9469\n",
      "Epoch 85/200\n",
      " - 7s - loss: 0.3718 - accuracy: 0.9515\n",
      "Epoch 86/200\n",
      " - 7s - loss: 0.3457 - accuracy: 0.9575\n",
      "Epoch 87/200\n",
      " - 7s - loss: 0.3467 - accuracy: 0.9552\n",
      "Epoch 88/200\n",
      " - 7s - loss: 0.3314 - accuracy: 0.9608\n",
      "Epoch 89/200\n",
      " - 7s - loss: 0.3657 - accuracy: 0.9526\n",
      "Epoch 90/200\n",
      " - 7s - loss: 0.3158 - accuracy: 0.9612\n",
      "Epoch 91/200\n",
      " - 7s - loss: 0.2913 - accuracy: 0.9690\n",
      "Epoch 92/200\n",
      " - 7s - loss: 0.2719 - accuracy: 0.9728\n",
      "Epoch 93/200\n",
      " - 7s - loss: 0.2614 - accuracy: 0.9742\n",
      "Epoch 94/200\n",
      " - 7s - loss: 0.2498 - accuracy: 0.9748\n",
      "Epoch 95/200\n",
      " - 7s - loss: 0.2373 - accuracy: 0.9805\n",
      "Epoch 96/200\n",
      " - 7s - loss: 0.2212 - accuracy: 0.9838\n",
      "Epoch 97/200\n",
      " - 7s - loss: 0.2113 - accuracy: 0.9849\n",
      "Epoch 98/200\n",
      " - 7s - loss: 0.2346 - accuracy: 0.9768\n",
      "Epoch 99/200\n",
      " - 6s - loss: 0.2493 - accuracy: 0.9741\n",
      "Epoch 100/200\n",
      " - 7s - loss: 0.2020 - accuracy: 0.9880\n",
      "Epoch 101/200\n",
      " - 7s - loss: 0.1705 - accuracy: 0.9907\n",
      "Epoch 102/200\n",
      " - 7s - loss: 0.1641 - accuracy: 0.9926\n",
      "Epoch 103/200\n",
      " - 7s - loss: 0.1417 - accuracy: 0.9948\n",
      "Epoch 104/200\n",
      " - 7s - loss: 0.1313 - accuracy: 0.9968\n",
      "Epoch 105/200\n",
      " - 7s - loss: 0.1306 - accuracy: 0.9963\n",
      "Epoch 106/200\n",
      " - 7s - loss: 0.1325 - accuracy: 0.9950\n",
      "Epoch 107/200\n",
      " - 7s - loss: 0.1199 - accuracy: 0.9967\n",
      "Epoch 108/200\n",
      " - 7s - loss: 0.1245 - accuracy: 0.9952\n",
      "Epoch 109/200\n",
      " - 7s - loss: 0.1095 - accuracy: 0.9976\n",
      "Epoch 110/200\n",
      " - 10s - loss: 0.1040 - accuracy: 0.9980\n",
      "Epoch 111/200\n",
      " - 7s - loss: 0.0910 - accuracy: 0.9981\n",
      "Epoch 112/200\n",
      " - 7s - loss: 0.0805 - accuracy: 0.9992\n",
      "Epoch 113/200\n",
      " - 7s - loss: 0.0716 - accuracy: 0.9996\n",
      "Epoch 114/200\n",
      " - 7s - loss: 0.0675 - accuracy: 0.9997\n",
      "Epoch 115/200\n",
      " - 7s - loss: 0.0646 - accuracy: 0.9996\n",
      "Epoch 116/200\n",
      " - 7s - loss: 0.0815 - accuracy: 0.9983\n",
      "Epoch 117/200\n",
      " - 7s - loss: 0.2298 - accuracy: 0.9605\n",
      "Epoch 118/200\n",
      " - 7s - loss: 0.1834 - accuracy: 0.9778\n",
      "Epoch 119/200\n",
      " - 7s - loss: 0.0850 - accuracy: 0.9965\n",
      "Epoch 120/200\n",
      " - 7s - loss: 0.0559 - accuracy: 0.9996\n",
      "Epoch 121/200\n",
      " - 7s - loss: 0.1016 - accuracy: 0.9918\n",
      "Epoch 122/200\n",
      " - 7s - loss: 0.0767 - accuracy: 0.9973\n",
      "Epoch 123/200\n",
      " - 7s - loss: 0.0505 - accuracy: 0.9993\n",
      "Epoch 124/200\n",
      " - 8s - loss: 0.0407 - accuracy: 0.9997\n",
      "Epoch 125/200\n",
      " - 7s - loss: 0.0358 - accuracy: 0.9997\n",
      "Epoch 126/200\n",
      " - 8s - loss: 0.0328 - accuracy: 0.9997\n",
      "Epoch 127/200\n",
      " - 8s - loss: 0.0316 - accuracy: 0.9995\n",
      "Epoch 128/200\n",
      " - 7s - loss: 0.0342 - accuracy: 0.9997\n",
      "Epoch 129/200\n",
      " - 8s - loss: 0.0349 - accuracy: 0.9997\n",
      "Epoch 130/200\n",
      " - 7s - loss: 0.0321 - accuracy: 0.9996\n",
      "Epoch 131/200\n",
      " - 7s - loss: 0.0290 - accuracy: 0.9996\n",
      "Epoch 132/200\n",
      " - 7s - loss: 0.0239 - accuracy: 0.9999\n",
      "Epoch 133/200\n",
      " - 7s - loss: 0.0219 - accuracy: 0.9996\n",
      "Epoch 134/200\n",
      " - 7s - loss: 0.0197 - accuracy: 0.9999\n",
      "Epoch 135/200\n",
      " - 7s - loss: 0.0189 - accuracy: 0.9999\n",
      "Epoch 136/200\n",
      " - 7s - loss: 0.0188 - accuracy: 0.9999\n",
      "Epoch 137/200\n",
      " - 7s - loss: 0.0200 - accuracy: 0.9999\n",
      "Epoch 138/200\n",
      " - 7s - loss: 0.6735 - accuracy: 0.8207\n",
      "Epoch 139/200\n",
      " - 7s - loss: 0.3186 - accuracy: 0.9206\n",
      "Epoch 140/200\n",
      " - 7s - loss: 0.0979 - accuracy: 0.9902\n",
      "Epoch 141/200\n",
      " - 7s - loss: 0.0560 - accuracy: 0.9988\n",
      "Epoch 142/200\n",
      " - 7s - loss: 0.0429 - accuracy: 0.9996\n",
      "Epoch 143/200\n",
      " - 7s - loss: 0.0353 - accuracy: 0.9997\n",
      "Epoch 144/200\n",
      " - 7s - loss: 0.0309 - accuracy: 0.9997\n",
      "Epoch 145/200\n",
      " - 7s - loss: 0.0274 - accuracy: 0.9999\n",
      "Epoch 146/200\n",
      " - 7s - loss: 0.0247 - accuracy: 0.9999\n",
      "Epoch 00146: early stopping\n",
      "88840\n",
      "2221\n",
      "16719688\n",
      "Model: \"sequential_92\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 50)            111050    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 200)               200800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 758,271\n",
      "Trainable params: 758,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 9s - loss: 6.3613 - accuracy: 0.1154\n",
      "Epoch 2/200\n",
      " - 8s - loss: 5.8266 - accuracy: 0.1164\n",
      "Epoch 3/200\n",
      " - 8s - loss: 5.6058 - accuracy: 0.1289\n",
      "Epoch 4/200\n",
      " - 8s - loss: 5.3746 - accuracy: 0.1461\n",
      "Epoch 5/200\n",
      " - 8s - loss: 5.2112 - accuracy: 0.1540\n",
      "Epoch 6/200\n",
      " - 9s - loss: 5.0568 - accuracy: 0.1585\n",
      "Epoch 7/200\n",
      " - 9s - loss: 4.9136 - accuracy: 0.1626\n",
      "Epoch 8/200\n",
      " - 8s - loss: 4.7728 - accuracy: 0.1683\n",
      "Epoch 9/200\n",
      " - 9s - loss: 4.6291 - accuracy: 0.1727\n",
      "Epoch 10/200\n",
      " - 8s - loss: 4.4933 - accuracy: 0.1767\n",
      "Epoch 11/200\n",
      " - 9s - loss: 4.3543 - accuracy: 0.1784\n",
      "Epoch 12/200\n",
      " - 8s - loss: 4.2175 - accuracy: 0.1853\n",
      "Epoch 13/200\n",
      " - 8s - loss: 4.0787 - accuracy: 0.1953\n",
      "Epoch 14/200\n",
      " - 9s - loss: 3.9408 - accuracy: 0.2105\n",
      "Epoch 15/200\n",
      " - 9s - loss: 3.8013 - accuracy: 0.2163\n",
      "Epoch 16/200\n",
      " - 8s - loss: 3.6607 - accuracy: 0.2387\n",
      "Epoch 17/200\n",
      " - 8s - loss: 3.5252 - accuracy: 0.2602\n",
      "Epoch 18/200\n",
      " - 9s - loss: 3.3902 - accuracy: 0.2865\n",
      "Epoch 19/200\n",
      " - 9s - loss: 3.2471 - accuracy: 0.3197\n",
      "Epoch 20/200\n",
      " - 8s - loss: 3.1109 - accuracy: 0.3459\n",
      "Epoch 21/200\n",
      " - 9s - loss: 2.9699 - accuracy: 0.3750\n",
      "Epoch 22/200\n",
      " - 9s - loss: 2.8369 - accuracy: 0.4009\n",
      "Epoch 23/200\n",
      " - 9s - loss: 2.7072 - accuracy: 0.4272\n",
      "Epoch 24/200\n",
      " - 8s - loss: 2.5766 - accuracy: 0.4578\n",
      "Epoch 25/200\n",
      " - 8s - loss: 2.4589 - accuracy: 0.4802\n",
      "Epoch 26/200\n",
      " - 8s - loss: 2.3365 - accuracy: 0.5045\n",
      "Epoch 27/200\n",
      " - 9s - loss: 2.2220 - accuracy: 0.5264\n",
      "Epoch 28/200\n",
      " - 9s - loss: 2.1086 - accuracy: 0.5590\n",
      "Epoch 29/200\n",
      " - 8s - loss: 2.0076 - accuracy: 0.5781\n",
      "Epoch 30/200\n",
      " - 8s - loss: 1.8999 - accuracy: 0.6048\n",
      "Epoch 31/200\n",
      " - 8s - loss: 1.8064 - accuracy: 0.6226\n",
      "Epoch 32/200\n",
      " - 8s - loss: 1.7096 - accuracy: 0.6460\n",
      "Epoch 33/200\n",
      " - 9s - loss: 1.6203 - accuracy: 0.6679\n",
      "Epoch 34/200\n",
      " - 9s - loss: 1.5278 - accuracy: 0.6909\n",
      "Epoch 35/200\n",
      " - 9s - loss: 1.4473 - accuracy: 0.7104\n",
      "Epoch 36/200\n",
      " - 9s - loss: 1.3653 - accuracy: 0.7264\n",
      "Epoch 37/200\n",
      " - 9s - loss: 1.2880 - accuracy: 0.7487\n",
      "Epoch 38/200\n",
      " - 8s - loss: 1.2175 - accuracy: 0.7639\n",
      "Epoch 39/200\n",
      " - 8s - loss: 1.1421 - accuracy: 0.7836\n",
      "Epoch 40/200\n",
      " - 9s - loss: 1.0743 - accuracy: 0.8014\n",
      "Epoch 41/200\n",
      " - 8s - loss: 1.0096 - accuracy: 0.8151\n",
      "Epoch 42/200\n",
      " - 9s - loss: 0.9452 - accuracy: 0.8293\n",
      "Epoch 43/200\n",
      " - 8s - loss: 0.8870 - accuracy: 0.8442\n",
      "Epoch 44/200\n",
      " - 8s - loss: 0.8279 - accuracy: 0.8576\n",
      "Epoch 45/200\n",
      " - 8s - loss: 0.7725 - accuracy: 0.8794\n",
      "Epoch 46/200\n",
      " - 8s - loss: 0.7173 - accuracy: 0.8838\n",
      "Epoch 47/200\n",
      " - 8s - loss: 0.6680 - accuracy: 0.8969\n",
      "Epoch 48/200\n",
      " - 8s - loss: 0.6218 - accuracy: 0.9069\n",
      "Epoch 49/200\n",
      " - 9s - loss: 0.5784 - accuracy: 0.9141\n",
      "Epoch 50/200\n",
      " - 8s - loss: 0.5344 - accuracy: 0.9249\n",
      "Epoch 51/200\n",
      " - 9s - loss: 0.5079 - accuracy: 0.9307\n",
      "Epoch 52/200\n",
      " - 8s - loss: 0.4630 - accuracy: 0.9410\n",
      "Epoch 53/200\n",
      " - 8s - loss: 0.4322 - accuracy: 0.9481\n",
      "Epoch 54/200\n",
      " - 9s - loss: 0.3842 - accuracy: 0.9568\n",
      "Epoch 55/200\n",
      " - 8s - loss: 0.3548 - accuracy: 0.9633\n",
      "Epoch 56/200\n",
      " - 9s - loss: 0.3200 - accuracy: 0.9684\n",
      "Epoch 57/200\n",
      " - 8s - loss: 0.2946 - accuracy: 0.9725\n",
      "Epoch 58/200\n",
      " - 9s - loss: 0.2992 - accuracy: 0.9757\n",
      "Epoch 59/200\n",
      " - 9s - loss: 0.3141 - accuracy: 0.9697\n",
      "Epoch 60/200\n",
      " - 8s - loss: 0.2297 - accuracy: 0.9845\n",
      "Epoch 61/200\n",
      " - 9s - loss: 0.1990 - accuracy: 0.9891\n",
      "Epoch 62/200\n",
      " - 9s - loss: 0.1809 - accuracy: 0.9926\n",
      "Epoch 63/200\n",
      " - 9s - loss: 0.1610 - accuracy: 0.9947\n",
      "Epoch 64/200\n",
      " - 9s - loss: 0.1463 - accuracy: 0.9946\n",
      "Epoch 65/200\n",
      " - 9s - loss: 0.1305 - accuracy: 0.9960\n",
      "Epoch 66/200\n",
      " - 8s - loss: 0.1180 - accuracy: 0.9977\n",
      "Epoch 67/200\n",
      " - 9s - loss: 0.1052 - accuracy: 0.9980\n",
      "Epoch 68/200\n",
      " - 9s - loss: 0.0948 - accuracy: 0.9992\n",
      "Epoch 69/200\n",
      " - 9s - loss: 0.0904 - accuracy: 0.9989\n",
      "Epoch 70/200\n",
      " - 9s - loss: 0.0872 - accuracy: 0.9992\n",
      "Epoch 71/200\n",
      " - 9s - loss: 0.0699 - accuracy: 0.9999\n",
      "Epoch 72/200\n",
      " - 8s - loss: 0.0615 - accuracy: 0.9999\n",
      "Epoch 73/200\n",
      " - 8s - loss: 0.0556 - accuracy: 0.9997\n",
      "Epoch 74/200\n",
      " - 9s - loss: 0.0493 - accuracy: 0.9997\n",
      "Epoch 75/200\n",
      " - 8s - loss: 0.1865 - accuracy: 0.9733\n",
      "Epoch 76/200\n",
      " - 9s - loss: 0.0834 - accuracy: 0.9965\n",
      "Epoch 77/200\n",
      " - 9s - loss: 0.0457 - accuracy: 0.9995\n",
      "Epoch 78/200\n",
      " - 9s - loss: 0.0362 - accuracy: 0.9997\n",
      "Epoch 79/200\n",
      " - 9s - loss: 0.0327 - accuracy: 0.9997\n",
      "Epoch 80/200\n",
      " - 9s - loss: 0.0285 - accuracy: 0.9997\n",
      "Epoch 81/200\n",
      " - 9s - loss: 0.0254 - accuracy: 0.9999\n",
      "Epoch 82/200\n",
      " - 8s - loss: 0.0231 - accuracy: 0.9999\n",
      "Epoch 83/200\n",
      " - 9s - loss: 0.0212 - accuracy: 0.9999\n",
      "Epoch 84/200\n",
      " - 9s - loss: 0.0195 - accuracy: 0.9999\n",
      "Epoch 85/200\n",
      " - 9s - loss: 0.0178 - accuracy: 0.9999\n",
      "Epoch 86/200\n",
      " - 8s - loss: 0.0163 - accuracy: 0.9997\n",
      "Epoch 87/200\n",
      " - 9s - loss: 0.0151 - accuracy: 0.9999\n",
      "Epoch 88/200\n",
      " - 9s - loss: 0.0138 - accuracy: 0.9997\n",
      "Epoch 89/200\n",
      " - 8s - loss: 0.0223 - accuracy: 0.9995\n",
      "Epoch 90/200\n",
      " - 9s - loss: 0.1380 - accuracy: 0.9738\n",
      "Epoch 91/200\n",
      " - 9s - loss: 0.0861 - accuracy: 0.9904\n",
      "Epoch 92/200\n",
      " - 9s - loss: 0.0242 - accuracy: 0.9995\n",
      "Epoch 93/200\n",
      " - 9s - loss: 0.0148 - accuracy: 0.9999\n",
      "Epoch 94/200\n",
      " - 9s - loss: 0.0124 - accuracy: 0.9999\n",
      "Epoch 95/200\n",
      " - 9s - loss: 0.0111 - accuracy: 0.9999\n",
      "Epoch 96/200\n",
      " - 9s - loss: 0.0101 - accuracy: 0.9997\n",
      "Epoch 97/200\n",
      " - 9s - loss: 0.0092 - accuracy: 0.9999\n",
      "Epoch 98/200\n",
      " - 9s - loss: 0.0085 - accuracy: 0.9999\n",
      "Epoch 99/200\n",
      " - 9s - loss: 0.0079 - accuracy: 0.9999\n",
      "Epoch 100/200\n",
      " - 9s - loss: 0.0073 - accuracy: 0.9997\n",
      "Epoch 101/200\n",
      " - 9s - loss: 0.0068 - accuracy: 0.9999\n",
      "Epoch 102/200\n",
      " - 9s - loss: 0.0064 - accuracy: 0.9997\n",
      "Epoch 103/200\n",
      " - 9s - loss: 0.0059 - accuracy: 0.9999\n",
      "Epoch 104/200\n",
      " - 9s - loss: 0.0056 - accuracy: 0.9997\n",
      "Epoch 105/200\n",
      " - 9s - loss: 0.0052 - accuracy: 0.9999\n",
      "Epoch 106/200\n",
      " - 9s - loss: 0.0048 - accuracy: 0.9999\n",
      "Epoch 107/200\n",
      " - 9s - loss: 0.0045 - accuracy: 0.9999\n",
      "Epoch 108/200\n",
      " - 9s - loss: 0.0042 - accuracy: 0.9999\n",
      "Epoch 109/200\n",
      " - 9s - loss: 0.0039 - accuracy: 0.9999\n",
      "Epoch 110/200\n",
      " - 9s - loss: 0.0037 - accuracy: 0.9997\n",
      "Epoch 111/200\n",
      " - 9s - loss: 0.0034 - accuracy: 0.9997\n",
      "Epoch 112/200\n",
      " - 9s - loss: 0.0032 - accuracy: 0.9997\n",
      "Epoch 113/200\n",
      " - 9s - loss: 0.0029 - accuracy: 0.9999\n",
      "Epoch 114/200\n",
      " - 9s - loss: 0.0027 - accuracy: 0.9999\n",
      "Epoch 115/200\n",
      " - 9s - loss: 0.0025 - accuracy: 0.9997\n",
      "Epoch 116/200\n",
      " - 9s - loss: 0.0024 - accuracy: 0.9999\n",
      "Epoch 117/200\n",
      " - 9s - loss: 0.0024 - accuracy: 0.9997\n",
      "Epoch 118/200\n",
      " - 9s - loss: 0.0021 - accuracy: 0.9997\n",
      "Epoch 119/200\n",
      " - 9s - loss: 0.0019 - accuracy: 0.9997\n",
      "Epoch 120/200\n",
      " - 9s - loss: 0.0441 - accuracy: 0.9872\n",
      "Epoch 121/200\n",
      " - 9s - loss: 0.2942 - accuracy: 0.9215\n",
      "Epoch 122/200\n",
      " - 9s - loss: 0.0348 - accuracy: 0.9944\n",
      "Epoch 123/200\n",
      " - 9s - loss: 0.0092 - accuracy: 0.9997\n",
      "Epoch 124/200\n",
      " - 9s - loss: 0.0063 - accuracy: 0.9997\n",
      "Epoch 125/200\n",
      " - 9s - loss: 0.0052 - accuracy: 0.9999\n",
      "Epoch 126/200\n",
      " - 9s - loss: 0.0046 - accuracy: 0.9999\n",
      "Epoch 127/200\n",
      " - 9s - loss: 0.0041 - accuracy: 0.9999\n",
      "Epoch 128/200\n",
      " - 9s - loss: 0.0038 - accuracy: 0.9997\n",
      "Epoch 129/200\n",
      " - 9s - loss: 0.0035 - accuracy: 0.9997\n",
      "Epoch 00129: early stopping\n",
      "88840\n",
      "2221\n",
      "16719688\n",
      "Model: \"sequential_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 100)           222100    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               240800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 909,321\n",
      "Trainable params: 909,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 10s - loss: 6.3481 - accuracy: 0.1142\n",
      "Epoch 2/200\n",
      " - 9s - loss: 5.7639 - accuracy: 0.1217\n",
      "Epoch 3/200\n",
      " - 10s - loss: 5.4673 - accuracy: 0.1451\n",
      "Epoch 4/200\n",
      " - 9s - loss: 5.2854 - accuracy: 0.1489\n",
      "Epoch 5/200\n",
      " - 10s - loss: 5.1298 - accuracy: 0.1607\n",
      "Epoch 6/200\n",
      " - 9s - loss: 4.9624 - accuracy: 0.1646\n",
      "Epoch 7/200\n",
      " - 9s - loss: 4.7913 - accuracy: 0.1690\n",
      "Epoch 8/200\n",
      " - 9s - loss: 4.6156 - accuracy: 0.1740\n",
      "Epoch 9/200\n",
      " - 10s - loss: 4.4321 - accuracy: 0.1819\n",
      "Epoch 10/200\n",
      " - 9s - loss: 4.2474 - accuracy: 0.1892\n",
      "Epoch 11/200\n",
      " - 9s - loss: 4.0475 - accuracy: 0.2047\n",
      "Epoch 12/200\n",
      " - 9s - loss: 3.8474 - accuracy: 0.2230\n",
      "Epoch 13/200\n",
      " - 9s - loss: 3.6436 - accuracy: 0.2504\n",
      "Epoch 14/200\n",
      " - 9s - loss: 3.4363 - accuracy: 0.2783\n",
      "Epoch 15/200\n",
      " - 9s - loss: 3.2283 - accuracy: 0.3209\n",
      "Epoch 16/200\n",
      " - 10s - loss: 3.0307 - accuracy: 0.3615\n",
      "Epoch 17/200\n",
      " - 10s - loss: 2.8298 - accuracy: 0.4074\n",
      "Epoch 18/200\n",
      " - 10s - loss: 2.6362 - accuracy: 0.4509\n",
      "Epoch 19/200\n",
      " - 9s - loss: 2.4488 - accuracy: 0.4936\n",
      "Epoch 20/200\n",
      " - 9s - loss: 2.2706 - accuracy: 0.5379\n",
      "Epoch 21/200\n",
      " - 9s - loss: 2.0968 - accuracy: 0.5745\n",
      "Epoch 22/200\n",
      " - 10s - loss: 1.9323 - accuracy: 0.6122\n",
      "Epoch 23/200\n",
      " - 10s - loss: 1.7780 - accuracy: 0.6460\n",
      "Epoch 24/200\n",
      " - 10s - loss: 1.6348 - accuracy: 0.6795\n",
      "Epoch 25/200\n",
      " - 10s - loss: 1.4973 - accuracy: 0.7135\n",
      "Epoch 26/200\n",
      " - 10s - loss: 1.3636 - accuracy: 0.7457\n",
      "Epoch 27/200\n",
      " - 9s - loss: 1.2384 - accuracy: 0.7752\n",
      "Epoch 28/200\n",
      " - 10s - loss: 1.1235 - accuracy: 0.8002\n",
      "Epoch 29/200\n",
      " - 9s - loss: 1.0167 - accuracy: 0.8312\n",
      "Epoch 30/200\n",
      " - 10s - loss: 0.9174 - accuracy: 0.8549\n",
      "Epoch 31/200\n",
      " - 9s - loss: 0.8237 - accuracy: 0.8769\n",
      "Epoch 32/200\n",
      " - 9s - loss: 0.7416 - accuracy: 0.8943\n",
      "Epoch 33/200\n",
      " - 9s - loss: 0.6585 - accuracy: 0.9094\n",
      "Epoch 34/200\n",
      " - 10s - loss: 0.5859 - accuracy: 0.9300\n",
      "Epoch 35/200\n",
      " - 10s - loss: 0.5176 - accuracy: 0.9430\n",
      "Epoch 36/200\n",
      " - 9s - loss: 0.4514 - accuracy: 0.9539\n",
      "Epoch 37/200\n",
      " - 9s - loss: 0.4005 - accuracy: 0.9639\n",
      "Epoch 38/200\n",
      " - 9s - loss: 0.3478 - accuracy: 0.9726\n",
      "Epoch 39/200\n",
      " - 9s - loss: 0.3055 - accuracy: 0.9789\n",
      "Epoch 40/200\n",
      " - 9s - loss: 0.2631 - accuracy: 0.9850\n",
      "Epoch 41/200\n",
      " - 10s - loss: 0.2264 - accuracy: 0.9908\n",
      "Epoch 42/200\n",
      " - 9s - loss: 0.1984 - accuracy: 0.9938\n",
      "Epoch 43/200\n",
      " - 10s - loss: 0.1715 - accuracy: 0.9950\n",
      "Epoch 44/200\n",
      " - 9s - loss: 0.1433 - accuracy: 0.9976\n",
      "Epoch 45/200\n",
      " - 9s - loss: 0.1231 - accuracy: 0.9976\n",
      "Epoch 46/200\n",
      " - 9s - loss: 0.1068 - accuracy: 0.9985\n",
      "Epoch 47/200\n",
      " - 10s - loss: 0.0916 - accuracy: 0.9992\n",
      "Epoch 48/200\n",
      " - 9s - loss: 0.0786 - accuracy: 0.9989\n",
      "Epoch 49/200\n",
      " - 9s - loss: 0.0691 - accuracy: 0.9996\n",
      "Epoch 50/200\n",
      " - 9s - loss: 0.0610 - accuracy: 0.9993\n",
      "Epoch 51/200\n",
      " - 9s - loss: 0.0491 - accuracy: 0.9997\n",
      "Epoch 52/200\n",
      " - 9s - loss: 0.0419 - accuracy: 0.9996\n",
      "Epoch 53/200\n",
      " - 10s - loss: 0.0563 - accuracy: 0.9983\n",
      "Epoch 54/200\n",
      " - 10s - loss: 0.1446 - accuracy: 0.9854\n",
      "Epoch 55/200\n",
      " - 10s - loss: 0.1489 - accuracy: 0.9847\n",
      "Epoch 56/200\n",
      " - 10s - loss: 0.0509 - accuracy: 0.9996\n",
      "Epoch 57/200\n",
      " - 9s - loss: 0.0294 - accuracy: 0.9997\n",
      "Epoch 58/200\n",
      " - 9s - loss: 0.0234 - accuracy: 0.9999\n",
      "Epoch 59/200\n",
      " - 9s - loss: 0.0202 - accuracy: 0.9997\n",
      "Epoch 60/200\n",
      " - 10s - loss: 0.0178 - accuracy: 0.9997\n",
      "Epoch 61/200\n",
      " - 9s - loss: 0.0159 - accuracy: 0.9999\n",
      "Epoch 62/200\n",
      " - 9s - loss: 0.0145 - accuracy: 0.9997\n",
      "Epoch 63/200\n",
      " - 9s - loss: 0.0130 - accuracy: 0.9999\n",
      "Epoch 64/200\n",
      " - 9s - loss: 0.0117 - accuracy: 0.9999\n",
      "Epoch 65/200\n",
      " - 9s - loss: 0.0106 - accuracy: 0.9997\n",
      "Epoch 66/200\n",
      " - 10s - loss: 0.0097 - accuracy: 0.9997\n",
      "Epoch 67/200\n",
      " - 9s - loss: 0.0087 - accuracy: 0.9999\n",
      "Epoch 68/200\n",
      " - 10s - loss: 0.0079 - accuracy: 0.9999\n",
      "Epoch 69/200\n",
      " - 9s - loss: 0.0072 - accuracy: 0.9997\n",
      "Epoch 70/200\n",
      " - 9s - loss: 0.0065 - accuracy: 0.9999\n",
      "Epoch 71/200\n",
      " - 9s - loss: 0.0059 - accuracy: 0.9997\n",
      "Epoch 72/200\n",
      " - 10s - loss: 0.0054 - accuracy: 0.9997\n",
      "Epoch 73/200\n",
      " - 10s - loss: 0.0049 - accuracy: 0.9997\n",
      "Epoch 74/200\n",
      " - 9s - loss: 0.0044 - accuracy: 0.9999\n",
      "Epoch 75/200\n",
      " - 9s - loss: 0.0040 - accuracy: 0.9999\n",
      "Epoch 76/200\n",
      " - 9s - loss: 0.1067 - accuracy: 0.9752\n",
      "Epoch 77/200\n",
      " - 9s - loss: 0.3456 - accuracy: 0.9176\n",
      "Epoch 78/200\n",
      " - 9s - loss: 0.0452 - accuracy: 0.9957\n",
      "Epoch 79/200\n",
      " - 10s - loss: 0.0163 - accuracy: 0.9996\n",
      "Epoch 80/200\n",
      " - 9s - loss: 0.0110 - accuracy: 0.9999\n",
      "Epoch 81/200\n",
      " - 10s - loss: 0.0091 - accuracy: 0.9999\n",
      "Epoch 82/200\n",
      " - 9s - loss: 0.0079 - accuracy: 0.9999\n",
      "Epoch 83/200\n",
      " - 9s - loss: 0.0070 - accuracy: 0.9997\n",
      "Epoch 84/200\n",
      " - 9s - loss: 0.0064 - accuracy: 0.9999\n",
      "Epoch 85/200\n",
      " - 10s - loss: 0.0057 - accuracy: 0.9999\n",
      "Epoch 00085: early stopping\n",
      "88840\n",
      "2221\n",
      "16719688\n",
      "Model: \"sequential_94\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 40, 200)           444200    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 1,211,421\n",
      "Trainable params: 1,211,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 12s - loss: 6.3235 - accuracy: 0.1152\n",
      "Epoch 2/200\n",
      " - 11s - loss: 5.7030 - accuracy: 0.1273\n",
      "Epoch 3/200\n",
      " - 11s - loss: 5.4055 - accuracy: 0.1468\n",
      "Epoch 4/200\n",
      " - 11s - loss: 5.2240 - accuracy: 0.1556\n",
      "Epoch 5/200\n",
      " - 12s - loss: 5.0535 - accuracy: 0.1650\n",
      "Epoch 6/200\n",
      " - 11s - loss: 4.8834 - accuracy: 0.1708\n",
      "Epoch 7/200\n",
      " - 12s - loss: 4.7016 - accuracy: 0.1813\n",
      "Epoch 8/200\n",
      " - 11s - loss: 4.5058 - accuracy: 0.1844\n",
      "Epoch 9/200\n",
      " - 11s - loss: 4.2956 - accuracy: 0.1985\n",
      "Epoch 10/200\n",
      " - 12s - loss: 4.0764 - accuracy: 0.2079\n",
      "Epoch 11/200\n",
      " - 11s - loss: 3.8521 - accuracy: 0.2322\n",
      "Epoch 12/200\n",
      " - 11s - loss: 3.6199 - accuracy: 0.2592\n",
      "Epoch 13/200\n",
      " - 11s - loss: 3.3851 - accuracy: 0.2958\n",
      "Epoch 14/200\n",
      " - 11s - loss: 3.1461 - accuracy: 0.3442\n",
      "Epoch 15/200\n",
      " - 12s - loss: 2.9083 - accuracy: 0.3974\n",
      "Epoch 16/200\n",
      " - 12s - loss: 2.6759 - accuracy: 0.4461\n",
      "Epoch 17/200\n",
      " - 12s - loss: 2.4488 - accuracy: 0.5027\n",
      "Epoch 18/200\n",
      " - 11s - loss: 2.2274 - accuracy: 0.5570\n",
      "Epoch 19/200\n",
      " - 11s - loss: 2.0221 - accuracy: 0.6008\n",
      "Epoch 20/200\n",
      " - 11s - loss: 1.8301 - accuracy: 0.6437\n",
      "Epoch 21/200\n",
      " - 12s - loss: 1.6495 - accuracy: 0.6896\n",
      "Epoch 22/200\n",
      " - 11s - loss: 1.4816 - accuracy: 0.7261\n",
      "Epoch 23/200\n",
      " - 11s - loss: 1.3311 - accuracy: 0.7633\n",
      "Epoch 24/200\n",
      " - 11s - loss: 1.1873 - accuracy: 0.7946\n",
      "Epoch 25/200\n",
      " - 11s - loss: 1.0540 - accuracy: 0.8285\n",
      "Epoch 26/200\n",
      " - 12s - loss: 0.9348 - accuracy: 0.8511\n",
      "Epoch 27/200\n",
      " - 11s - loss: 0.8296 - accuracy: 0.8759\n",
      "Epoch 28/200\n",
      " - 12s - loss: 0.7293 - accuracy: 0.9000\n",
      "Epoch 29/200\n",
      " - 11s - loss: 0.6368 - accuracy: 0.9176\n",
      "Epoch 30/200\n",
      " - 11s - loss: 0.5564 - accuracy: 0.9369\n",
      "Epoch 31/200\n",
      " - 12s - loss: 0.4869 - accuracy: 0.9477\n",
      "Epoch 32/200\n",
      " - 12s - loss: 0.4203 - accuracy: 0.9619\n",
      "Epoch 33/200\n",
      " - 11s - loss: 0.3620 - accuracy: 0.9718\n",
      "Epoch 34/200\n",
      " - 11s - loss: 0.3105 - accuracy: 0.9806\n",
      "Epoch 35/200\n",
      " - 11s - loss: 0.2661 - accuracy: 0.9872\n",
      "Epoch 36/200\n",
      " - 12s - loss: 0.2335 - accuracy: 0.9892\n",
      "Epoch 37/200\n",
      " - 12s - loss: 0.1942 - accuracy: 0.9924\n",
      "Epoch 38/200\n",
      " - 12s - loss: 0.1628 - accuracy: 0.9951\n",
      "Epoch 39/200\n",
      " - 11s - loss: 0.1431 - accuracy: 0.9964\n",
      "Epoch 40/200\n",
      " - 11s - loss: 0.1172 - accuracy: 0.9968\n",
      "Epoch 41/200\n",
      " - 11s - loss: 0.0981 - accuracy: 0.9993\n",
      "Epoch 42/200\n",
      " - 12s - loss: 0.0833 - accuracy: 0.9993\n",
      "Epoch 43/200\n",
      " - 11s - loss: 0.0701 - accuracy: 0.9997\n",
      "Epoch 44/200\n",
      " - 11s - loss: 0.0601 - accuracy: 0.9996\n",
      "Epoch 45/200\n",
      " - 11s - loss: 0.0511 - accuracy: 0.9997\n",
      "Epoch 46/200\n",
      " - 11s - loss: 0.0436 - accuracy: 0.9999\n",
      "Epoch 47/200\n",
      " - 12s - loss: 0.0381 - accuracy: 0.9997\n",
      "Epoch 48/200\n",
      " - 12s - loss: 0.0324 - accuracy: 0.9999\n",
      "Epoch 49/200\n",
      " - 12s - loss: 0.0283 - accuracy: 0.9999\n",
      "Epoch 50/200\n",
      " - 11s - loss: 0.0245 - accuracy: 0.9999\n",
      "Epoch 51/200\n",
      " - 11s - loss: 0.0216 - accuracy: 0.9997\n",
      "Epoch 52/200\n",
      " - 12s - loss: 0.0218 - accuracy: 0.9997\n",
      "Epoch 53/200\n",
      " - 11s - loss: 0.2391 - accuracy: 0.9592\n",
      "Epoch 54/200\n",
      " - 11s - loss: 0.0749 - accuracy: 0.9952\n",
      "Epoch 55/200\n",
      " - 11s - loss: 0.0265 - accuracy: 0.9995\n",
      "Epoch 56/200\n",
      " - 11s - loss: 0.0179 - accuracy: 0.9997\n",
      "Epoch 57/200\n",
      " - 12s - loss: 0.0149 - accuracy: 0.9999\n",
      "Epoch 58/200\n",
      " - 11s - loss: 0.0130 - accuracy: 0.9997\n",
      "Epoch 59/200\n",
      " - 12s - loss: 0.0115 - accuracy: 0.9999\n",
      "Epoch 60/200\n",
      " - 11s - loss: 0.0103 - accuracy: 0.9999\n",
      "Epoch 61/200\n",
      " - 11s - loss: 0.0094 - accuracy: 0.9997\n",
      "Epoch 62/200\n",
      " - 11s - loss: 0.0085 - accuracy: 0.9999\n",
      "Epoch 63/200\n",
      " - 12s - loss: 0.0077 - accuracy: 0.9997\n",
      "Epoch 64/200\n",
      " - 11s - loss: 0.0070 - accuracy: 0.9999\n",
      "Epoch 65/200\n",
      " - 11s - loss: 0.0064 - accuracy: 0.9999\n",
      "Epoch 66/200\n",
      " - 11s - loss: 0.0059 - accuracy: 0.9997\n",
      "Epoch 67/200\n",
      " - 11s - loss: 0.0054 - accuracy: 0.9997\n",
      "Epoch 68/200\n",
      " - 12s - loss: 0.0049 - accuracy: 0.9997\n",
      "Epoch 69/200\n",
      " - 12s - loss: 0.0045 - accuracy: 0.9999\n",
      "Epoch 70/200\n",
      " - 12s - loss: 0.0041 - accuracy: 0.9999\n",
      "Epoch 71/200\n",
      " - 11s - loss: 0.0038 - accuracy: 0.9997\n",
      "Epoch 72/200\n",
      " - 11s - loss: 0.0034 - accuracy: 0.9999\n",
      "Epoch 73/200\n",
      " - 12s - loss: 0.0031 - accuracy: 0.9999\n",
      "Epoch 74/200\n",
      " - 11s - loss: 0.0029 - accuracy: 0.9997\n",
      "Epoch 75/200\n",
      " - 11s - loss: 0.0026 - accuracy: 0.9999\n",
      "Epoch 76/200\n",
      " - 11s - loss: 0.0024 - accuracy: 0.9999\n",
      "Epoch 77/200\n",
      " - 11s - loss: 0.0021 - accuracy: 0.9999\n",
      "Epoch 78/200\n",
      " - 12s - loss: 0.0020 - accuracy: 0.9999\n",
      "Epoch 79/200\n",
      " - 11s - loss: 0.0018 - accuracy: 0.9997\n",
      "Epoch 80/200\n",
      " - 12s - loss: 0.0017 - accuracy: 0.9999\n",
      "Epoch 81/200\n",
      " - 11s - loss: 0.0015 - accuracy: 0.9999\n",
      "Epoch 82/200\n",
      " - 11s - loss: 0.0014 - accuracy: 0.9997\n",
      "Epoch 83/200\n",
      " - 12s - loss: 0.0013 - accuracy: 0.9999\n",
      "Epoch 84/200\n",
      " - 11s - loss: 0.0012 - accuracy: 0.9999\n",
      "Epoch 85/200\n",
      " - 11s - loss: 0.2769 - accuracy: 0.9264\n",
      "Epoch 86/200\n",
      " - 11s - loss: 0.0601 - accuracy: 0.9876\n",
      "Epoch 87/200\n",
      " - 11s - loss: 0.0109 - accuracy: 0.9995\n",
      "Epoch 88/200\n",
      " - 11s - loss: 0.0058 - accuracy: 0.9997\n",
      "Epoch 89/200\n",
      " - 12s - loss: 0.0046 - accuracy: 0.9997\n",
      "Epoch 90/200\n",
      " - 12s - loss: 0.0039 - accuracy: 0.9999\n",
      "Epoch 91/200\n",
      " - 12s - loss: 0.0035 - accuracy: 0.9999\n",
      "Epoch 92/200\n",
      " - 11s - loss: 0.0031 - accuracy: 0.9999\n",
      "Epoch 93/200\n",
      " - 11s - loss: 0.0028 - accuracy: 0.9999\n",
      "Epoch 94/200\n",
      " - 12s - loss: 0.0026 - accuracy: 0.9997\n",
      "Epoch 00094: early stopping\n",
      "88840\n",
      "2221\n",
      "16719688\n",
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 40, 10)            22210     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 400)               657600    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2221)              890621    \n",
      "=================================================================\n",
      "Total params: 1,570,431\n",
      "Trainable params: 1,570,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 27s - loss: 6.3363 - accuracy: 0.1126\n",
      "Epoch 2/200\n",
      " - 27s - loss: 5.8779 - accuracy: 0.1164\n",
      "Epoch 3/200\n",
      " - 27s - loss: 5.6267 - accuracy: 0.1269\n",
      "Epoch 4/200\n",
      " - 26s - loss: 5.3998 - accuracy: 0.1459\n",
      "Epoch 5/200\n",
      " - 27s - loss: 5.2269 - accuracy: 0.1529\n",
      "Epoch 6/200\n",
      " - 27s - loss: 5.0567 - accuracy: 0.1571\n",
      "Epoch 7/200\n",
      " - 28s - loss: 4.8737 - accuracy: 0.1585\n",
      "Epoch 8/200\n",
      " - 27s - loss: 4.7141 - accuracy: 0.1614\n",
      "Epoch 9/200\n",
      " - 27s - loss: 4.4610 - accuracy: 0.1678\n",
      "Epoch 10/200\n",
      " - 27s - loss: 4.1990 - accuracy: 0.1775\n",
      "Epoch 11/200\n",
      " - 27s - loss: 3.9289 - accuracy: 0.1926\n",
      "Epoch 12/200\n",
      " - 27s - loss: 3.6371 - accuracy: 0.2226\n",
      "Epoch 13/200\n",
      " - 27s - loss: 3.3386 - accuracy: 0.2699\n",
      "Epoch 14/200\n",
      " - 27s - loss: 3.0309 - accuracy: 0.3346\n",
      "Epoch 15/200\n",
      " - 26s - loss: 2.7214 - accuracy: 0.4024\n",
      "Epoch 16/200\n",
      " - 27s - loss: 2.4223 - accuracy: 0.4722\n",
      "Epoch 17/200\n",
      " - 27s - loss: 2.1248 - accuracy: 0.5497\n",
      "Epoch 18/200\n",
      " - 27s - loss: 1.8595 - accuracy: 0.6121\n",
      "Epoch 19/200\n",
      " - 26s - loss: 1.6203 - accuracy: 0.6732\n",
      "Epoch 20/200\n",
      " - 27s - loss: 1.3868 - accuracy: 0.7237\n",
      "Epoch 21/200\n",
      " - 27s - loss: 1.1805 - accuracy: 0.7847\n",
      "Epoch 22/200\n",
      " - 27s - loss: 0.9959 - accuracy: 0.8273\n",
      "Epoch 23/200\n",
      " - 27s - loss: 0.8354 - accuracy: 0.8644\n",
      "Epoch 24/200\n",
      " - 26s - loss: 0.6812 - accuracy: 0.9041\n",
      "Epoch 25/200\n",
      " - 27s - loss: 0.5608 - accuracy: 0.9342\n",
      "Epoch 26/200\n",
      " - 27s - loss: 0.4476 - accuracy: 0.9584\n",
      "Epoch 27/200\n",
      " - 27s - loss: 0.3549 - accuracy: 0.9760\n",
      "Epoch 28/200\n",
      " - 26s - loss: 0.2721 - accuracy: 0.9892\n",
      "Epoch 29/200\n",
      " - 27s - loss: 0.2081 - accuracy: 0.9950\n",
      "Epoch 30/200\n",
      " - 27s - loss: 0.1567 - accuracy: 0.9973\n",
      "Epoch 31/200\n",
      " - 27s - loss: 0.1195 - accuracy: 0.9989\n",
      "Epoch 32/200\n",
      " - 27s - loss: 0.0906 - accuracy: 0.9996\n",
      "Epoch 33/200\n",
      " - 27s - loss: 0.0689 - accuracy: 0.9999\n",
      "Epoch 34/200\n",
      " - 27s - loss: 0.1286 - accuracy: 0.9927\n",
      "Epoch 35/200\n",
      " - 27s - loss: 0.2306 - accuracy: 0.9807\n",
      "Epoch 36/200\n",
      " - 27s - loss: 0.0822 - accuracy: 0.9993\n",
      "Epoch 37/200\n",
      " - 26s - loss: 0.0503 - accuracy: 0.9991\n",
      "Epoch 38/200\n",
      " - 27s - loss: 0.0328 - accuracy: 0.9996\n",
      "Epoch 39/200\n",
      " - 27s - loss: 0.0253 - accuracy: 0.9997\n",
      "Epoch 40/200\n",
      " - 27s - loss: 0.0204 - accuracy: 0.9999\n",
      "Epoch 41/200\n",
      " - 27s - loss: 0.0176 - accuracy: 0.9997\n",
      "Epoch 42/200\n",
      " - 26s - loss: 0.0153 - accuracy: 0.9997\n",
      "Epoch 43/200\n",
      " - 27s - loss: 0.0135 - accuracy: 0.9997\n",
      "Epoch 44/200\n",
      " - 27s - loss: 0.0117 - accuracy: 0.9999\n",
      "Epoch 45/200\n",
      " - 27s - loss: 0.0104 - accuracy: 0.9999\n",
      "Epoch 46/200\n",
      " - 26s - loss: 0.0092 - accuracy: 0.9999\n",
      "Epoch 47/200\n",
      " - 27s - loss: 0.0082 - accuracy: 0.9999\n",
      "Epoch 48/200\n",
      " - 27s - loss: 0.0074 - accuracy: 0.9997\n",
      "Epoch 49/200\n",
      " - 26s - loss: 0.0065 - accuracy: 0.9999\n",
      "Epoch 50/200\n",
      " - 27s - loss: 0.0058 - accuracy: 0.9999\n",
      "Epoch 51/200\n",
      " - 26s - loss: 0.0052 - accuracy: 0.9997\n",
      "Epoch 52/200\n",
      " - 27s - loss: 0.0047 - accuracy: 0.9997\n",
      "Epoch 53/200\n",
      " - 27s - loss: 0.0041 - accuracy: 0.9999\n",
      "Epoch 54/200\n",
      " - 27s - loss: 0.0125 - accuracy: 0.9979\n",
      "Epoch 55/200\n",
      " - 27s - loss: 1.2723 - accuracy: 0.6605\n",
      "Epoch 56/200\n",
      " - 27s - loss: 0.1585 - accuracy: 0.9797\n",
      "Epoch 57/200\n",
      " - 27s - loss: 0.0433 - accuracy: 0.9993\n",
      "Epoch 58/200\n",
      " - 27s - loss: 0.0228 - accuracy: 0.9997\n",
      "Epoch 59/200\n",
      " - 27s - loss: 0.0171 - accuracy: 0.9997\n",
      "Epoch 60/200\n",
      " - 26s - loss: 0.0140 - accuracy: 0.9999\n",
      "Epoch 61/200\n",
      " - 27s - loss: 0.0120 - accuracy: 0.9997\n",
      "Epoch 62/200\n",
      " - 27s - loss: 0.0104 - accuracy: 0.9999\n",
      "Epoch 63/200\n",
      " - 27s - loss: 0.0092 - accuracy: 0.9999\n",
      "Epoch 00063: early stopping\n",
      "88840\n",
      "2221\n",
      "16719688\n",
      "Model: \"sequential_96\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 40, 50)            111050    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 400)               721600    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2221)              890621    \n",
      "=================================================================\n",
      "Total params: 1,723,271\n",
      "Trainable params: 1,723,271\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 28s - loss: 6.3457 - accuracy: 0.1125\n",
      "Epoch 2/200\n",
      " - 28s - loss: 5.9258 - accuracy: 0.1132\n",
      "Epoch 3/200\n",
      " - 28s - loss: 5.6805 - accuracy: 0.1190\n",
      "Epoch 4/200\n",
      " - 28s - loss: 5.4832 - accuracy: 0.1387\n",
      "Epoch 5/200\n",
      " - 28s - loss: 5.3168 - accuracy: 0.1461\n",
      "Epoch 6/200\n",
      " - 28s - loss: 5.1520 - accuracy: 0.1517\n",
      "Epoch 7/200\n",
      " - 28s - loss: 4.9972 - accuracy: 0.1557\n",
      "Epoch 8/200\n",
      " - 28s - loss: 4.8312 - accuracy: 0.1606\n",
      "Epoch 9/200\n",
      " - 28s - loss: 4.6508 - accuracy: 0.1659\n",
      "Epoch 10/200\n",
      " - 28s - loss: 4.4461 - accuracy: 0.1728\n",
      "Epoch 11/200\n",
      " - 29s - loss: 4.2180 - accuracy: 0.1816\n",
      "Epoch 12/200\n",
      " - 28s - loss: 3.9624 - accuracy: 0.1993\n",
      "Epoch 13/200\n",
      " - 28s - loss: 3.6851 - accuracy: 0.2266\n",
      "Epoch 14/200\n",
      " - 28s - loss: 3.5151 - accuracy: 0.2532\n",
      "Epoch 15/200\n",
      " - 28s - loss: 3.1484 - accuracy: 0.3181\n",
      "Epoch 16/200\n",
      " - 28s - loss: 2.8551 - accuracy: 0.3859\n",
      "Epoch 17/200\n",
      " - 28s - loss: 2.5943 - accuracy: 0.4365\n",
      "Epoch 18/200\n",
      " - 28s - loss: 2.3323 - accuracy: 0.5009\n",
      "Epoch 19/200\n",
      " - 28s - loss: 2.1003 - accuracy: 0.5551\n",
      "Epoch 20/200\n",
      " - 28s - loss: 1.8808 - accuracy: 0.6053\n",
      "Epoch 21/200\n",
      " - 28s - loss: 1.6611 - accuracy: 0.6605\n",
      "Epoch 22/200\n",
      " - 28s - loss: 1.4702 - accuracy: 0.7066\n",
      "Epoch 23/200\n",
      " - 28s - loss: 1.2956 - accuracy: 0.7440\n",
      "Epoch 24/200\n",
      " - 28s - loss: 1.1281 - accuracy: 0.7876\n",
      "Epoch 25/200\n",
      " - 28s - loss: 0.9671 - accuracy: 0.8247\n",
      "Epoch 26/200\n",
      " - 28s - loss: 0.8506 - accuracy: 0.8564\n",
      "Epoch 27/200\n",
      " - 28s - loss: 0.7125 - accuracy: 0.8917\n",
      "Epoch 28/200\n",
      " - 29s - loss: 0.6030 - accuracy: 0.9179\n",
      "Epoch 29/200\n",
      " - 28s - loss: 0.5081 - accuracy: 0.9396\n",
      "Epoch 30/200\n",
      " - 28s - loss: 0.4099 - accuracy: 0.9576\n",
      "Epoch 31/200\n",
      " - 28s - loss: 0.3307 - accuracy: 0.9765\n",
      "Epoch 32/200\n",
      " - 28s - loss: 0.2666 - accuracy: 0.9850\n",
      "Epoch 33/200\n",
      " - 28s - loss: 0.2117 - accuracy: 0.9928\n",
      "Epoch 34/200\n",
      " - 28s - loss: 0.1677 - accuracy: 0.9954\n",
      "Epoch 35/200\n",
      " - 28s - loss: 0.1315 - accuracy: 0.9983\n",
      "Epoch 36/200\n",
      " - 29s - loss: 0.1063 - accuracy: 0.9995\n",
      "Epoch 37/200\n",
      " - 28s - loss: 0.0838 - accuracy: 0.9996\n",
      "Epoch 38/200\n",
      " - 28s - loss: 0.0662 - accuracy: 0.9997\n",
      "Epoch 39/200\n",
      " - 28s - loss: 0.0532 - accuracy: 0.9999\n",
      "Epoch 40/200\n",
      " - 28s - loss: 0.0434 - accuracy: 0.9997\n",
      "Epoch 41/200\n",
      " - 28s - loss: 0.0364 - accuracy: 0.9997\n",
      "Epoch 42/200\n",
      " - 28s - loss: 0.0306 - accuracy: 0.9999\n",
      "Epoch 43/200\n",
      " - 28s - loss: 0.0256 - accuracy: 0.9997\n",
      "Epoch 44/200\n",
      " - 28s - loss: 0.0222 - accuracy: 0.9997\n",
      "Epoch 45/200\n",
      " - 28s - loss: 0.0187 - accuracy: 0.9999\n",
      "Epoch 46/200\n",
      " - 28s - loss: 0.0161 - accuracy: 0.9997\n",
      "Epoch 47/200\n",
      " - 28s - loss: 0.0140 - accuracy: 0.9999\n",
      "Epoch 48/200\n",
      " - 28s - loss: 0.0120 - accuracy: 0.9999\n",
      "Epoch 49/200\n",
      " - 28s - loss: 0.0109 - accuracy: 0.9999\n",
      "Epoch 50/200\n",
      " - 28s - loss: 0.6758 - accuracy: 0.8207\n",
      "Epoch 51/200\n",
      " - 28s - loss: 0.2338 - accuracy: 0.9623\n",
      "Epoch 52/200\n",
      " - 28s - loss: 0.0598 - accuracy: 0.9973\n",
      "Epoch 53/200\n",
      " - 28s - loss: 0.0236 - accuracy: 0.9997\n",
      "Epoch 54/200\n",
      " - 28s - loss: 0.0166 - accuracy: 0.9999\n",
      "Epoch 55/200\n",
      " - 28s - loss: 0.0137 - accuracy: 0.9997\n",
      "Epoch 56/200\n",
      " - 28s - loss: 0.0118 - accuracy: 0.9997\n",
      "Epoch 57/200\n",
      " - 28s - loss: 0.0104 - accuracy: 0.9997\n",
      "Epoch 58/200\n",
      " - 28s - loss: 0.0092 - accuracy: 0.9997\n",
      "Epoch 59/200\n",
      " - 28s - loss: 0.0082 - accuracy: 0.9999\n",
      "Epoch 60/200\n",
      " - 28s - loss: 0.0074 - accuracy: 0.9997\n",
      "Epoch 61/200\n",
      " - 28s - loss: 0.0067 - accuracy: 0.9997\n",
      "Epoch 62/200\n",
      " - 28s - loss: 0.0060 - accuracy: 0.9999\n",
      "Epoch 63/200\n",
      " - 28s - loss: 0.0055 - accuracy: 0.9997\n",
      "Epoch 64/200\n",
      " - 28s - loss: 0.0051 - accuracy: 0.9997\n",
      "Epoch 65/200\n",
      " - 28s - loss: 0.0045 - accuracy: 0.9999\n",
      "Epoch 66/200\n",
      " - 28s - loss: 0.0042 - accuracy: 0.9999\n",
      "Epoch 67/200\n",
      " - 28s - loss: 0.0038 - accuracy: 0.9999\n",
      "Epoch 68/200\n",
      " - 28s - loss: 0.0034 - accuracy: 0.9999\n",
      "Epoch 69/200\n",
      " - 28s - loss: 0.0032 - accuracy: 0.9997\n",
      "Epoch 70/200\n",
      " - 28s - loss: 0.0029 - accuracy: 0.9999\n",
      "Epoch 71/200\n",
      " - 28s - loss: 0.0027 - accuracy: 0.9999\n",
      "Epoch 72/200\n",
      " - 28s - loss: 0.0025 - accuracy: 0.9997\n",
      "Epoch 73/200\n",
      " - 28s - loss: 0.0023 - accuracy: 0.9997\n",
      "Epoch 74/200\n",
      " - 28s - loss: 0.0021 - accuracy: 0.9997\n",
      "Epoch 75/200\n",
      " - 29s - loss: 0.0019 - accuracy: 0.9999\n",
      "Epoch 76/200\n",
      " - 28s - loss: 0.0017 - accuracy: 0.9999\n",
      "Epoch 77/200\n",
      " - 28s - loss: 0.0016 - accuracy: 0.9997\n",
      "Epoch 78/200\n",
      " - 28s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 79/200\n",
      " - 28s - loss: 0.0015 - accuracy: 0.9997\n",
      "Epoch 80/200\n",
      " - 28s - loss: 0.0013 - accuracy: 0.9999\n",
      "Epoch 81/200\n",
      " - 28s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 82/200\n",
      " - 28s - loss: 0.0010 - accuracy: 0.9999\n",
      "Epoch 83/200\n",
      " - 28s - loss: 9.6295e-04 - accuracy: 0.9999\n",
      "Epoch 84/200\n",
      " - 28s - loss: 8.9696e-04 - accuracy: 0.9999\n",
      "Epoch 85/200\n",
      " - 28s - loss: 7.9701e-04 - accuracy: 0.9997\n",
      "Epoch 86/200\n",
      " - 28s - loss: 7.6679e-04 - accuracy: 0.9999\n",
      "Epoch 87/200\n",
      " - 28s - loss: 7.1771e-04 - accuracy: 0.9999\n",
      "Epoch 88/200\n",
      " - 28s - loss: 6.7890e-04 - accuracy: 0.9999\n",
      "Epoch 89/200\n",
      " - 28s - loss: 0.4323 - accuracy: 0.8917\n",
      "Epoch 90/200\n",
      " - 28s - loss: 0.7450 - accuracy: 0.7993\n",
      "Epoch 91/200\n",
      " - 28s - loss: 0.0711 - accuracy: 0.9910\n",
      "Epoch 92/200\n",
      " - 28s - loss: 0.0193 - accuracy: 0.9995\n",
      "Epoch 93/200\n",
      " - 28s - loss: 0.0101 - accuracy: 0.9999\n",
      "Epoch 94/200\n",
      " - 28s - loss: 0.0077 - accuracy: 0.9999\n",
      "Epoch 95/200\n",
      " - 28s - loss: 0.0066 - accuracy: 0.9997\n",
      "Epoch 96/200\n",
      " - 29s - loss: 0.0056 - accuracy: 0.9999\n",
      "Epoch 97/200\n",
      " - 29s - loss: 0.0050 - accuracy: 0.9997\n",
      "Epoch 98/200\n",
      " - 29s - loss: 0.0044 - accuracy: 0.9997\n",
      "Epoch 00098: early stopping\n",
      "88840\n",
      "2221\n",
      "16719688\n",
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 40, 100)           222100    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 400)               801600    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2221)              890621    \n",
      "=================================================================\n",
      "Total params: 1,914,321\n",
      "Trainable params: 1,914,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 31s - loss: 6.3219 - accuracy: 0.1113\n",
      "Epoch 2/200\n",
      " - 32s - loss: 5.6790 - accuracy: 0.1296\n",
      "Epoch 3/200\n",
      " - 31s - loss: 5.3845 - accuracy: 0.1472\n",
      "Epoch 4/200\n",
      " - 31s - loss: 5.2035 - accuracy: 0.1524\n",
      "Epoch 5/200\n",
      " - 30s - loss: 4.9725 - accuracy: 0.1618\n",
      "Epoch 6/200\n",
      " - 31s - loss: 4.7145 - accuracy: 0.1702\n",
      "Epoch 7/200\n",
      " - 30s - loss: 4.4200 - accuracy: 0.1781\n",
      "Epoch 8/200\n",
      " - 30s - loss: 4.0661 - accuracy: 0.1914\n",
      "Epoch 9/200\n",
      " - 30s - loss: 3.6829 - accuracy: 0.2279\n",
      "Epoch 10/200\n",
      " - 31s - loss: 3.2644 - accuracy: 0.2893\n",
      "Epoch 11/200\n",
      " - 30s - loss: 2.8364 - accuracy: 0.3770\n",
      "Epoch 12/200\n",
      " - 30s - loss: 2.4208 - accuracy: 0.4794\n",
      "Epoch 13/200\n",
      " - 30s - loss: 2.0342 - accuracy: 0.5712\n",
      "Epoch 14/200\n",
      " - 31s - loss: 1.6965 - accuracy: 0.6577\n",
      "Epoch 15/200\n",
      " - 30s - loss: 1.3913 - accuracy: 0.7326\n",
      "Epoch 16/200\n",
      " - 30s - loss: 1.1244 - accuracy: 0.7962\n",
      "Epoch 17/200\n",
      " - 30s - loss: 0.8905 - accuracy: 0.8564\n",
      "Epoch 18/200\n",
      " - 31s - loss: 0.6988 - accuracy: 0.9065\n",
      "Epoch 19/200\n",
      " - 30s - loss: 0.5323 - accuracy: 0.9438\n",
      "Epoch 20/200\n",
      " - 30s - loss: 0.3946 - accuracy: 0.9716\n",
      "Epoch 21/200\n",
      " - 30s - loss: 0.2903 - accuracy: 0.9851\n",
      "Epoch 22/200\n",
      " - 31s - loss: 0.2088 - accuracy: 0.9943\n",
      "Epoch 23/200\n",
      " - 30s - loss: 0.1487 - accuracy: 0.9983\n",
      "Epoch 24/200\n",
      " - 30s - loss: 0.1087 - accuracy: 0.9991\n",
      "Epoch 25/200\n",
      " - 30s - loss: 0.0806 - accuracy: 0.9997\n",
      "Epoch 26/200\n",
      " - 31s - loss: 0.0617 - accuracy: 0.9997\n",
      "Epoch 27/200\n",
      " - 30s - loss: 0.0484 - accuracy: 0.9999\n",
      "Epoch 28/200\n",
      " - 30s - loss: 0.0391 - accuracy: 0.9997\n",
      "Epoch 29/200\n",
      " - 30s - loss: 0.0320 - accuracy: 0.9999\n",
      "Epoch 30/200\n",
      " - 31s - loss: 0.0268 - accuracy: 0.9997\n",
      "Epoch 31/200\n",
      " - 30s - loss: 0.0223 - accuracy: 0.9999\n",
      "Epoch 32/200\n",
      " - 30s - loss: 0.0190 - accuracy: 0.9997\n",
      "Epoch 33/200\n",
      " - 30s - loss: 0.0163 - accuracy: 0.9997\n",
      "Epoch 34/200\n",
      " - 30s - loss: 0.0139 - accuracy: 0.9997\n",
      "Epoch 35/200\n",
      " - 30s - loss: 0.0121 - accuracy: 0.9997\n",
      "Epoch 36/200\n",
      " - 30s - loss: 0.0105 - accuracy: 0.9997\n",
      "Epoch 37/200\n",
      " - 31s - loss: 0.0092 - accuracy: 0.9997\n",
      "Epoch 38/200\n",
      " - 31s - loss: 0.0080 - accuracy: 0.9997\n",
      "Epoch 39/200\n",
      " - 30s - loss: 0.0069 - accuracy: 0.9997\n",
      "Epoch 40/200\n",
      " - 30s - loss: 0.0066 - accuracy: 0.9999\n",
      "Epoch 41/200\n",
      " - 30s - loss: 0.0056 - accuracy: 0.9997\n",
      "Epoch 42/200\n",
      " - 30s - loss: 0.6033 - accuracy: 0.8482\n",
      "Epoch 43/200\n",
      " - 30s - loss: 0.2764 - accuracy: 0.9437\n",
      "Epoch 44/200\n",
      " - 30s - loss: 0.0391 - accuracy: 0.9988\n",
      "Epoch 45/200\n",
      " - 31s - loss: 0.0151 - accuracy: 0.9999\n",
      "Epoch 46/200\n",
      " - 31s - loss: 0.0109 - accuracy: 0.9999\n",
      "Epoch 47/200\n",
      " - 31s - loss: 0.0089 - accuracy: 0.9999\n",
      "Epoch 48/200\n",
      " - 30s - loss: 0.0076 - accuracy: 0.9999\n",
      "Epoch 49/200\n",
      " - 30s - loss: 0.0066 - accuracy: 0.9999\n",
      "Epoch 50/200\n",
      " - 31s - loss: 0.0060 - accuracy: 0.9997\n",
      "Epoch 51/200\n",
      " - 30s - loss: 0.0053 - accuracy: 0.9999\n",
      "Epoch 52/200\n",
      " - 30s - loss: 0.0048 - accuracy: 0.9997\n",
      "Epoch 53/200\n",
      " - 30s - loss: 0.0043 - accuracy: 0.9999\n",
      "Epoch 54/200\n",
      " - 31s - loss: 0.0039 - accuracy: 0.9997\n",
      "Epoch 55/200\n",
      " - 31s - loss: 0.0035 - accuracy: 0.9999\n",
      "Epoch 56/200\n",
      " - 30s - loss: 0.0031 - accuracy: 0.9999\n",
      "Epoch 57/200\n",
      " - 31s - loss: 0.0030 - accuracy: 0.9997\n",
      "Epoch 58/200\n",
      " - 31s - loss: 0.0026 - accuracy: 0.9999\n",
      "Epoch 59/200\n",
      " - 31s - loss: 0.0025 - accuracy: 0.9999\n",
      "Epoch 60/200\n",
      " - 30s - loss: 0.0023 - accuracy: 0.9999\n",
      "Epoch 61/200\n",
      " - 31s - loss: 0.0021 - accuracy: 0.9999\n",
      "Epoch 62/200\n",
      " - 31s - loss: 0.0019 - accuracy: 0.9999\n",
      "Epoch 63/200\n",
      " - 30s - loss: 0.0017 - accuracy: 0.9999\n",
      "Epoch 64/200\n",
      " - 30s - loss: 0.0017 - accuracy: 0.9997\n",
      "Epoch 65/200\n",
      " - 30s - loss: 0.0015 - accuracy: 0.9999\n",
      "Epoch 66/200\n",
      " - 30s - loss: 0.0014 - accuracy: 0.9999\n",
      "Epoch 67/200\n",
      " - 31s - loss: 0.0013 - accuracy: 0.9999\n",
      "Epoch 68/200\n",
      " - 30s - loss: 0.0012 - accuracy: 0.9997\n",
      "Epoch 69/200\n",
      " - 31s - loss: 0.0011 - accuracy: 0.9999\n",
      "Epoch 70/200\n",
      " - 30s - loss: 0.0011 - accuracy: 0.9997\n",
      "Epoch 71/200\n",
      " - 30s - loss: 9.5769e-04 - accuracy: 0.9999\n",
      "Epoch 72/200\n",
      " - 30s - loss: 8.9072e-04 - accuracy: 0.9999\n",
      "Epoch 73/200\n",
      " - 31s - loss: 9.2726e-04 - accuracy: 0.9997\n",
      "Epoch 74/200\n",
      " - 30s - loss: 7.9218e-04 - accuracy: 0.9999\n",
      "Epoch 75/200\n",
      " - 30s - loss: 8.1456e-04 - accuracy: 0.9997\n",
      "Epoch 76/200\n",
      " - 30s - loss: 6.9696e-04 - accuracy: 0.9999\n",
      "Epoch 77/200\n",
      " - 31s - loss: 6.5326e-04 - accuracy: 0.9999\n",
      "Epoch 78/200\n",
      " - 30s - loss: 5.9873e-04 - accuracy: 0.9997\n",
      "Epoch 79/200\n",
      " - 30s - loss: 5.8807e-04 - accuracy: 0.9999\n",
      "Epoch 80/200\n",
      " - 30s - loss: 6.1110e-04 - accuracy: 0.9997\n",
      "Epoch 81/200\n",
      " - 31s - loss: 5.3865e-04 - accuracy: 0.9999\n",
      "Epoch 82/200\n",
      " - 31s - loss: 5.1392e-04 - accuracy: 0.9999\n",
      "Epoch 83/200\n",
      " - 31s - loss: 5.4450e-04 - accuracy: 0.9997\n",
      "Epoch 84/200\n",
      " - 31s - loss: 5.2960e-04 - accuracy: 0.9997\n",
      "Epoch 85/200\n",
      " - 31s - loss: 5.1018e-04 - accuracy: 0.9997\n",
      "Epoch 86/200\n",
      " - 31s - loss: 3.6564e-04 - accuracy: 0.9999\n",
      "Epoch 87/200\n",
      " - 31s - loss: 3.4116e-04 - accuracy: 0.9999\n",
      "Epoch 88/200\n",
      " - 30s - loss: 4.1831e-04 - accuracy: 0.9999\n",
      "Epoch 89/200\n",
      " - 31s - loss: 4.0500e-04 - accuracy: 0.9999\n",
      "Epoch 90/200\n",
      " - 30s - loss: 4.3699e-04 - accuracy: 0.9997\n",
      "Epoch 91/200\n",
      " - 30s - loss: 0.2911 - accuracy: 0.9187\n",
      "Epoch 92/200\n",
      " - 30s - loss: 0.5589 - accuracy: 0.8495\n",
      "Epoch 93/200\n",
      " - 31s - loss: 0.0338 - accuracy: 0.9971\n",
      "Epoch 94/200\n",
      " - 30s - loss: 0.0083 - accuracy: 0.9997\n",
      "Epoch 95/200\n",
      " - 30s - loss: 0.0054 - accuracy: 0.9997\n",
      "Epoch 96/200\n",
      " - 30s - loss: 0.0044 - accuracy: 0.9997\n",
      "Epoch 97/200\n",
      " - 32s - loss: 0.0037 - accuracy: 0.9997\n",
      "Epoch 00097: early stopping\n",
      "88840\n",
      "2221\n",
      "16719688\n",
      "Model: \"sequential_98\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 40, 200)           444200    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2221)              890621    \n",
      "=================================================================\n",
      "Total params: 2,296,421\n",
      "Trainable params: 2,296,421\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 34s - loss: 6.3133 - accuracy: 0.1152\n",
      "Epoch 2/200\n",
      " - 34s - loss: 5.6290 - accuracy: 0.1355\n",
      "Epoch 3/200\n",
      " - 34s - loss: 5.3480 - accuracy: 0.1482\n",
      "Epoch 4/200\n",
      " - 34s - loss: 5.1388 - accuracy: 0.1578\n",
      "Epoch 5/200\n",
      " - 34s - loss: 4.9185 - accuracy: 0.1623\n",
      "Epoch 6/200\n",
      " - 34s - loss: 4.6597 - accuracy: 0.1699\n",
      "Epoch 7/200\n",
      " - 34s - loss: 4.3462 - accuracy: 0.1857\n",
      "Epoch 8/200\n",
      " - 34s - loss: 3.9720 - accuracy: 0.2094\n",
      "Epoch 9/200\n",
      " - 34s - loss: 3.5442 - accuracy: 0.2520\n",
      "Epoch 10/200\n",
      " - 34s - loss: 3.0778 - accuracy: 0.3316\n",
      "Epoch 11/200\n",
      " - 34s - loss: 2.6062 - accuracy: 0.4420\n",
      "Epoch 12/200\n",
      " - 34s - loss: 2.1528 - accuracy: 0.5515\n",
      "Epoch 13/200\n",
      " - 34s - loss: 1.7493 - accuracy: 0.6488\n",
      "Epoch 14/200\n",
      " - 34s - loss: 1.4001 - accuracy: 0.7298\n",
      "Epoch 15/200\n",
      " - 34s - loss: 1.0904 - accuracy: 0.8066\n",
      "Epoch 16/200\n",
      " - 34s - loss: 0.8427 - accuracy: 0.8672\n",
      "Epoch 17/200\n",
      " - 34s - loss: 0.6285 - accuracy: 0.9204\n",
      "Epoch 18/200\n",
      " - 34s - loss: 0.4607 - accuracy: 0.9555\n",
      "Epoch 19/200\n",
      " - 34s - loss: 0.3335 - accuracy: 0.9774\n",
      "Epoch 20/200\n",
      " - 34s - loss: 0.2307 - accuracy: 0.9908\n",
      "Epoch 21/200\n",
      " - 34s - loss: 0.1623 - accuracy: 0.9967\n",
      "Epoch 22/200\n",
      " - 34s - loss: 0.1134 - accuracy: 0.9992\n",
      "Epoch 23/200\n",
      " - 34s - loss: 0.0847 - accuracy: 0.9997\n",
      "Epoch 24/200\n",
      " - 34s - loss: 0.0626 - accuracy: 0.9999\n",
      "Epoch 25/200\n",
      " - 34s - loss: 0.0481 - accuracy: 0.9997\n",
      "Epoch 26/200\n",
      " - 34s - loss: 0.0383 - accuracy: 0.9997\n",
      "Epoch 27/200\n",
      " - 34s - loss: 0.0310 - accuracy: 0.9999\n",
      "Epoch 28/200\n",
      " - 34s - loss: 0.0259 - accuracy: 0.9997\n",
      "Epoch 29/200\n",
      " - 34s - loss: 0.0216 - accuracy: 0.9997\n",
      "Epoch 30/200\n",
      " - 34s - loss: 0.0183 - accuracy: 0.9997\n",
      "Epoch 31/200\n",
      " - 34s - loss: 0.0156 - accuracy: 0.9997\n",
      "Epoch 32/200\n",
      " - 34s - loss: 0.0133 - accuracy: 0.9999\n",
      "Epoch 33/200\n",
      " - 34s - loss: 0.0116 - accuracy: 0.9999\n",
      "Epoch 34/200\n",
      " - 34s - loss: 0.0099 - accuracy: 0.9999\n",
      "Epoch 35/200\n",
      " - 34s - loss: 0.0087 - accuracy: 0.9997\n",
      "Epoch 36/200\n",
      " - 34s - loss: 0.0075 - accuracy: 0.9997\n",
      "Epoch 37/200\n",
      " - 34s - loss: 0.0065 - accuracy: 0.9999\n",
      "Epoch 38/200\n",
      " - 34s - loss: 0.0058 - accuracy: 0.9997\n",
      "Epoch 39/200\n",
      " - 34s - loss: 0.0049 - accuracy: 0.9999\n",
      "Epoch 40/200\n",
      " - 34s - loss: 0.0043 - accuracy: 0.9997\n",
      "Epoch 41/200\n",
      " - 34s - loss: 0.0038 - accuracy: 0.9999\n",
      "Epoch 42/200\n",
      " - 34s - loss: 0.0035 - accuracy: 0.9997\n",
      "Epoch 43/200\n",
      " - 34s - loss: 0.0030 - accuracy: 0.9999\n",
      "Epoch 44/200\n",
      " - 34s - loss: 0.0026 - accuracy: 0.9999\n",
      "Epoch 45/200\n",
      " - 34s - loss: 0.0024 - accuracy: 0.9999\n",
      "Epoch 46/200\n",
      " - 35s - loss: 0.0024 - accuracy: 0.9997\n",
      "Epoch 47/200\n",
      " - 34s - loss: 0.0021 - accuracy: 0.9999\n",
      "Epoch 48/200\n",
      " - 34s - loss: 0.0021 - accuracy: 0.9997\n",
      "Epoch 49/200\n",
      " - 34s - loss: 0.6765 - accuracy: 0.8257\n",
      "Epoch 50/200\n",
      " - 34s - loss: 0.1650 - accuracy: 0.9708\n",
      "Epoch 51/200\n",
      " - 34s - loss: 0.0231 - accuracy: 0.9991\n",
      "Epoch 52/200\n",
      " - 34s - loss: 0.0098 - accuracy: 0.9999\n",
      "Epoch 53/200\n",
      " - 34s - loss: 0.0073 - accuracy: 0.9999\n",
      "Epoch 54/200\n",
      " - 34s - loss: 0.0061 - accuracy: 0.9999\n",
      "Epoch 55/200\n",
      " - 34s - loss: 0.0053 - accuracy: 0.9999\n",
      "Epoch 56/200\n",
      " - 35s - loss: 0.0047 - accuracy: 0.9997\n",
      "Epoch 57/200\n",
      " - 34s - loss: 0.0041 - accuracy: 0.9999\n",
      "Epoch 00057: early stopping\n",
      "128240\n",
      "3206\n",
      "43527862\n",
      "Model: \"sequential_99\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 40, 5)             16030     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 300)               367200    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3206)              965006    \n",
      "=================================================================\n",
      "Total params: 1,348,236\n",
      "Trainable params: 1,348,236\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 29s - loss: 6.3976 - accuracy: 0.1109\n",
      "Epoch 2/200\n",
      " - 29s - loss: 5.9787 - accuracy: 0.1134\n",
      "Epoch 3/200\n",
      " - 29s - loss: 5.7799 - accuracy: 0.1227\n",
      "Epoch 4/200\n",
      " - 29s - loss: 5.5603 - accuracy: 0.1438\n",
      "Epoch 5/200\n",
      " - 29s - loss: 5.3783 - accuracy: 0.1488\n",
      "Epoch 6/200\n",
      " - 29s - loss: 5.2246 - accuracy: 0.1517\n",
      "Epoch 7/200\n",
      " - 29s - loss: 5.4013 - accuracy: 0.1428\n",
      "Epoch 8/200\n",
      " - 29s - loss: 5.1092 - accuracy: 0.1550\n",
      "Epoch 9/200\n",
      " - 29s - loss: 4.9125 - accuracy: 0.1564\n",
      "Epoch 10/200\n",
      " - 29s - loss: 4.7673 - accuracy: 0.1592\n",
      "Epoch 11/200\n",
      " - 29s - loss: 4.6220 - accuracy: 0.1618\n",
      "Epoch 12/200\n",
      " - 29s - loss: 4.4706 - accuracy: 0.1656\n",
      "Epoch 13/200\n",
      " - 29s - loss: 4.3045 - accuracy: 0.1725\n",
      "Epoch 14/200\n",
      " - 28s - loss: 4.1408 - accuracy: 0.1799\n",
      "Epoch 15/200\n",
      " - 29s - loss: 3.9523 - accuracy: 0.1974\n",
      "Epoch 16/200\n",
      " - 29s - loss: 3.7567 - accuracy: 0.2218\n",
      "Epoch 17/200\n",
      " - 29s - loss: 3.5674 - accuracy: 0.2460\n",
      "Epoch 18/200\n",
      " - 29s - loss: 3.3837 - accuracy: 0.2791\n",
      "Epoch 19/200\n",
      " - 29s - loss: 3.1944 - accuracy: 0.3166\n",
      "Epoch 20/200\n",
      " - 29s - loss: 3.0128 - accuracy: 0.3504\n",
      "Epoch 21/200\n",
      " - 29s - loss: 2.8449 - accuracy: 0.3811\n",
      "Epoch 22/200\n",
      " - 29s - loss: 2.6656 - accuracy: 0.4187\n",
      "Epoch 23/200\n",
      " - 29s - loss: 2.5054 - accuracy: 0.4509\n",
      "Epoch 24/200\n",
      " - 29s - loss: 2.3499 - accuracy: 0.4857\n",
      "Epoch 25/200\n",
      " - 29s - loss: 2.2243 - accuracy: 0.5091\n",
      "Epoch 26/200\n",
      " - 29s - loss: 2.0607 - accuracy: 0.5474\n",
      "Epoch 27/200\n",
      " - 29s - loss: 1.9436 - accuracy: 0.5707\n",
      "Epoch 28/200\n",
      " - 29s - loss: 1.8304 - accuracy: 0.5953\n",
      "Epoch 29/200\n",
      " - 29s - loss: 1.6924 - accuracy: 0.6275\n",
      "Epoch 30/200\n",
      " - 29s - loss: 1.5749 - accuracy: 0.6593\n",
      "Epoch 31/200\n",
      " - 29s - loss: 1.4645 - accuracy: 0.6819\n",
      "Epoch 32/200\n",
      " - 29s - loss: 1.3522 - accuracy: 0.7114\n",
      "Epoch 33/200\n",
      " - 29s - loss: 1.2634 - accuracy: 0.7315\n",
      "Epoch 34/200\n",
      " - 29s - loss: 1.2022 - accuracy: 0.7510\n",
      "Epoch 35/200\n",
      " - 28s - loss: 1.1008 - accuracy: 0.7720\n",
      "Epoch 36/200\n",
      " - 29s - loss: 0.9917 - accuracy: 0.8013\n",
      "Epoch 37/200\n",
      " - 29s - loss: 0.9197 - accuracy: 0.8212\n",
      "Epoch 38/200\n",
      " - 29s - loss: 0.8542 - accuracy: 0.8365\n",
      "Epoch 39/200\n",
      " - 29s - loss: 0.7745 - accuracy: 0.8565\n",
      "Epoch 40/200\n",
      " - 29s - loss: 0.7169 - accuracy: 0.8737\n",
      "Epoch 41/200\n",
      " - 29s - loss: 0.6536 - accuracy: 0.8866\n",
      "Epoch 42/200\n",
      " - 29s - loss: 0.6075 - accuracy: 0.8950\n",
      "Epoch 43/200\n",
      " - 29s - loss: 0.5231 - accuracy: 0.9177\n",
      "Epoch 44/200\n",
      " - 29s - loss: 0.4833 - accuracy: 0.9263\n",
      "Epoch 45/200\n",
      " - 29s - loss: 0.5368 - accuracy: 0.9098\n",
      "Epoch 46/200\n",
      " - 29s - loss: 0.4150 - accuracy: 0.9410\n",
      "Epoch 47/200\n",
      " - 28s - loss: 0.3872 - accuracy: 0.9458\n",
      "Epoch 48/200\n",
      " - 29s - loss: 0.3981 - accuracy: 0.9421\n",
      "Epoch 49/200\n",
      " - 29s - loss: 0.3440 - accuracy: 0.9549\n",
      "Epoch 50/200\n",
      " - 29s - loss: 0.2825 - accuracy: 0.9691\n",
      "Epoch 51/200\n",
      " - 29s - loss: 0.2276 - accuracy: 0.9772\n",
      "Epoch 52/200\n",
      " - 29s - loss: 0.1984 - accuracy: 0.9845\n",
      "Epoch 53/200\n",
      " - 29s - loss: 0.3715 - accuracy: 0.9392\n",
      "Epoch 54/200\n",
      " - 29s - loss: 0.2300 - accuracy: 0.9743\n",
      "Epoch 55/200\n",
      " - 29s - loss: 0.1785 - accuracy: 0.9850\n",
      "Epoch 56/200\n",
      " - 29s - loss: 0.1544 - accuracy: 0.9884\n",
      "Epoch 57/200\n",
      " - 30s - loss: 0.1091 - accuracy: 0.9950\n",
      "Epoch 58/200\n",
      " - 29s - loss: 0.0926 - accuracy: 0.9960\n",
      "Epoch 59/200\n",
      " - 29s - loss: 0.5076 - accuracy: 0.8894\n",
      "Epoch 60/200\n",
      " - 29s - loss: 0.2734 - accuracy: 0.9529\n",
      "Epoch 61/200\n",
      " - 29s - loss: 0.1233 - accuracy: 0.9901\n",
      "Epoch 62/200\n",
      " - 29s - loss: 0.0832 - accuracy: 0.9961\n",
      "Epoch 63/200\n",
      " - 29s - loss: 0.0567 - accuracy: 0.9988\n",
      "Epoch 64/200\n",
      " - 29s - loss: 0.0534 - accuracy: 0.9990\n",
      "Epoch 65/200\n",
      " - 29s - loss: 0.0388 - accuracy: 0.9996\n",
      "Epoch 66/200\n",
      " - 29s - loss: 0.0592 - accuracy: 0.9971\n",
      "Epoch 67/200\n",
      " - 29s - loss: 0.6005 - accuracy: 0.8500\n",
      "Epoch 68/200\n",
      " - 29s - loss: 0.2531 - accuracy: 0.9519\n",
      "Epoch 69/200\n",
      " - 29s - loss: 0.1045 - accuracy: 0.9905\n",
      "Epoch 70/200\n",
      " - 29s - loss: 0.0700 - accuracy: 0.9961\n",
      "Epoch 71/200\n",
      " - 29s - loss: 0.1076 - accuracy: 0.9868\n",
      "Epoch 72/200\n",
      " - 29s - loss: 0.1293 - accuracy: 0.9831\n",
      "Epoch 73/200\n",
      " - 29s - loss: 0.0673 - accuracy: 0.9957\n",
      "Epoch 74/200\n",
      " - 29s - loss: 0.0312 - accuracy: 0.9996\n",
      "Epoch 75/200\n",
      " - 29s - loss: 0.0201 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      " - 29s - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      " - 29s - loss: 0.0159 - accuracy: 0.9999\n",
      "Epoch 78/200\n",
      " - 29s - loss: 0.0125 - accuracy: 0.9999\n",
      "Epoch 79/200\n",
      " - 29s - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      " - 28s - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      " - 29s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      " - 29s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      " - 29s - loss: 0.7270 - accuracy: 0.8342\n",
      "Epoch 84/200\n",
      " - 29s - loss: 0.8385 - accuracy: 0.7802\n",
      "Epoch 85/200\n",
      " - 29s - loss: 0.2266 - accuracy: 0.9536\n",
      "Epoch 86/200\n",
      " - 29s - loss: 0.1525 - accuracy: 0.9740\n",
      "Epoch 87/200\n",
      " - 29s - loss: 0.0667 - accuracy: 0.9937\n",
      "Epoch 88/200\n",
      " - 29s - loss: 0.0338 - accuracy: 0.9991\n",
      "Epoch 89/200\n",
      " - 29s - loss: 0.0283 - accuracy: 0.9991\n",
      "Epoch 90/200\n",
      " - 29s - loss: 0.0574 - accuracy: 0.9926\n",
      "Epoch 91/200\n",
      " - 29s - loss: 0.1378 - accuracy: 0.9755\n",
      "Epoch 92/200\n",
      " - 29s - loss: 0.1041 - accuracy: 0.9822\n",
      "Epoch 00092: early stopping\n",
      "128240\n",
      "3206\n",
      "43527862\n",
      "Model: \"sequential_100\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 40, 10)            32060     \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 300)               373200    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 3206)              965006    \n",
      "=================================================================\n",
      "Total params: 1,370,266\n",
      "Trainable params: 1,370,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 28s - loss: 6.3911 - accuracy: 0.1128\n",
      "Epoch 2/200\n",
      " - 29s - loss: 5.9975 - accuracy: 0.1121\n",
      "Epoch 3/200\n",
      " - 28s - loss: 5.7807 - accuracy: 0.1177\n",
      "Epoch 4/200\n",
      " - 28s - loss: 5.5832 - accuracy: 0.1362\n",
      "Epoch 5/200\n",
      " - 28s - loss: 5.4006 - accuracy: 0.1472\n",
      "Epoch 6/200\n",
      " - 29s - loss: 5.2541 - accuracy: 0.1522\n",
      "Epoch 7/200\n",
      " - 28s - loss: 5.0897 - accuracy: 0.1536\n",
      "Epoch 8/200\n",
      " - 29s - loss: 4.9189 - accuracy: 0.1581\n",
      "Epoch 9/200\n",
      " - 28s - loss: 4.7344 - accuracy: 0.1612\n",
      "Epoch 10/200\n",
      " - 28s - loss: 4.5243 - accuracy: 0.1656\n",
      "Epoch 11/200\n",
      " - 29s - loss: 4.2974 - accuracy: 0.1724\n",
      "Epoch 12/200\n",
      " - 29s - loss: 4.0608 - accuracy: 0.1907\n",
      "Epoch 13/200\n",
      " - 28s - loss: 3.8626 - accuracy: 0.2148\n",
      "Epoch 14/200\n",
      " - 29s - loss: 3.6237 - accuracy: 0.2488\n",
      "Epoch 15/200\n",
      " - 29s - loss: 3.3587 - accuracy: 0.2962\n",
      "Epoch 16/200\n",
      " - 29s - loss: 3.1256 - accuracy: 0.3381\n",
      "Epoch 17/200\n",
      " - 28s - loss: 2.9108 - accuracy: 0.3840\n",
      "Epoch 18/200\n",
      " - 29s - loss: 2.7015 - accuracy: 0.4228\n",
      "Epoch 19/200\n",
      " - 29s - loss: 2.5038 - accuracy: 0.4685\n",
      "Epoch 20/200\n",
      " - 28s - loss: 2.3154 - accuracy: 0.5050\n",
      "Epoch 21/200\n",
      " - 29s - loss: 2.1395 - accuracy: 0.5392\n",
      "Epoch 22/200\n",
      " - 28s - loss: 1.9738 - accuracy: 0.5791\n",
      "Epoch 23/200\n",
      " - 29s - loss: 1.8161 - accuracy: 0.6141\n",
      "Epoch 24/200\n",
      " - 28s - loss: 1.6777 - accuracy: 0.6448\n",
      "Epoch 25/200\n",
      " - 29s - loss: 1.5260 - accuracy: 0.6808\n",
      "Epoch 26/200\n",
      " - 28s - loss: 1.4011 - accuracy: 0.7108\n",
      "Epoch 27/200\n",
      " - 29s - loss: 1.2829 - accuracy: 0.7398\n",
      "Epoch 28/200\n",
      " - 28s - loss: 1.1710 - accuracy: 0.7646\n",
      "Epoch 29/200\n",
      " - 28s - loss: 1.0609 - accuracy: 0.7952\n",
      "Epoch 30/200\n",
      " - 28s - loss: 0.9625 - accuracy: 0.8146\n",
      "Epoch 31/200\n",
      " - 29s - loss: 0.8628 - accuracy: 0.8423\n",
      "Epoch 32/200\n",
      " - 29s - loss: 0.7764 - accuracy: 0.8632\n",
      "Epoch 33/200\n",
      " - 29s - loss: 0.7018 - accuracy: 0.8831\n",
      "Epoch 34/200\n",
      " - 28s - loss: 0.6166 - accuracy: 0.9056\n",
      "Epoch 35/200\n",
      " - 29s - loss: 0.5515 - accuracy: 0.9167\n",
      "Epoch 36/200\n",
      " - 29s - loss: 0.4881 - accuracy: 0.9315\n",
      "Epoch 37/200\n",
      " - 28s - loss: 0.4303 - accuracy: 0.9432\n",
      "Epoch 38/200\n",
      " - 28s - loss: 0.3813 - accuracy: 0.9535\n",
      "Epoch 39/200\n",
      " - 28s - loss: 0.3286 - accuracy: 0.9657\n",
      "Epoch 40/200\n",
      " - 29s - loss: 0.3168 - accuracy: 0.9671\n",
      "Epoch 41/200\n",
      " - 28s - loss: 0.2958 - accuracy: 0.9692\n",
      "Epoch 42/200\n",
      " - 29s - loss: 0.2140 - accuracy: 0.9854\n",
      "Epoch 43/200\n",
      " - 28s - loss: 0.1661 - accuracy: 0.9927\n",
      "Epoch 44/200\n",
      " - 29s - loss: 0.1425 - accuracy: 0.9945\n",
      "Epoch 45/200\n",
      " - 28s - loss: 0.1359 - accuracy: 0.9947\n",
      "Epoch 46/200\n",
      " - 28s - loss: 0.2040 - accuracy: 0.9805\n",
      "Epoch 47/200\n",
      " - 28s - loss: 0.1977 - accuracy: 0.9824\n",
      "Epoch 48/200\n",
      " - 29s - loss: 0.1230 - accuracy: 0.9943\n",
      "Epoch 49/200\n",
      " - 29s - loss: 0.0725 - accuracy: 0.9992\n",
      "Epoch 50/200\n",
      " - 29s - loss: 0.0468 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      " - 28s - loss: 0.0356 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      " - 29s - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      " - 29s - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      " - 29s - loss: 0.0448 - accuracy: 0.9954\n",
      "Epoch 55/200\n",
      " - 28s - loss: 0.8033 - accuracy: 0.7879\n",
      "Epoch 56/200\n",
      " - 29s - loss: 0.1741 - accuracy: 0.9775\n",
      "Epoch 57/200\n",
      " - 29s - loss: 0.0701 - accuracy: 0.9980\n",
      "Epoch 58/200\n",
      " - 28s - loss: 0.0507 - accuracy: 0.9990\n",
      "Epoch 59/200\n",
      " - 28s - loss: 0.0312 - accuracy: 0.9999\n",
      "Epoch 60/200\n",
      " - 29s - loss: 0.0236 - accuracy: 0.9999\n",
      "Epoch 61/200\n",
      " - 29s - loss: 0.0190 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      " - 28s - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      " - 29s - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      " - 28s - loss: 0.0175 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      " - 29s - loss: 0.6822 - accuracy: 0.8179\n",
      "Epoch 66/200\n",
      " - 28s - loss: 0.2967 - accuracy: 0.9319\n",
      "Epoch 67/200\n",
      " - 29s - loss: 0.1131 - accuracy: 0.9887\n",
      "Epoch 68/200\n",
      " - 28s - loss: 0.0480 - accuracy: 0.9993\n",
      "Epoch 69/200\n",
      " - 29s - loss: 0.0289 - accuracy: 0.9999\n",
      "Epoch 70/200\n",
      " - 29s - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      " - 29s - loss: 0.0165 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      " - 28s - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      " - 29s - loss: 0.0159 - accuracy: 0.9999\n",
      "Epoch 74/200\n",
      " - 29s - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      " - 29s - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      " - 28s - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      " - 29s - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      " - 29s - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      " - 28s - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      " - 28s - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      " - 28s - loss: 0.9053 - accuracy: 0.7704\n",
      "Epoch 82/200\n",
      " - 29s - loss: 0.4854 - accuracy: 0.8673\n",
      "Epoch 83/200\n",
      " - 28s - loss: 0.1337 - accuracy: 0.9775\n",
      "Epoch 84/200\n",
      " - 29s - loss: 0.0574 - accuracy: 0.9965\n",
      "Epoch 85/200\n",
      " - 28s - loss: 0.0290 - accuracy: 0.9999\n",
      "Epoch 86/200\n",
      " - 29s - loss: 0.0191 - accuracy: 0.9999\n",
      "Epoch 87/200\n",
      " - 28s - loss: 0.0147 - accuracy: 0.9999\n",
      "Epoch 88/200\n",
      " - 29s - loss: 0.0115 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      " - 28s - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      " - 29s - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 00090: early stopping\n",
      "128240\n",
      "3206\n",
      "43527862\n",
      "Model: \"sequential_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 40, 25)            80150     \n",
      "_________________________________________________________________\n",
      "lstm_11 (LSTM)               (None, 300)               391200    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 3206)              965006    \n",
      "=================================================================\n",
      "Total params: 1,436,356\n",
      "Trainable params: 1,436,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 29s - loss: 6.3623 - accuracy: 0.1125\n",
      "Epoch 2/200\n",
      " - 29s - loss: 5.8648 - accuracy: 0.1178\n",
      "Epoch 3/200\n",
      " - 29s - loss: 5.5815 - accuracy: 0.1443\n",
      "Epoch 4/200\n",
      " - 29s - loss: 5.3763 - accuracy: 0.1526\n",
      "Epoch 5/200\n",
      " - 29s - loss: 5.1840 - accuracy: 0.1564\n",
      "Epoch 6/200\n",
      " - 29s - loss: 4.9874 - accuracy: 0.1595\n",
      "Epoch 7/200\n",
      " - 29s - loss: 4.7652 - accuracy: 0.1654\n",
      "Epoch 8/200\n",
      " - 29s - loss: 4.5194 - accuracy: 0.1705\n",
      "Epoch 9/200\n",
      " - 29s - loss: 4.2419 - accuracy: 0.1805\n",
      "Epoch 10/200\n",
      " - 29s - loss: 3.9493 - accuracy: 0.2037\n",
      "Epoch 11/200\n",
      " - 29s - loss: 3.6545 - accuracy: 0.2403\n",
      "Epoch 12/200\n",
      " - 29s - loss: 3.3542 - accuracy: 0.2923\n",
      "Epoch 13/200\n",
      " - 29s - loss: 3.0640 - accuracy: 0.3515\n",
      "Epoch 14/200\n",
      " - 29s - loss: 2.7877 - accuracy: 0.4020\n",
      "Epoch 15/200\n",
      " - 29s - loss: 2.5238 - accuracy: 0.4647\n",
      "Epoch 16/200\n",
      " - 29s - loss: 2.2841 - accuracy: 0.5137\n",
      "Epoch 17/200\n",
      " - 29s - loss: 2.0651 - accuracy: 0.5601\n",
      "Epoch 18/200\n",
      " - 29s - loss: 1.8732 - accuracy: 0.6030\n",
      "Epoch 19/200\n",
      " - 29s - loss: 1.6605 - accuracy: 0.6557\n",
      "Epoch 20/200\n",
      " - 29s - loss: 1.4869 - accuracy: 0.6971\n",
      "Epoch 21/200\n",
      " - 29s - loss: 1.3278 - accuracy: 0.7362\n",
      "Epoch 22/200\n",
      " - 29s - loss: 1.1773 - accuracy: 0.7709\n",
      "Epoch 23/200\n",
      " - 29s - loss: 1.0408 - accuracy: 0.8040\n",
      "Epoch 24/200\n",
      " - 29s - loss: 0.9128 - accuracy: 0.8370\n",
      "Epoch 25/200\n",
      " - 29s - loss: 0.7980 - accuracy: 0.8681\n",
      "Epoch 26/200\n",
      " - 29s - loss: 0.6838 - accuracy: 0.8937\n",
      "Epoch 27/200\n",
      " - 29s - loss: 0.5878 - accuracy: 0.9170\n",
      "Epoch 28/200\n",
      " - 29s - loss: 0.5103 - accuracy: 0.9354\n",
      "Epoch 29/200\n",
      " - 29s - loss: 0.4307 - accuracy: 0.9498\n",
      "Epoch 30/200\n",
      " - 29s - loss: 0.3615 - accuracy: 0.9649\n",
      "Epoch 31/200\n",
      " - 29s - loss: 0.3138 - accuracy: 0.9735\n",
      "Epoch 32/200\n",
      " - 29s - loss: 0.2417 - accuracy: 0.9862\n",
      "Epoch 33/200\n",
      " - 29s - loss: 0.1845 - accuracy: 0.9937\n",
      "Epoch 34/200\n",
      " - 29s - loss: 0.1491 - accuracy: 0.9962\n",
      "Epoch 35/200\n",
      " - 29s - loss: 0.1231 - accuracy: 0.9978\n",
      "Epoch 36/200\n",
      " - 29s - loss: 0.1202 - accuracy: 0.9980\n",
      "Epoch 37/200\n",
      " - 29s - loss: 0.0894 - accuracy: 0.9990\n",
      "Epoch 38/200\n",
      " - 29s - loss: 0.0831 - accuracy: 0.9990\n",
      "Epoch 39/200\n",
      " - 29s - loss: 0.0606 - accuracy: 0.9997\n",
      "Epoch 40/200\n",
      " - 29s - loss: 0.0398 - accuracy: 0.9999\n",
      "Epoch 41/200\n",
      " - 29s - loss: 0.0269 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      " - 29s - loss: 0.4069 - accuracy: 0.8978\n",
      "Epoch 43/200\n",
      " - 29s - loss: 0.2205 - accuracy: 0.9648\n",
      "Epoch 44/200\n",
      " - 29s - loss: 0.0591 - accuracy: 0.9988\n",
      "Epoch 45/200\n",
      " - 29s - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      " - 30s - loss: 0.0212 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      " - 29s - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      " - 29s - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      " - 29s - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      " - 29s - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      " - 29s - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      " - 29s - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      " - 29s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      " - 29s - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      " - 29s - loss: 0.8229 - accuracy: 0.7752\n",
      "Epoch 56/200\n",
      " - 29s - loss: 0.2706 - accuracy: 0.9442\n",
      "Epoch 57/200\n",
      " - 29s - loss: 0.0707 - accuracy: 0.9959\n",
      "Epoch 58/200\n",
      " - 29s - loss: 0.0397 - accuracy: 0.9990\n",
      "Epoch 59/200\n",
      " - 29s - loss: 0.0234 - accuracy: 0.9999\n",
      "Epoch 60/200\n",
      " - 29s - loss: 0.0157 - accuracy: 0.9999\n",
      "Epoch 61/200\n",
      " - 29s - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      " - 30s - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      " - 29s - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      " - 29s - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 00064: early stopping\n",
      "128240\n",
      "3206\n",
      "43527862\n",
      "Model: \"sequential_102\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 40, 50)            160300    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 300)               421200    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3206)              965006    \n",
      "=================================================================\n",
      "Total params: 1,546,506\n",
      "Trainable params: 1,546,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 30s - loss: 6.3636 - accuracy: 0.1131\n",
      "Epoch 2/200\n",
      " - 30s - loss: 5.7920 - accuracy: 0.1294\n",
      "Epoch 3/200\n",
      " - 30s - loss: 5.5553 - accuracy: 0.1466\n",
      "Epoch 4/200\n",
      " - 30s - loss: 5.3936 - accuracy: 0.1519\n",
      "Epoch 5/200\n",
      " - 30s - loss: 5.2463 - accuracy: 0.1557\n",
      "Epoch 6/200\n",
      " - 30s - loss: 5.0990 - accuracy: 0.1581\n",
      "Epoch 7/200\n",
      " - 30s - loss: 4.9418 - accuracy: 0.1630\n",
      "Epoch 8/200\n",
      " - 30s - loss: 4.7574 - accuracy: 0.1659\n",
      "Epoch 9/200\n",
      " - 30s - loss: 4.5500 - accuracy: 0.1697\n",
      "Epoch 10/200\n",
      " - 30s - loss: 4.3125 - accuracy: 0.1760\n",
      "Epoch 11/200\n",
      " - 30s - loss: 4.0442 - accuracy: 0.1949\n",
      "Epoch 12/200\n",
      " - 30s - loss: 3.7557 - accuracy: 0.2374\n",
      "Epoch 13/200\n",
      " - 30s - loss: 3.4595 - accuracy: 0.2833\n",
      "Epoch 14/200\n",
      " - 30s - loss: 3.1543 - accuracy: 0.3369\n",
      "Epoch 15/200\n",
      " - 30s - loss: 2.8672 - accuracy: 0.3890\n",
      "Epoch 16/200\n",
      " - 30s - loss: 2.5970 - accuracy: 0.4413\n",
      "Epoch 17/200\n",
      " - 35s - loss: 2.3449 - accuracy: 0.4869\n",
      "Epoch 18/200\n",
      " - 31s - loss: 2.1116 - accuracy: 0.5377\n",
      "Epoch 19/200\n",
      " - 31s - loss: 1.8970 - accuracy: 0.5892\n",
      "Epoch 20/200\n",
      " - 31s - loss: 1.6944 - accuracy: 0.6309\n",
      "Epoch 21/200\n",
      " - 33s - loss: 1.5073 - accuracy: 0.6769\n",
      "Epoch 22/200\n",
      " - 31s - loss: 1.3344 - accuracy: 0.7152\n",
      "Epoch 23/200\n",
      " - 31s - loss: 1.1744 - accuracy: 0.7561\n",
      "Epoch 24/200\n",
      " - 32s - loss: 1.0336 - accuracy: 0.7924\n",
      "Epoch 25/200\n",
      " - 31s - loss: 0.8848 - accuracy: 0.8299\n",
      "Epoch 26/200\n",
      " - 30s - loss: 0.7755 - accuracy: 0.8600\n",
      "Epoch 27/200\n",
      " - 31s - loss: 0.6685 - accuracy: 0.8879\n",
      "Epoch 28/200\n",
      " - 30s - loss: 0.5429 - accuracy: 0.9157\n",
      "Epoch 29/200\n",
      " - 31s - loss: 0.4486 - accuracy: 0.9405\n",
      "Epoch 30/200\n",
      " - 30s - loss: 0.3652 - accuracy: 0.9596\n",
      "Epoch 31/200\n",
      " - 31s - loss: 0.3111 - accuracy: 0.9698\n",
      "Epoch 32/200\n",
      " - 30s - loss: 0.2443 - accuracy: 0.9824\n",
      "Epoch 33/200\n",
      " - 31s - loss: 0.1924 - accuracy: 0.9889\n",
      "Epoch 34/200\n",
      " - 30s - loss: 0.1564 - accuracy: 0.9916\n",
      "Epoch 35/200\n",
      " - 31s - loss: 0.1339 - accuracy: 0.9950\n",
      "Epoch 36/200\n",
      " - 30s - loss: 0.1067 - accuracy: 0.9967\n",
      "Epoch 37/200\n",
      " - 31s - loss: 0.1075 - accuracy: 0.9959\n",
      "Epoch 38/200\n",
      " - 30s - loss: 0.1072 - accuracy: 0.9946\n",
      "Epoch 39/200\n",
      " - 30s - loss: 0.1733 - accuracy: 0.9801\n",
      "Epoch 40/200\n",
      " - 31s - loss: 0.1611 - accuracy: 0.9812\n",
      "Epoch 41/200\n",
      " - 31s - loss: 0.0658 - accuracy: 0.9976\n",
      "Epoch 42/200\n",
      " - 31s - loss: 0.0287 - accuracy: 0.9997\n",
      "Epoch 43/200\n",
      " - 33s - loss: 0.0163 - accuracy: 0.9999\n",
      "Epoch 44/200\n",
      " - 31s - loss: 0.0118 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      " - 31s - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      " - 31s - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      " - 30s - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      " - 32s - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      " - 31s - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      " - 33s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      " - 35s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      " - 36s - loss: 0.7060 - accuracy: 0.8220\n",
      "Epoch 53/200\n",
      " - 34s - loss: 0.7598 - accuracy: 0.7984\n",
      "Epoch 54/200\n",
      " - 31s - loss: 0.1833 - accuracy: 0.9679\n",
      "Epoch 55/200\n",
      " - 30s - loss: 0.0676 - accuracy: 0.9958\n",
      "Epoch 56/200\n",
      " - 32s - loss: 0.0296 - accuracy: 0.9999\n",
      "Epoch 57/200\n",
      " - 30s - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      " - 30s - loss: 0.0143 - accuracy: 0.9996\n",
      "Epoch 59/200\n",
      " - 30s - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      " - 31s - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      " - 31s - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 00061: early stopping\n",
      "128240\n",
      "3206\n",
      "43527862\n",
      "Model: \"sequential_103\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_13 (Embedding)     (None, 40, 100)           320600    \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 300)               481200    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3206)              965006    \n",
      "=================================================================\n",
      "Total params: 1,766,806\n",
      "Trainable params: 1,766,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 33s - loss: 6.3480 - accuracy: 0.1131\n",
      "Epoch 2/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bce9f1641183>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mtest2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetTrainSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mtest2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mtest2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0museWordEmbedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddingSize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumEpoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'model_unit400'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive - California Institute of Technology\\class\\Machine Learning and Data Mining\\Project 3\\git\\CS155_PROJECT3\\LSTMforSonnet.py\u001b[0m in \u001b[0;36mTrain\u001b[1;34m(self, useWordEmbedding, embeddingSize, patience, numEpoch, fileName)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0museWordEmbedding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_orig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumEpoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumEpoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# LSTM with word embedding - dimension: 5, 10, 25, with Spenser \n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "embedSize = [10, 50, 100, 200]\n",
    "for dim in embedSize:\n",
    "    test2 = LSTM_word()\n",
    "    test2.SonnetLoader('Spenser_v2')\n",
    "    test2.getTrainSeq()\n",
    "    test2.getMapping()\n",
    "    test2.Train(useWordEmbedding = True, embeddingSize = dim, numEpoch = 200, fileName = 'model_Spenser')\n",
    "    \n",
    "for dim in embedSize:\n",
    "    test2 = LSTM_word(LSTM_numUnits = 400)\n",
    "    test2.SonnetLoader('Spenser_v2')\n",
    "    test2.getTrainSeq()\n",
    "    test2.getMapping()\n",
    "    test2.Train(useWordEmbedding = True, embeddingSize = dim, numEpoch = 200, fileName = 'model_Spenser_unit400')    \n",
    "    \n",
    "embedSize = [5, 10, 25, 50, 100, 200]\n",
    "for dim in embedSize:\n",
    "    test2 = LSTM_word(LSTM_numUnits = 300)\n",
    "    test2.SonnetLoader('shakespeare')\n",
    "    test2.getTrainSeq()\n",
    "    test2.getMapping()\n",
    "    test2.Train(useWordEmbedding = True, embeddingSize = dim, numEpoch = 200, fileName = 'model_unit400') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128240\n",
      "3206\n",
      "43527862\n",
      "Model: \"sequential_104\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 40, 5)             16030     \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 400)               649600    \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 1,951,236\n",
      "Trainable params: 1,951,236\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 45s - loss: 6.3883 - accuracy: 0.1092\n",
      "Epoch 2/200\n",
      " - 45s - loss: 5.9565 - accuracy: 0.1134\n",
      "Epoch 3/200\n",
      " - 45s - loss: 5.7012 - accuracy: 0.1333\n",
      "Epoch 4/200\n",
      " - 45s - loss: 5.4650 - accuracy: 0.1485\n",
      "Epoch 5/200\n",
      " - 46s - loss: 5.2809 - accuracy: 0.1515\n",
      "Epoch 6/200\n",
      " - 45s - loss: 5.0935 - accuracy: 0.1532\n",
      "Epoch 7/200\n",
      " - 46s - loss: 4.8783 - accuracy: 0.1586\n",
      "Epoch 8/200\n",
      " - 45s - loss: 4.6335 - accuracy: 0.1612\n",
      "Epoch 9/200\n",
      " - 45s - loss: 4.3496 - accuracy: 0.1692\n",
      "Epoch 10/200\n",
      " - 46s - loss: 4.0397 - accuracy: 0.1846\n",
      "Epoch 11/200\n",
      " - 45s - loss: 3.7049 - accuracy: 0.2186\n",
      "Epoch 12/200\n",
      " - 45s - loss: 3.3621 - accuracy: 0.2775\n",
      "Epoch 13/200\n",
      " - 45s - loss: 3.0187 - accuracy: 0.3460\n",
      "Epoch 14/200\n",
      " - 44s - loss: 2.6969 - accuracy: 0.4156\n",
      "Epoch 15/200\n",
      " - 46s - loss: 2.3859 - accuracy: 0.4813\n",
      "Epoch 16/200\n",
      " - 45s - loss: 2.1059 - accuracy: 0.5434\n",
      "Epoch 17/200\n",
      " - 44s - loss: 1.8486 - accuracy: 0.6088\n",
      "Epoch 18/200\n",
      " - 46s - loss: 1.6054 - accuracy: 0.6619\n",
      "Epoch 19/200\n",
      " - 45s - loss: 1.3924 - accuracy: 0.7153\n",
      "Epoch 20/200\n",
      " - 44s - loss: 1.1878 - accuracy: 0.7663\n",
      "Epoch 21/200\n",
      " - 44s - loss: 1.0121 - accuracy: 0.8112\n",
      "Epoch 22/200\n",
      " - 44s - loss: 0.8502 - accuracy: 0.8534\n",
      "Epoch 23/200\n",
      " - 45s - loss: 0.7023 - accuracy: 0.8917\n",
      "Epoch 24/200\n",
      " - 44s - loss: 0.5778 - accuracy: 0.9199\n",
      "Epoch 25/200\n",
      " - 45s - loss: 0.4974 - accuracy: 0.9353\n",
      "Epoch 26/200\n",
      " - 45s - loss: 0.3704 - accuracy: 0.9645\n",
      "Epoch 27/200\n",
      " - 44s - loss: 0.2872 - accuracy: 0.9789\n",
      "Epoch 28/200\n",
      " - 44s - loss: 0.2221 - accuracy: 0.9884\n",
      "Epoch 29/200\n",
      " - 44s - loss: 0.1686 - accuracy: 0.9946\n",
      "Epoch 30/200\n",
      " - 44s - loss: 0.1680 - accuracy: 0.9937\n",
      "Epoch 31/200\n",
      " - 45s - loss: 0.1458 - accuracy: 0.9948\n",
      "Epoch 32/200\n",
      " - 45s - loss: 0.1196 - accuracy: 0.9951\n",
      "Epoch 33/200\n",
      " - 45s - loss: 0.1208 - accuracy: 0.9957\n",
      "Epoch 34/200\n",
      " - 47s - loss: 0.0862 - accuracy: 0.9973\n",
      "Epoch 35/200\n",
      " - 53s - loss: 0.0368 - accuracy: 0.9999\n",
      "Epoch 36/200\n",
      " - 45s - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      " - 45s - loss: 0.0154 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      " - 44s - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      " - 45s - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      " - 45s - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      " - 44s - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      " - 45s - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      " - 45s - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      " - 45s - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      " - 45s - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      " - 45s - loss: 1.6676 - accuracy: 0.5835\n",
      "Epoch 47/200\n",
      " - 45s - loss: 0.5549 - accuracy: 0.8640\n",
      "Epoch 48/200\n",
      " - 45s - loss: 0.1656 - accuracy: 0.9783\n",
      "Epoch 49/200\n",
      " - 45s - loss: 0.0687 - accuracy: 0.9976\n",
      "Epoch 50/200\n",
      " - 45s - loss: 0.0349 - accuracy: 0.9999\n",
      "Epoch 51/200\n",
      " - 45s - loss: 0.0215 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      " - 45s - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      " - 47s - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      " - 45s - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      " - 46s - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 00055: early stopping\n",
      "128240\n",
      "3206\n",
      "43527862\n",
      "Model: \"sequential_105\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 40, 10)            32060     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 400)               657600    \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 1,975,266\n",
      "Trainable params: 1,975,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 45s - loss: 6.3710 - accuracy: 0.1122\n",
      "Epoch 2/200\n",
      " - 45s - loss: 5.8956 - accuracy: 0.1171\n",
      "Epoch 3/200\n",
      " - 45s - loss: 5.6029 - accuracy: 0.1424\n",
      "Epoch 4/200\n",
      " - 47s - loss: 5.3897 - accuracy: 0.1510\n",
      "Epoch 5/200\n",
      " - 46s - loss: 5.2018 - accuracy: 0.1546\n",
      "Epoch 6/200\n",
      " - 45s - loss: 4.9955 - accuracy: 0.1582\n",
      "Epoch 7/200\n",
      " - 45s - loss: 4.7571 - accuracy: 0.1602\n",
      "Epoch 8/200\n",
      " - 45s - loss: 4.4665 - accuracy: 0.1669\n",
      "Epoch 9/200\n",
      " - 45s - loss: 4.1417 - accuracy: 0.1845\n",
      "Epoch 10/200\n",
      " - 45s - loss: 3.7829 - accuracy: 0.2168\n",
      "Epoch 11/200\n",
      " - 45s - loss: 3.4080 - accuracy: 0.2713\n",
      "Epoch 12/200\n",
      " - 45s - loss: 3.3859 - accuracy: 0.3222\n",
      "Epoch 13/200\n",
      " - 46s - loss: 6.5602 - accuracy: 0.1125\n",
      "Epoch 14/200\n",
      " - 45s - loss: 5.7547 - accuracy: 0.1219\n",
      "Epoch 15/200\n",
      " - 45s - loss: 5.5589 - accuracy: 0.1288\n",
      "Epoch 16/200\n",
      " - 46s - loss: 5.4975 - accuracy: 0.1318\n",
      "Epoch 17/200\n",
      " - 45s - loss: 5.5405 - accuracy: 0.1361\n",
      "Epoch 18/200\n",
      " - 46s - loss: 5.2242 - accuracy: 0.1442\n",
      "Epoch 19/200\n",
      " - 47s - loss: 5.0168 - accuracy: 0.1525\n",
      "Epoch 20/200\n",
      " - 45s - loss: 4.8582 - accuracy: 0.1574\n",
      "Epoch 21/200\n",
      " - 45s - loss: 4.7225 - accuracy: 0.1637\n",
      "Epoch 22/200\n",
      " - 45s - loss: 4.5695 - accuracy: 0.1694\n",
      "Epoch 00022: early stopping\n",
      "128240\n",
      "3206\n",
      "43527862\n",
      "Model: \"sequential_106\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 40, 25)            80150     \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 400)               681600    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 2,047,356\n",
      "Trainable params: 2,047,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 46s - loss: 6.4078 - accuracy: 0.1111\n",
      "Epoch 2/200\n",
      " - 46s - loss: 5.9154 - accuracy: 0.1148\n",
      "Epoch 3/200\n",
      " - 46s - loss: 5.6223 - accuracy: 0.1414\n",
      "Epoch 4/200\n",
      " - 46s - loss: 5.3968 - accuracy: 0.1526\n",
      "Epoch 5/200\n",
      " - 47s - loss: 5.1970 - accuracy: 0.1556\n",
      "Epoch 6/200\n",
      " - 47s - loss: 4.9969 - accuracy: 0.1595\n",
      "Epoch 7/200\n",
      " - 47s - loss: 4.7681 - accuracy: 0.1645\n",
      "Epoch 8/200\n",
      " - 48s - loss: 4.5033 - accuracy: 0.1705\n",
      "Epoch 9/200\n",
      " - 47s - loss: 4.1983 - accuracy: 0.1851\n",
      "Epoch 10/200\n",
      " - 46s - loss: 3.8645 - accuracy: 0.2084\n",
      "Epoch 11/200\n",
      " - 46s - loss: 3.5092 - accuracy: 0.2629\n",
      "Epoch 12/200\n",
      " - 46s - loss: 3.1426 - accuracy: 0.3306\n",
      "Epoch 13/200\n",
      " - 46s - loss: 2.7991 - accuracy: 0.3966\n",
      "Epoch 14/200\n",
      " - 46s - loss: 2.4743 - accuracy: 0.4699\n",
      "Epoch 15/200\n",
      " - 46s - loss: 2.1752 - accuracy: 0.5268\n",
      "Epoch 16/200\n",
      " - 46s - loss: 1.9031 - accuracy: 0.5914\n",
      "Epoch 17/200\n",
      " - 46s - loss: 1.6595 - accuracy: 0.6486\n",
      "Epoch 18/200\n",
      " - 46s - loss: 1.4345 - accuracy: 0.7063\n",
      "Epoch 19/200\n",
      " - 46s - loss: 1.2320 - accuracy: 0.7569\n",
      "Epoch 20/200\n",
      " - 46s - loss: 1.0432 - accuracy: 0.8039\n",
      "Epoch 21/200\n",
      " - 46s - loss: 0.8739 - accuracy: 0.8484\n",
      "Epoch 22/200\n",
      " - 46s - loss: 0.7330 - accuracy: 0.8822\n",
      "Epoch 23/200\n",
      " - 46s - loss: 0.5960 - accuracy: 0.9168\n",
      "Epoch 24/200\n",
      " - 47s - loss: 0.4769 - accuracy: 0.9440\n",
      "Epoch 25/200\n",
      " - 46s - loss: 0.3771 - accuracy: 0.9650\n",
      "Epoch 26/200\n",
      " - 46s - loss: 0.2971 - accuracy: 0.9779\n",
      "Epoch 27/200\n",
      " - 46s - loss: 0.2196 - accuracy: 0.9897\n",
      "Epoch 28/200\n",
      " - 46s - loss: 0.1667 - accuracy: 0.9948\n",
      "Epoch 29/200\n",
      " - 46s - loss: 0.1366 - accuracy: 0.9963\n",
      "Epoch 30/200\n",
      " - 46s - loss: 0.1030 - accuracy: 0.9993\n",
      "Epoch 31/200\n",
      " - 46s - loss: 0.0681 - accuracy: 0.9996\n",
      "Epoch 32/200\n",
      " - 46s - loss: 0.0494 - accuracy: 0.9999\n",
      "Epoch 33/200\n",
      " - 46s - loss: 0.2736 - accuracy: 0.9535\n",
      "Epoch 34/200\n",
      " - 46s - loss: 0.2342 - accuracy: 0.9670\n",
      "Epoch 35/200\n",
      " - 47s - loss: 0.0555 - accuracy: 0.9996\n",
      "Epoch 36/200\n",
      " - 46s - loss: 0.0267 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      " - 46s - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      " - 46s - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      " - 46s - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      " - 46s - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      " - 46s - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      " - 46s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      " - 48s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      " - 46s - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      " - 48s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      " - 47s - loss: 0.0052 - accuracy: 0.9999\n",
      "Epoch 47/200\n",
      " - 47s - loss: 1.0934 - accuracy: 0.7094\n",
      "Epoch 48/200\n",
      " - 48s - loss: 0.1705 - accuracy: 0.9730\n",
      "Epoch 49/200\n",
      " - 47s - loss: 0.0427 - accuracy: 0.9993\n",
      "Epoch 50/200\n",
      " - 47s - loss: 0.0204 - accuracy: 0.9999\n",
      "Epoch 51/200\n",
      " - 46s - loss: 0.0142 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      " - 47s - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      " - 47s - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      " - 47s - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      " - 47s - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 00055: early stopping\n",
      "128240\n",
      "3206\n",
      "43527862\n",
      "Model: \"sequential_107\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_17 (Embedding)     (None, 40, 50)            160300    \n",
      "_________________________________________________________________\n",
      "lstm_17 (LSTM)               (None, 400)               721600    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 2,167,506\n",
      "Trainable params: 2,167,506\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 49s - loss: 6.3454 - accuracy: 0.1130\n",
      "Epoch 2/200\n",
      " - 51s - loss: 5.7397 - accuracy: 0.1357\n",
      "Epoch 3/200\n",
      " - 50s - loss: 5.4593 - accuracy: 0.1502\n",
      "Epoch 4/200\n",
      " - 48s - loss: 5.2314 - accuracy: 0.1575\n",
      "Epoch 5/200\n",
      " - 49s - loss: 5.0052 - accuracy: 0.1619\n",
      "Epoch 6/200\n",
      " - 49s - loss: 4.7350 - accuracy: 0.1680\n",
      "Epoch 7/200\n",
      " - 48s - loss: 4.4001 - accuracy: 0.1804\n",
      "Epoch 8/200\n",
      " - 53s - loss: 3.9957 - accuracy: 0.2018\n",
      "Epoch 9/200\n",
      " - 48s - loss: 3.5513 - accuracy: 0.2547\n",
      "Epoch 10/200\n",
      " - 49s - loss: 3.0860 - accuracy: 0.3377\n",
      "Epoch 11/200\n",
      " - 49s - loss: 2.6371 - accuracy: 0.4266\n",
      "Epoch 12/200\n",
      " - 50s - loss: 2.2243 - accuracy: 0.5180\n",
      "Epoch 13/200\n",
      " - 51s - loss: 1.8510 - accuracy: 0.6055\n",
      "Epoch 14/200\n",
      " - 48s - loss: 1.5260 - accuracy: 0.6881\n",
      "Epoch 15/200\n",
      " - 49s - loss: 1.2383 - accuracy: 0.7604\n",
      "Epoch 16/200\n",
      " - 49s - loss: 0.9792 - accuracy: 0.8254\n",
      "Epoch 17/200\n",
      " - 51s - loss: 0.7590 - accuracy: 0.8826\n",
      "Epoch 18/200\n",
      " - 50s - loss: 0.5733 - accuracy: 0.9251\n",
      "Epoch 19/200\n",
      " - 49s - loss: 0.4236 - accuracy: 0.9558\n",
      "Epoch 20/200\n",
      " - 51s - loss: 0.2987 - accuracy: 0.9803\n",
      "Epoch 21/200\n",
      " - 49s - loss: 0.2014 - accuracy: 0.9928\n",
      "Epoch 22/200\n",
      " - 51s - loss: 0.1347 - accuracy: 0.9979\n",
      "Epoch 23/200\n",
      " - 47s - loss: 0.0883 - accuracy: 0.9997\n",
      "Epoch 24/200\n",
      " - 48s - loss: 0.0586 - accuracy: 0.9999\n",
      "Epoch 25/200\n",
      " - 48s - loss: 0.0395 - accuracy: 1.0000\n",
      "Epoch 26/200\n",
      " - 48s - loss: 0.0306 - accuracy: 1.0000\n",
      "Epoch 27/200\n",
      " - 48s - loss: 0.0249 - accuracy: 0.9999\n",
      "Epoch 28/200\n",
      " - 48s - loss: 0.4483 - accuracy: 0.9006\n",
      "Epoch 29/200\n",
      " - 48s - loss: 0.2001 - accuracy: 0.9751\n",
      "Epoch 30/200\n",
      " - 49s - loss: 0.0414 - accuracy: 0.9994\n",
      "Epoch 31/200\n",
      " - 49s - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 32/200\n",
      " - 49s - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 33/200\n",
      " - 48s - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 34/200\n",
      " - 49s - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 35/200\n",
      " - 49s - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 36/200\n",
      " - 49s - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      " - 52s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      " - 48s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      " - 48s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      " - 50s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      " - 48s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      " - 49s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      " - 49s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      " - 48s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      " - 49s - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      " - 48s - loss: 9.5566e-04 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      " - 50s - loss: 7.8084e-04 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      " - 48s - loss: 6.3722e-04 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      " - 48s - loss: 5.3219e-04 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      " - 49s - loss: 4.2853e-04 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      " - 48s - loss: 0.0567 - accuracy: 0.9862\n",
      "Epoch 52/200\n",
      " - 49s - loss: 1.1879 - accuracy: 0.6799\n",
      "Epoch 53/200\n",
      " - 49s - loss: 0.1054 - accuracy: 0.9846\n",
      "Epoch 54/200\n",
      " - 50s - loss: 0.0260 - accuracy: 0.9995\n",
      "Epoch 55/200\n",
      " - 50s - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      " - 50s - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      " - 51s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      " - 48s - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      " - 49s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      " - 49s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 00060: early stopping\n",
      "128240\n",
      "3206\n",
      "43527862\n",
      "Model: \"sequential_108\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_18 (Embedding)     (None, 40, 100)           320600    \n",
      "_________________________________________________________________\n",
      "lstm_18 (LSTM)               (None, 400)               801600    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 2,407,806\n",
      "Trainable params: 2,407,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 54s - loss: 6.3680 - accuracy: 0.1128\n",
      "Epoch 2/200\n",
      " - 53s - loss: 5.8805 - accuracy: 0.1191\n",
      "Epoch 3/200\n",
      " - 52s - loss: 5.5860 - accuracy: 0.1433\n",
      "Epoch 4/200\n",
      " - 52s - loss: 5.3504 - accuracy: 0.1534\n",
      "Epoch 5/200\n",
      " - 52s - loss: 5.1419 - accuracy: 0.1603\n",
      "Epoch 6/200\n",
      " - 52s - loss: 4.9562 - accuracy: 0.1656\n",
      "Epoch 7/200\n",
      " - 52s - loss: 4.7040 - accuracy: 0.1735\n",
      "Epoch 8/200\n",
      " - 53s - loss: 4.5066 - accuracy: 0.1813\n",
      "Epoch 9/200\n",
      " - 52s - loss: 4.3394 - accuracy: 0.1876\n",
      "Epoch 10/200\n",
      " - 52s - loss: 3.9570 - accuracy: 0.2123\n",
      "Epoch 11/200\n",
      " - 52s - loss: 3.6399 - accuracy: 0.2507\n",
      "Epoch 12/200\n",
      " - 52s - loss: 3.3210 - accuracy: 0.3068\n",
      "Epoch 13/200\n",
      " - 55s - loss: 2.9865 - accuracy: 0.3685\n",
      "Epoch 14/200\n",
      " - 53s - loss: 2.6751 - accuracy: 0.4306\n",
      "Epoch 15/200\n",
      " - 53s - loss: 2.3765 - accuracy: 0.4895\n",
      "Epoch 16/200\n",
      " - 53s - loss: 2.0961 - accuracy: 0.5551\n",
      "Epoch 17/200\n",
      " - 54s - loss: 1.8426 - accuracy: 0.6063\n",
      "Epoch 18/200\n",
      " - 56s - loss: 1.6067 - accuracy: 0.6577\n",
      "Epoch 19/200\n",
      " - 53s - loss: 1.3831 - accuracy: 0.7146\n",
      "Epoch 20/200\n",
      " - 54s - loss: 1.1844 - accuracy: 0.7593\n",
      "Epoch 21/200\n",
      " - 54s - loss: 0.9975 - accuracy: 0.8089\n",
      "Epoch 22/200\n",
      " - 56s - loss: 0.8252 - accuracy: 0.8511\n",
      "Epoch 23/200\n",
      " - 56s - loss: 0.6752 - accuracy: 0.8909\n",
      "Epoch 24/200\n",
      " - 55s - loss: 0.5360 - accuracy: 0.9236\n",
      "Epoch 25/200\n",
      " - 53s - loss: 0.4870 - accuracy: 0.9351\n",
      "Epoch 26/200\n",
      " - 53s - loss: 0.3334 - accuracy: 0.9650\n",
      "Epoch 27/200\n",
      " - 53s - loss: 0.2515 - accuracy: 0.9823\n",
      "Epoch 28/200\n",
      " - 56s - loss: 0.1815 - accuracy: 0.9923\n",
      "Epoch 29/200\n",
      " - 65s - loss: 0.1239 - accuracy: 0.9978\n",
      "Epoch 30/200\n",
      " - 53s - loss: 0.0940 - accuracy: 0.9989\n",
      "Epoch 31/200\n",
      " - 53s - loss: 0.0647 - accuracy: 1.0000\n",
      "Epoch 32/200\n",
      " - 53s - loss: 0.0451 - accuracy: 0.9999\n",
      "Epoch 33/200\n",
      " - 52s - loss: 0.0321 - accuracy: 1.0000\n",
      "Epoch 34/200\n",
      " - 52s - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 35/200\n",
      " - 52s - loss: 0.0187 - accuracy: 1.0000\n",
      "Epoch 36/200\n",
      " - 52s - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      " - 52s - loss: 0.0181 - accuracy: 0.9993\n",
      "Epoch 38/200\n",
      " - 52s - loss: 0.7249 - accuracy: 0.8085\n",
      "Epoch 39/200\n",
      " - 52s - loss: 0.1063 - accuracy: 0.9912\n",
      "Epoch 40/200\n",
      " - 52s - loss: 0.0313 - accuracy: 0.9998\n",
      "Epoch 41/200\n",
      " - 53s - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      " - 53s - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      " - 52s - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      " - 52s - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      " - 52s - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      " - 52s - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      " - 52s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      " - 51s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      " - 52s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      " - 52s - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      " - 54s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      " - 54s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      " - 52s - loss: 0.0043 - accuracy: 0.9996\n",
      "Epoch 54/200\n",
      " - 52s - loss: 0.7569 - accuracy: 0.8018\n",
      "Epoch 55/200\n",
      " - 54s - loss: 0.0875 - accuracy: 0.9879\n",
      "Epoch 56/200\n",
      " - 54s - loss: 0.0215 - accuracy: 0.9995\n",
      "Epoch 57/200\n",
      " - 53s - loss: 0.0104 - accuracy: 0.9999\n",
      "Epoch 58/200\n",
      " - 55s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      " - 54s - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      " - 53s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      " - 52s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      " - 52s - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 00062: early stopping\n",
      "128240\n",
      "3206\n",
      "43527862\n",
      "Model: \"sequential_109\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (None, 40, 200)           641200    \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 2,888,406\n",
      "Trainable params: 2,888,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 62s - loss: 6.2969 - accuracy: 0.1162\n",
      "Epoch 2/200\n",
      " - 63s - loss: 5.6309 - accuracy: 0.1443\n",
      "Epoch 3/200\n",
      " - 62s - loss: 5.3391 - accuracy: 0.1553\n",
      "Epoch 4/200\n",
      " - 63s - loss: 5.0677 - accuracy: 0.1662\n",
      "Epoch 5/200\n",
      " - 62s - loss: 4.7402 - accuracy: 0.1754\n",
      "Epoch 6/200\n",
      " - 63s - loss: 4.3234 - accuracy: 0.1903\n",
      "Epoch 7/200\n",
      " - 62s - loss: 3.8225 - accuracy: 0.2305\n",
      "Epoch 8/200\n",
      " - 63s - loss: 3.2549 - accuracy: 0.3097\n",
      "Epoch 9/200\n",
      " - 62s - loss: 2.6731 - accuracy: 0.4311\n",
      "Epoch 10/200\n",
      " - 62s - loss: 2.1423 - accuracy: 0.5478\n",
      "Epoch 11/200\n",
      " - 62s - loss: 1.6829 - accuracy: 0.6579\n",
      "Epoch 12/200\n",
      " - 63s - loss: 1.2872 - accuracy: 0.7547\n",
      "Epoch 13/200\n",
      " - 62s - loss: 0.9622 - accuracy: 0.8346\n",
      "Epoch 14/200\n",
      " - 63s - loss: 0.6953 - accuracy: 0.8968\n",
      "Epoch 15/200\n",
      " - 62s - loss: 0.4764 - accuracy: 0.9446\n",
      "Epoch 16/200\n",
      " - 62s - loss: 0.3163 - accuracy: 0.9753\n",
      "Epoch 17/200\n",
      " - 62s - loss: 0.2025 - accuracy: 0.9911\n",
      "Epoch 18/200\n",
      " - 62s - loss: 0.1243 - accuracy: 0.9979\n",
      "Epoch 19/200\n",
      " - 62s - loss: 0.0735 - accuracy: 0.9998\n",
      "Epoch 20/200\n",
      " - 62s - loss: 0.0467 - accuracy: 1.0000\n",
      "Epoch 21/200\n",
      " - 62s - loss: 0.0323 - accuracy: 1.0000\n",
      "Epoch 22/200\n",
      " - 62s - loss: 0.0237 - accuracy: 1.0000\n",
      "Epoch 23/200\n",
      " - 62s - loss: 0.0180 - accuracy: 1.0000\n",
      "Epoch 24/200\n",
      " - 62s - loss: 0.0138 - accuracy: 1.0000\n",
      "Epoch 25/200\n",
      " - 62s - loss: 0.0107 - accuracy: 1.0000\n",
      "Epoch 26/200\n",
      " - 62s - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 27/200\n",
      " - 63s - loss: 0.5186 - accuracy: 0.8774\n",
      "Epoch 28/200\n",
      " - 62s - loss: 0.1636 - accuracy: 0.9787\n",
      "Epoch 29/200\n",
      " - 64s - loss: 0.0256 - accuracy: 0.9998\n",
      "Epoch 30/200\n",
      " - 63s - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 31/200\n",
      " - 64s - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 32/200\n",
      " - 63s - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 33/200\n",
      " - 65s - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 34/200\n",
      " - 62s - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 35/200\n",
      " - 62s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 36/200\n",
      " - 62s - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      " - 62s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      " - 63s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      " - 63s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      " - 63s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      " - 63s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      " - 66s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      " - 65s - loss: 9.3769e-04 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      " - 63s - loss: 7.7704e-04 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      " - 62s - loss: 6.4122e-04 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      " - 63s - loss: 5.2738e-04 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      " - 62s - loss: 4.3351e-04 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      " - 63s - loss: 3.5525e-04 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      " - 62s - loss: 2.9040e-04 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      " - 63s - loss: 2.3739e-04 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      " - 62s - loss: 1.9332e-04 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      " - 66s - loss: 1.5742e-04 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      " - 63s - loss: 1.2794e-04 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      " - 64s - loss: 1.0413e-04 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      " - 62s - loss: 8.4800e-05 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      " - 63s - loss: 0.6972 - accuracy: 0.8092\n",
      "Epoch 57/200\n",
      " - 63s - loss: 0.1520 - accuracy: 0.9670\n",
      "Epoch 58/200\n",
      " - 62s - loss: 0.0177 - accuracy: 0.9993\n",
      "Epoch 59/200\n",
      " - 62s - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      " - 62s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      " - 62s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      " - 62s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      " - 62s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      " - 62s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      " - 62s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 00065: early stopping\n"
     ]
    }
   ],
   "source": [
    "embedSize = [5, 10, 25, 50, 100, 200]\n",
    "for dim in embedSize:\n",
    "    test2 = LSTM_word(LSTM_numUnits = 400)\n",
    "    test2.SonnetLoader('shakespeare')\n",
    "    test2.getTrainSeq()\n",
    "    test2.getMapping()\n",
    "    test2.Train(useWordEmbedding = True, embeddingSize = dim, numEpoch = 200, fileName = 'model_unit400') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to C:\\Users\\HyeongChan\n",
      "[nltk_data]     Jo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               1322800   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3206)              323806    \n",
      "=================================================================\n",
      "Total params: 1,646,606\n",
      "Trainable params: 1,646,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      " - 52s - loss: 6.4254 - accuracy: 0.1125\n",
      "Epoch 2/200\n",
      " - 52s - loss: 6.0200 - accuracy: 0.1134\n",
      "Epoch 3/200\n",
      " - 51s - loss: 5.9665 - accuracy: 0.1134\n",
      "Epoch 4/200\n",
      " - 52s - loss: 5.9059 - accuracy: 0.1137\n",
      "Epoch 5/200\n",
      " - 52s - loss: 5.7505 - accuracy: 0.1279\n",
      "Epoch 6/200\n",
      " - 53s - loss: 5.6205 - accuracy: 0.1436\n",
      "Epoch 7/200\n",
      " - 52s - loss: 5.5116 - accuracy: 0.1494\n",
      "Epoch 8/200\n",
      " - 53s - loss: 5.4100 - accuracy: 0.1550\n",
      "Epoch 9/200\n",
      " - 52s - loss: 5.3124 - accuracy: 0.1595\n",
      "Epoch 10/200\n",
      " - 53s - loss: 5.2002 - accuracy: 0.1633\n",
      "Epoch 11/200\n",
      " - 55s - loss: 5.0977 - accuracy: 0.1696\n",
      "Epoch 12/200\n",
      " - 63s - loss: 4.9967 - accuracy: 0.1729\n",
      "Epoch 13/200\n",
      " - 63s - loss: 4.8983 - accuracy: 0.1796\n",
      "Epoch 14/200\n",
      " - 64s - loss: 4.7930 - accuracy: 0.1850\n",
      "Epoch 15/200\n",
      " - 52s - loss: 4.6910 - accuracy: 0.1891\n",
      "Epoch 16/200\n",
      " - 52s - loss: 4.5817 - accuracy: 0.1985\n",
      "Epoch 17/200\n",
      " - 52s - loss: 4.4772 - accuracy: 0.2051\n",
      "Epoch 18/200\n",
      " - 52s - loss: 4.3670 - accuracy: 0.2101\n",
      "Epoch 19/200\n",
      " - 53s - loss: 4.2534 - accuracy: 0.2194\n",
      "Epoch 20/200\n",
      " - 52s - loss: 4.1426 - accuracy: 0.2304\n",
      "Epoch 21/200\n",
      " - 52s - loss: 4.0306 - accuracy: 0.2403\n",
      "Epoch 22/200\n",
      " - 55s - loss: 3.9169 - accuracy: 0.2495\n",
      "Epoch 23/200\n",
      " - 55s - loss: 3.8047 - accuracy: 0.2657\n",
      "Epoch 24/200\n",
      " - 54s - loss: 3.7010 - accuracy: 0.2735\n",
      "Epoch 25/200\n",
      " - 55s - loss: 3.5936 - accuracy: 0.2888\n",
      "Epoch 26/200\n",
      " - 54s - loss: 3.4706 - accuracy: 0.3025\n",
      "Epoch 27/200\n",
      " - 53s - loss: 3.3611 - accuracy: 0.3205\n",
      "Epoch 28/200\n",
      " - 53s - loss: 3.2425 - accuracy: 0.3392\n",
      "Epoch 29/200\n",
      " - 52s - loss: 3.1336 - accuracy: 0.3615\n",
      "Epoch 30/200\n",
      " - 52s - loss: 3.0197 - accuracy: 0.3805\n",
      "Epoch 31/200\n",
      " - 52s - loss: 2.9042 - accuracy: 0.4053\n",
      "Epoch 32/200\n",
      " - 52s - loss: 2.8159 - accuracy: 0.4281\n",
      "Epoch 33/200\n",
      " - 52s - loss: 2.7094 - accuracy: 0.4532\n",
      "Epoch 34/200\n",
      " - 52s - loss: 2.6104 - accuracy: 0.4722\n",
      "Epoch 35/200\n",
      " - 52s - loss: 2.4876 - accuracy: 0.5036\n",
      "Epoch 36/200\n",
      " - 52s - loss: 2.3941 - accuracy: 0.5218\n",
      "Epoch 37/200\n",
      " - 52s - loss: 2.2851 - accuracy: 0.5452\n",
      "Epoch 38/200\n",
      " - 52s - loss: 2.1940 - accuracy: 0.5676\n",
      "Epoch 39/200\n",
      " - 52s - loss: 2.1048 - accuracy: 0.5840\n",
      "Epoch 40/200\n",
      " - 52s - loss: 2.0290 - accuracy: 0.6040\n",
      "Epoch 41/200\n",
      " - 51s - loss: 1.9157 - accuracy: 0.6312\n",
      "Epoch 42/200\n",
      " - 52s - loss: 1.8341 - accuracy: 0.6459\n",
      "Epoch 43/200\n",
      " - 52s - loss: 1.7491 - accuracy: 0.6634\n",
      "Epoch 44/200\n",
      " - 52s - loss: 1.6532 - accuracy: 0.6874\n",
      "Epoch 45/200\n",
      " - 52s - loss: 1.5782 - accuracy: 0.7036\n",
      "Epoch 46/200\n",
      " - 52s - loss: 1.5126 - accuracy: 0.7159\n",
      "Epoch 47/200\n",
      " - 52s - loss: 1.4411 - accuracy: 0.7297\n",
      "Epoch 48/200\n",
      " - 52s - loss: 1.3760 - accuracy: 0.7475\n",
      "Epoch 49/200\n",
      " - 52s - loss: 1.3132 - accuracy: 0.7583\n",
      "Epoch 50/200\n",
      " - 52s - loss: 1.2294 - accuracy: 0.7798\n",
      "Epoch 51/200\n",
      " - 54s - loss: 1.1705 - accuracy: 0.7911\n",
      "Epoch 52/200\n",
      " - 52s - loss: 1.1221 - accuracy: 0.7991\n",
      "Epoch 53/200\n",
      " - 52s - loss: 1.0552 - accuracy: 0.8134\n",
      "Epoch 54/200\n",
      " - 53s - loss: 0.9841 - accuracy: 0.8300\n",
      "Epoch 55/200\n",
      " - 54s - loss: 0.9419 - accuracy: 0.8354\n",
      "Epoch 56/200\n",
      " - 53s - loss: 0.8982 - accuracy: 0.8492\n",
      "Epoch 57/200\n",
      " - 54s - loss: 0.8647 - accuracy: 0.8508\n",
      "Epoch 58/200\n",
      " - 55s - loss: 0.8049 - accuracy: 0.8657\n",
      "Epoch 59/200\n",
      " - 55s - loss: 0.7798 - accuracy: 0.8694\n",
      "Epoch 60/200\n",
      " - 55s - loss: 0.7356 - accuracy: 0.8794\n",
      "Epoch 61/200\n",
      " - 58s - loss: 0.6971 - accuracy: 0.8858\n",
      "Epoch 62/200\n",
      " - 68s - loss: 0.6672 - accuracy: 0.8908\n",
      "Epoch 63/200\n",
      " - 61s - loss: 0.6250 - accuracy: 0.9034\n",
      "Epoch 64/200\n",
      " - 54s - loss: 0.5616 - accuracy: 0.9195\n",
      "Epoch 65/200\n",
      " - 83s - loss: 0.5482 - accuracy: 0.9194\n",
      "Epoch 66/200\n",
      " - 78s - loss: 0.5258 - accuracy: 0.9247\n",
      "Epoch 67/200\n",
      " - 77s - loss: 0.4979 - accuracy: 0.9289\n",
      "Epoch 68/200\n",
      " - 75s - loss: 0.4715 - accuracy: 0.9339\n",
      "Epoch 69/200\n",
      " - 77s - loss: 0.4523 - accuracy: 0.9381\n",
      "Epoch 70/200\n",
      " - 79s - loss: 0.4237 - accuracy: 0.9410\n",
      "Epoch 71/200\n",
      " - 78s - loss: 0.3965 - accuracy: 0.9481\n",
      "Epoch 72/200\n",
      " - 79s - loss: 0.3632 - accuracy: 0.9563\n",
      "Epoch 73/200\n",
      " - 76s - loss: 0.3774 - accuracy: 0.9482\n",
      "Epoch 74/200\n",
      " - 77s - loss: 0.3712 - accuracy: 0.9484\n",
      "Epoch 75/200\n",
      " - 78s - loss: 0.3453 - accuracy: 0.9546\n",
      "Epoch 76/200\n",
      " - 76s - loss: 0.3215 - accuracy: 0.9599\n",
      "Epoch 77/200\n",
      " - 74s - loss: 0.3137 - accuracy: 0.9607\n",
      "Epoch 78/200\n",
      " - 74s - loss: 0.3084 - accuracy: 0.9602\n",
      "Epoch 79/200\n",
      " - 74s - loss: 0.2453 - accuracy: 0.9741\n",
      "Epoch 80/200\n",
      " - 75s - loss: 0.2238 - accuracy: 0.9785\n",
      "Epoch 81/200\n",
      " - 76s - loss: 0.2166 - accuracy: 0.9789\n",
      "Epoch 82/200\n",
      " - 75s - loss: 0.2679 - accuracy: 0.9647\n",
      "Epoch 83/200\n",
      " - 75s - loss: 0.2600 - accuracy: 0.9660\n",
      "Epoch 84/200\n",
      " - 76s - loss: 0.2119 - accuracy: 0.9761\n",
      "Epoch 85/200\n",
      " - 75s - loss: 0.1786 - accuracy: 0.9831\n",
      "Epoch 86/200\n",
      " - 79s - loss: 0.1604 - accuracy: 0.9870\n",
      "Epoch 87/200\n",
      " - 77s - loss: 0.1660 - accuracy: 0.9857\n",
      "Epoch 88/200\n",
      " - 74s - loss: 0.2007 - accuracy: 0.9744\n",
      "Epoch 89/200\n",
      " - 74s - loss: 0.2436 - accuracy: 0.9646\n",
      "Epoch 90/200\n",
      " - 74s - loss: 0.1717 - accuracy: 0.9814\n",
      "Epoch 91/200\n",
      " - 74s - loss: 0.1472 - accuracy: 0.9862\n",
      "Epoch 92/200\n",
      " - 74s - loss: 0.1171 - accuracy: 0.9914\n",
      "Epoch 93/200\n",
      " - 74s - loss: 0.1244 - accuracy: 0.9892\n",
      "Epoch 94/200\n",
      " - 74s - loss: 0.1936 - accuracy: 0.9734\n",
      "Epoch 95/200\n",
      " - 73s - loss: 0.1803 - accuracy: 0.9769\n",
      "Epoch 96/200\n",
      " - 74s - loss: 0.1490 - accuracy: 0.9827\n",
      "Epoch 97/200\n",
      " - 74s - loss: 0.0871 - accuracy: 0.9951\n",
      "Epoch 98/200\n",
      " - 73s - loss: 0.0613 - accuracy: 0.9982\n",
      "Epoch 99/200\n",
      " - 74s - loss: 0.0877 - accuracy: 0.9925\n",
      "Epoch 100/200\n",
      " - 73s - loss: 0.2279 - accuracy: 0.9600\n",
      "Epoch 101/200\n",
      " - 74s - loss: 0.1559 - accuracy: 0.9787\n",
      "Epoch 102/200\n",
      " - 74s - loss: 0.0991 - accuracy: 0.9913\n",
      "Epoch 103/200\n",
      " - 74s - loss: 0.0711 - accuracy: 0.9955\n",
      "Epoch 104/200\n",
      " - 75s - loss: 0.0642 - accuracy: 0.9968\n",
      "Epoch 105/200\n",
      " - 75s - loss: 0.0762 - accuracy: 0.9938\n",
      "Epoch 106/200\n",
      " - 74s - loss: 0.1361 - accuracy: 0.9816\n",
      "Epoch 107/200\n",
      " - 76s - loss: 0.1274 - accuracy: 0.9819\n",
      "Epoch 108/200\n",
      " - 74s - loss: 0.0652 - accuracy: 0.9960\n",
      "Epoch 00108: early stopping\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 200)               2725600   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3206)              644406    \n",
      "=================================================================\n",
      "Total params: 3,370,006\n",
      "Trainable params: 3,370,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      " - 153s - loss: 6.4146 - accuracy: 0.1126\n",
      "Epoch 2/200\n",
      " - 152s - loss: 6.0181 - accuracy: 0.1134\n",
      "Epoch 3/200\n",
      " - 157s - loss: 5.9067 - accuracy: 0.1153\n",
      "Epoch 4/200\n",
      " - 160s - loss: 5.6691 - accuracy: 0.1417\n",
      "Epoch 5/200\n",
      " - 154s - loss: 5.4555 - accuracy: 0.1545\n",
      "Epoch 6/200\n",
      " - 153s - loss: 5.3657 - accuracy: 0.1600\n",
      "Epoch 7/200\n",
      " - 155s - loss: 5.0678 - accuracy: 0.1724\n",
      "Epoch 8/200\n",
      " - 152s - loss: 4.8550 - accuracy: 0.1816\n",
      "Epoch 9/200\n",
      " - 153s - loss: 4.6719 - accuracy: 0.1942\n",
      "Epoch 10/200\n",
      " - 154s - loss: 4.4489 - accuracy: 0.2095\n",
      "Epoch 11/200\n",
      " - 153s - loss: 4.2238 - accuracy: 0.2240\n",
      "Epoch 12/200\n",
      " - 167s - loss: 3.9799 - accuracy: 0.2474\n",
      "Epoch 13/200\n",
      " - 173s - loss: 3.7539 - accuracy: 0.2729\n",
      "Epoch 14/200\n",
      " - 163s - loss: 3.4922 - accuracy: 0.3111\n",
      "Epoch 15/200\n",
      " - 170s - loss: 3.2205 - accuracy: 0.3546\n",
      "Epoch 16/200\n",
      " - 162s - loss: 2.9456 - accuracy: 0.4080\n",
      "Epoch 17/200\n",
      " - 170s - loss: 2.6628 - accuracy: 0.4712\n",
      "Epoch 18/200\n",
      " - 164s - loss: 2.4006 - accuracy: 0.5313\n",
      "Epoch 19/200\n",
      " - 169s - loss: 2.1405 - accuracy: 0.5867\n",
      "Epoch 20/200\n",
      " - 165s - loss: 1.8894 - accuracy: 0.6459\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/200\n",
      " - 171s - loss: 1.6564 - accuracy: 0.6960\n",
      "Epoch 22/200\n",
      " - 170s - loss: 1.4491 - accuracy: 0.7387\n",
      "Epoch 23/200\n",
      " - 160s - loss: 1.2643 - accuracy: 0.7769\n",
      "Epoch 24/200\n",
      " - 154s - loss: 1.0791 - accuracy: 0.8240\n",
      "Epoch 25/200\n",
      " - 122s - loss: 0.9344 - accuracy: 0.8497\n",
      "Epoch 26/200\n",
      " - 145s - loss: 0.9078 - accuracy: 0.8497\n",
      "Epoch 27/200\n",
      " - 154s - loss: 0.8204 - accuracy: 0.8718\n",
      "Epoch 28/200\n",
      " - 153s - loss: 0.5744 - accuracy: 0.9226\n",
      "Epoch 29/200\n",
      " - 154s - loss: 0.4642 - accuracy: 0.9465\n",
      "Epoch 30/200\n",
      " - 154s - loss: 0.3816 - accuracy: 0.9607\n",
      "Epoch 31/200\n",
      " - 153s - loss: 0.3314 - accuracy: 0.9696\n",
      "Epoch 32/200\n",
      " - 153s - loss: 0.2735 - accuracy: 0.9794\n",
      "Epoch 33/200\n",
      " - 153s - loss: 0.2272 - accuracy: 0.9855\n",
      "Epoch 34/200\n",
      " - 152s - loss: 0.1990 - accuracy: 0.9874\n",
      "Epoch 35/200\n",
      " - 152s - loss: 0.1577 - accuracy: 0.9912\n",
      "Epoch 36/200\n",
      " - 153s - loss: 0.1493 - accuracy: 0.9903\n",
      "Epoch 37/200\n",
      " - 153s - loss: 0.2149 - accuracy: 0.9753\n",
      "Epoch 38/200\n",
      " - 154s - loss: 0.1184 - accuracy: 0.9945\n",
      "Epoch 39/200\n",
      " - 153s - loss: 0.0793 - accuracy: 0.9974\n",
      "Epoch 40/200\n",
      " - 153s - loss: 0.0461 - accuracy: 0.9995\n",
      "Epoch 41/200\n",
      " - 152s - loss: 0.0354 - accuracy: 0.9995\n",
      "Epoch 42/200\n",
      " - 153s - loss: 0.0244 - accuracy: 0.9999\n",
      "Epoch 43/200\n",
      " - 152s - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      " - 152s - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      " - 153s - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      " - 154s - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      " - 152s - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      " - 153s - loss: 0.8163 - accuracy: 0.8076\n",
      "Epoch 49/200\n",
      " - 153s - loss: 0.1372 - accuracy: 0.9831\n",
      "Epoch 50/200\n",
      " - 153s - loss: 0.0421 - accuracy: 0.9991\n",
      "Epoch 51/200\n",
      " - 152s - loss: 0.0217 - accuracy: 0.9999\n",
      "Epoch 52/200\n",
      " - 153s - loss: 0.0145 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      " - 155s - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      " - 153s - loss: 0.0093 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      " - 153s - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      " - 154s - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 57/200\n",
      " - 153s - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      " - 153s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      " - 153s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      " - 153s - loss: 0.3512 - accuracy: 0.9098\n",
      "Epoch 61/200\n",
      " - 153s - loss: 0.2786 - accuracy: 0.9432\n",
      "Epoch 62/200\n",
      " - 153s - loss: 0.0528 - accuracy: 0.9973\n",
      "Epoch 63/200\n",
      " - 153s - loss: 0.0193 - accuracy: 0.9998\n",
      "Epoch 64/200\n",
      " - 153s - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      " - 152s - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      " - 153s - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      " - 153s - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      " - 153s - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      " - 153s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      " - 153s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      " - 153s - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      " - 153s - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      " - 152s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      " - 153s - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      " - 153s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      " - 154s - loss: 0.5342 - accuracy: 0.8690\n",
      "Epoch 77/200\n",
      " - 153s - loss: 0.0724 - accuracy: 0.9909\n",
      "Epoch 78/200\n",
      " - 154s - loss: 0.0280 - accuracy: 0.9981\n",
      "Epoch 79/200\n",
      " - 159s - loss: 0.0101 - accuracy: 0.9999\n",
      "Epoch 80/200\n",
      " - 153s - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      " - 155s - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      " - 153s - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      " - 153s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      " - 153s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      " - 154s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 00085: early stopping\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 300)               4208400   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 3206)              965006    \n",
      "=================================================================\n",
      "Total params: 5,173,406\n",
      "Trainable params: 5,173,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/200\n",
      " - 238s - loss: 6.3920 - accuracy: 0.1128\n",
      "Epoch 2/200\n",
      " - 240s - loss: 5.9831 - accuracy: 0.1138\n",
      "Epoch 3/200\n",
      " - 240s - loss: 5.7090 - accuracy: 0.1376\n",
      "Epoch 4/200\n",
      " - 239s - loss: 5.4519 - accuracy: 0.1574\n",
      "Epoch 5/200\n",
      " - 240s - loss: 5.3172 - accuracy: 0.1611\n",
      "Epoch 6/200\n",
      " - 240s - loss: 4.9591 - accuracy: 0.1779\n",
      "Epoch 7/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-baa022bd1911>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtest2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetTrainSeq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mtest2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetMapping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mtest2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0museWordEmbedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumEpoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileName\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"model_unit%d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive - California Institute of Technology\\class\\Machine Learning and Data Mining\\Project 3\\git\\CS155_PROJECT3\\LSTMforSonnet.py\u001b[0m in \u001b[0;36mTrain\u001b[1;34m(self, useWordEmbedding, embeddingSize, patience, numEpoch, fileName)\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_orig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumEpoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumEpoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m         \u001b[1;31m# save the model & mapping to file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "numUnit = [100, 200, 300, 400]\n",
    "for un in numUnit:\n",
    "    test2 = LSTM_word(LSTM_numUnits = un)\n",
    "    test2.SonnetLoader('shakespeare')\n",
    "    test2.getTrainSeq()\n",
    "    test2.getMapping()\n",
    "    test2.Train(useWordEmbedding = False, numEpoch = 200, fileName = \"model_unit%d\" % un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 100)           320600    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 300)               481200    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3206)              965006    \n",
      "=================================================================\n",
      "Total params: 1,766,806\n",
      "Trainable params: 1,766,806\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 54s - loss: 6.3453 - accuracy: 0.1128\n",
      "Epoch 2/200\n",
      " - 54s - loss: 5.7748 - accuracy: 0.1332\n",
      "Epoch 3/200\n",
      " - 54s - loss: 5.4916 - accuracy: 0.1491\n",
      "Epoch 4/200\n",
      " - 54s - loss: 5.2988 - accuracy: 0.1557\n",
      "Epoch 5/200\n",
      " - 49s - loss: 5.1222 - accuracy: 0.1617\n",
      "Epoch 6/200\n",
      " - 35s - loss: 4.9357 - accuracy: 0.1648\n",
      "Epoch 7/200\n",
      " - 36s - loss: 4.7174 - accuracy: 0.1729\n",
      "Epoch 8/200\n",
      " - 40s - loss: 4.4585 - accuracy: 0.1833\n",
      "Epoch 9/200\n",
      " - 35s - loss: 4.1603 - accuracy: 0.1995\n",
      "Epoch 10/200\n",
      " - 37s - loss: 3.8351 - accuracy: 0.2250\n",
      "Epoch 11/200\n",
      " - 36s - loss: 3.5004 - accuracy: 0.2741\n",
      "Epoch 12/200\n",
      " - 34s - loss: 3.1624 - accuracy: 0.3342\n",
      "Epoch 13/200\n",
      " - 35s - loss: 2.8407 - accuracy: 0.3986\n",
      "Epoch 14/200\n",
      " - 35s - loss: 2.5360 - accuracy: 0.4615\n",
      "Epoch 15/200\n",
      " - 34s - loss: 2.2508 - accuracy: 0.5221\n",
      "Epoch 16/200\n",
      " - 34s - loss: 1.9974 - accuracy: 0.5809\n",
      "Epoch 17/200\n",
      " - 35s - loss: 1.7558 - accuracy: 0.6374\n",
      "Epoch 18/200\n",
      " - 34s - loss: 1.5466 - accuracy: 0.6889\n",
      "Epoch 19/200\n",
      " - 34s - loss: 1.3521 - accuracy: 0.7341\n",
      "Epoch 20/200\n",
      " - 35s - loss: 1.1693 - accuracy: 0.7773\n",
      "Epoch 21/200\n",
      " - 35s - loss: 1.0059 - accuracy: 0.8174\n",
      "Epoch 22/200\n",
      " - 34s - loss: 0.8622 - accuracy: 0.8522\n",
      "Epoch 23/200\n",
      " - 34s - loss: 0.7278 - accuracy: 0.8840\n",
      "Epoch 24/200\n",
      " - 34s - loss: 0.6095 - accuracy: 0.9128\n",
      "Epoch 25/200\n",
      " - 33s - loss: 0.5019 - accuracy: 0.9383\n",
      "Epoch 26/200\n",
      " - 33s - loss: 0.4096 - accuracy: 0.9548\n",
      "Epoch 27/200\n",
      " - 35s - loss: 0.3313 - accuracy: 0.9687\n",
      "Epoch 28/200\n",
      " - 34s - loss: 0.2663 - accuracy: 0.9814\n",
      "Epoch 29/200\n",
      " - 35s - loss: 0.2066 - accuracy: 0.9898\n",
      "Epoch 30/200\n",
      " - 35s - loss: 0.1547 - accuracy: 0.9950\n",
      "Epoch 31/200\n",
      " - 35s - loss: 0.1161 - accuracy: 0.9986\n",
      "Epoch 32/200\n",
      " - 35s - loss: 0.0892 - accuracy: 0.9996\n",
      "Epoch 33/200\n",
      " - 35s - loss: 0.0708 - accuracy: 0.9992\n",
      "Epoch 34/200\n",
      " - 36s - loss: 0.0651 - accuracy: 0.9994\n",
      "Epoch 35/200\n",
      " - 35s - loss: 0.0504 - accuracy: 0.9996\n",
      "Epoch 36/200\n",
      " - 34s - loss: 0.0326 - accuracy: 0.9998\n",
      "Epoch 37/200\n",
      " - 35s - loss: 0.0361 - accuracy: 0.9995\n",
      "Epoch 38/200\n",
      " - 34s - loss: 0.0260 - accuracy: 0.9996\n",
      "Epoch 39/200\n",
      " - 34s - loss: 0.0897 - accuracy: 0.9859\n",
      "Epoch 40/200\n",
      " - 33s - loss: 0.2965 - accuracy: 0.9412\n",
      "Epoch 41/200\n",
      " - 34s - loss: 0.0456 - accuracy: 0.9985\n",
      "Epoch 42/200\n",
      " - 34s - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      " - 33s - loss: 0.0125 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      " - 34s - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      " - 35s - loss: 0.0086 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      " - 34s - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      " - 33s - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      " - 35s - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      " - 34s - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      " - 33s - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      " - 34s - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      " - 35s - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      " - 34s - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      " - 34s - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      " - 35s - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      " - 34s - loss: 0.5698 - accuracy: 0.8486\n",
      "Epoch 57/200\n",
      " - 34s - loss: 0.1637 - accuracy: 0.9654\n",
      "Epoch 58/200\n",
      " - 35s - loss: 0.0420 - accuracy: 0.9973\n",
      "Epoch 59/200\n",
      " - 36s - loss: 0.0173 - accuracy: 0.9999\n",
      "Epoch 60/200\n",
      " - 34s - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      " - 34s - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      " - 35s - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      " - 35s - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      " - 33s - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      " - 33s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 00065: early stopping\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 200)           641200    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 300)               601200    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3206)              965006    \n",
      "=================================================================\n",
      "Total params: 2,207,406\n",
      "Trainable params: 2,207,406\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 40s - loss: 6.3380 - accuracy: 0.1125\n",
      "Epoch 2/200\n",
      " - 40s - loss: 5.6855 - accuracy: 0.1411\n",
      "Epoch 3/200\n",
      " - 41s - loss: 5.4056 - accuracy: 0.1531\n",
      "Epoch 4/200\n",
      " - 41s - loss: 5.1701 - accuracy: 0.1626\n",
      "Epoch 5/200\n",
      " - 40s - loss: 4.9134 - accuracy: 0.1726\n",
      "Epoch 6/200\n",
      " - 41s - loss: 4.5986 - accuracy: 0.1808\n",
      "Epoch 7/200\n",
      " - 41s - loss: 4.2373 - accuracy: 0.1991\n",
      "Epoch 8/200\n",
      " - 40s - loss: 3.8382 - accuracy: 0.2291\n",
      "Epoch 9/200\n",
      " - 41s - loss: 3.4144 - accuracy: 0.2917\n",
      "Epoch 10/200\n",
      " - 41s - loss: 2.9883 - accuracy: 0.3737\n",
      "Epoch 11/200\n",
      " - 40s - loss: 2.5776 - accuracy: 0.4592\n",
      "Epoch 12/200\n",
      " - 41s - loss: 2.1979 - accuracy: 0.5411\n",
      "Epoch 13/200\n",
      " - 41s - loss: 1.8569 - accuracy: 0.6192\n",
      "Epoch 14/200\n",
      " - 40s - loss: 1.5538 - accuracy: 0.6908\n",
      "Epoch 15/200\n",
      " - 42s - loss: 1.2782 - accuracy: 0.7589\n",
      "Epoch 16/200\n",
      " - 41s - loss: 1.0412 - accuracy: 0.8167\n",
      "Epoch 17/200\n",
      " - 40s - loss: 0.8348 - accuracy: 0.8648\n",
      "Epoch 18/200\n",
      " - 42s - loss: 0.6617 - accuracy: 0.9036\n",
      "Epoch 19/200\n",
      " - 41s - loss: 0.5069 - accuracy: 0.9378\n",
      "Epoch 20/200\n",
      " - 41s - loss: 0.3839 - accuracy: 0.9619\n",
      "Epoch 21/200\n",
      " - 41s - loss: 0.2840 - accuracy: 0.9784\n",
      "Epoch 22/200\n",
      " - 41s - loss: 0.2051 - accuracy: 0.9903\n",
      "Epoch 23/200\n",
      " - 40s - loss: 0.1449 - accuracy: 0.9962\n",
      "Epoch 24/200\n",
      " - 42s - loss: 0.1011 - accuracy: 0.9991\n",
      "Epoch 25/200\n",
      " - 42s - loss: 0.0717 - accuracy: 0.9997\n",
      "Epoch 26/200\n",
      " - 41s - loss: 0.0565 - accuracy: 0.9999\n",
      "Epoch 27/200\n",
      " - 43s - loss: 0.0377 - accuracy: 1.0000\n",
      "Epoch 28/200\n",
      " - 42s - loss: 0.0277 - accuracy: 1.0000\n",
      "Epoch 29/200\n",
      " - 42s - loss: 0.0213 - accuracy: 1.0000\n",
      "Epoch 30/200\n",
      " - 41s - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 31/200\n",
      " - 40s - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 32/200\n",
      " - 40s - loss: 0.3584 - accuracy: 0.9135\n",
      "Epoch 33/200\n",
      " - 41s - loss: 0.1695 - accuracy: 0.9755\n",
      "Epoch 34/200\n",
      " - 41s - loss: 0.0305 - accuracy: 0.9995\n",
      "Epoch 35/200\n",
      " - 40s - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 36/200\n",
      " - 41s - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 37/200\n",
      " - 40s - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 38/200\n",
      " - 41s - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 39/200\n",
      " - 42s - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 40/200\n",
      " - 41s - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 41/200\n",
      " - 42s - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 42/200\n",
      " - 42s - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 43/200\n",
      " - 40s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 44/200\n",
      " - 41s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 45/200\n",
      " - 41s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 46/200\n",
      " - 40s - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 47/200\n",
      " - 41s - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 48/200\n",
      " - 41s - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 49/200\n",
      " - 40s - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 50/200\n",
      " - 41s - loss: 9.4363e-04 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      " - 41s - loss: 7.8407e-04 - accuracy: 1.0000\n",
      "Epoch 52/200\n",
      " - 40s - loss: 6.4611e-04 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      " - 41s - loss: 5.3229e-04 - accuracy: 1.0000\n",
      "Epoch 54/200\n",
      " - 41s - loss: 4.4253e-04 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      " - 43s - loss: 0.4933 - accuracy: 0.8689\n",
      "Epoch 56/200\n",
      " - 41s - loss: 0.1052 - accuracy: 0.9788\n",
      "Epoch 57/200\n",
      " - 40s - loss: 0.0155 - accuracy: 0.9997\n",
      "Epoch 58/200\n",
      " - 40s - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      " - 41s - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 60/200\n",
      " - 40s - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 61/200\n",
      " - 40s - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      " - 41s - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      " - 40s - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      " - 40s - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 00064: early stopping\n"
     ]
    }
   ],
   "source": [
    "embedSize = [100, 200]\n",
    "for dim in embedSize:\n",
    "    test2 = LSTM_word(LSTM_numUnits = 300)\n",
    "    test2.SonnetLoader('shakespeare')\n",
    "    test2.getTrainSeq()\n",
    "    test2.getMapping()\n",
    "    test2.Train(useWordEmbedding = True, embeddingSize = dim, numEpoch = 200, fileName = 'model_unit300') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 40, 5)             11105     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 400)               649600    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2221)              890621    \n",
      "=================================================================\n",
      "Total params: 1,551,326\n",
      "Trainable params: 1,551,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 39s - loss: 6.3463 - accuracy: 0.1130\n",
      "Epoch 2/200\n",
      " - 39s - loss: 5.9247 - accuracy: 0.1161\n",
      "Epoch 3/200\n",
      " - 39s - loss: 5.8025 - accuracy: 0.1164\n",
      "Epoch 4/200\n",
      " - 39s - loss: 5.7180 - accuracy: 0.1176\n",
      "Epoch 5/200\n",
      " - 38s - loss: 5.5231 - accuracy: 0.1308\n",
      "Epoch 6/200\n",
      " - 38s - loss: 5.3197 - accuracy: 0.1433\n",
      "Epoch 7/200\n",
      " - 38s - loss: 5.1401 - accuracy: 0.1504\n",
      "Epoch 8/200\n",
      " - 38s - loss: 4.9992 - accuracy: 0.1529\n",
      "Epoch 9/200\n",
      " - 37s - loss: 4.8019 - accuracy: 0.1587\n",
      "Epoch 10/200\n",
      " - 38s - loss: 4.6079 - accuracy: 0.1603\n",
      "Epoch 11/200\n",
      " - 38s - loss: 4.3974 - accuracy: 0.1655\n",
      "Epoch 12/200\n",
      " - 37s - loss: 4.1635 - accuracy: 0.1777\n",
      "Epoch 13/200\n",
      " - 38s - loss: 3.9162 - accuracy: 0.1942\n",
      "Epoch 14/200\n",
      " - 38s - loss: 3.6658 - accuracy: 0.2209\n",
      "Epoch 15/200\n",
      " - 38s - loss: 3.3980 - accuracy: 0.2582\n",
      "Epoch 16/200\n",
      " - 38s - loss: 3.1317 - accuracy: 0.3019\n",
      "Epoch 17/200\n",
      " - 38s - loss: 2.8660 - accuracy: 0.3664\n",
      "Epoch 18/200\n",
      " - 39s - loss: 2.5998 - accuracy: 0.4260\n",
      "Epoch 19/200\n",
      " - 40s - loss: 2.3554 - accuracy: 0.4867\n",
      "Epoch 20/200\n",
      " - 41s - loss: 2.1193 - accuracy: 0.5421\n",
      "Epoch 21/200\n",
      " - 42s - loss: 1.8960 - accuracy: 0.6059\n",
      "Epoch 22/200\n",
      " - 38s - loss: 1.6881 - accuracy: 0.6538\n",
      "Epoch 23/200\n",
      " - 38s - loss: 1.4989 - accuracy: 0.7002\n",
      "Epoch 24/200\n",
      " - 38s - loss: 1.3167 - accuracy: 0.7436\n",
      "Epoch 25/200\n",
      " - 37s - loss: 1.1488 - accuracy: 0.7877\n",
      "Epoch 26/200\n",
      " - 38s - loss: 0.9956 - accuracy: 0.8330\n",
      "Epoch 27/200\n",
      " - 37s - loss: 0.8594 - accuracy: 0.8621\n",
      "Epoch 28/200\n",
      " - 38s - loss: 0.7334 - accuracy: 0.8951\n",
      "Epoch 29/200\n",
      " - 37s - loss: 0.6191 - accuracy: 0.9235\n",
      "Epoch 30/200\n",
      " - 38s - loss: 0.5337 - accuracy: 0.9435\n",
      "Epoch 31/200\n",
      " - 37s - loss: 0.4299 - accuracy: 0.9615\n",
      "Epoch 32/200\n",
      " - 38s - loss: 0.3585 - accuracy: 0.9734\n",
      "Epoch 33/200\n",
      " - 39s - loss: 0.2896 - accuracy: 0.9821\n",
      "Epoch 34/200\n",
      " - 38s - loss: 0.2346 - accuracy: 0.9895\n",
      "Epoch 35/200\n",
      " - 38s - loss: 0.1904 - accuracy: 0.9932\n",
      "Epoch 36/200\n",
      " - 39s - loss: 0.1509 - accuracy: 0.9965\n",
      "Epoch 37/200\n",
      " - 39s - loss: 0.1207 - accuracy: 0.9984\n",
      "Epoch 38/200\n",
      " - 41s - loss: 0.0980 - accuracy: 0.9991\n",
      "Epoch 39/200\n",
      " - 38s - loss: 0.0765 - accuracy: 0.9996\n",
      "Epoch 40/200\n",
      " - 38s - loss: 0.0609 - accuracy: 0.9999\n",
      "Epoch 41/200\n",
      " - 38s - loss: 0.0492 - accuracy: 0.9999\n",
      "Epoch 42/200\n",
      " - 38s - loss: 0.0404 - accuracy: 0.9999\n",
      "Epoch 43/200\n",
      " - 39s - loss: 0.0346 - accuracy: 0.9997\n",
      "Epoch 44/200\n",
      " - 38s - loss: 0.0287 - accuracy: 0.9999\n",
      "Epoch 45/200\n",
      " - 38s - loss: 0.0241 - accuracy: 0.9999\n",
      "Epoch 46/200\n",
      " - 38s - loss: 0.0207 - accuracy: 0.9999\n",
      "Epoch 47/200\n",
      " - 38s - loss: 0.9110 - accuracy: 0.7641\n",
      "Epoch 48/200\n",
      " - 39s - loss: 0.4074 - accuracy: 0.9218\n",
      "Epoch 49/200\n",
      " - 39s - loss: 0.0857 - accuracy: 0.9961\n",
      "Epoch 50/200\n",
      " - 38s - loss: 0.0398 - accuracy: 0.9997\n",
      "Epoch 51/200\n",
      " - 39s - loss: 0.0287 - accuracy: 0.9997\n",
      "Epoch 52/200\n",
      " - 39s - loss: 0.0233 - accuracy: 0.9997\n",
      "Epoch 53/200\n",
      " - 40s - loss: 0.0198 - accuracy: 0.9999\n",
      "Epoch 54/200\n",
      " - 40s - loss: 0.0173 - accuracy: 0.9999\n",
      "Epoch 55/200\n",
      " - 39s - loss: 0.0152 - accuracy: 0.9997\n",
      "Epoch 56/200\n",
      " - 40s - loss: 0.0135 - accuracy: 0.9997\n",
      "Epoch 57/200\n",
      " - 42s - loss: 0.0120 - accuracy: 0.9997\n",
      "Epoch 58/200\n",
      " - 38s - loss: 0.0108 - accuracy: 0.9997\n",
      "Epoch 59/200\n",
      " - 39s - loss: 0.0097 - accuracy: 0.9997\n",
      "Epoch 60/200\n",
      " - 40s - loss: 0.0088 - accuracy: 0.9999\n",
      "Epoch 61/200\n",
      " - 39s - loss: 0.0078 - accuracy: 0.9999\n",
      "Epoch 62/200\n",
      " - 39s - loss: 0.0072 - accuracy: 0.9999\n",
      "Epoch 63/200\n",
      " - 39s - loss: 0.0065 - accuracy: 0.9997\n",
      "Epoch 64/200\n",
      " - 38s - loss: 0.0089 - accuracy: 0.9993\n",
      "Epoch 65/200\n",
      " - 39s - loss: 1.0862 - accuracy: 0.7086\n",
      "Epoch 66/200\n",
      " - 39s - loss: 0.2166 - accuracy: 0.9611\n",
      "Epoch 67/200\n",
      " - 40s - loss: 0.0469 - accuracy: 0.9993\n",
      "Epoch 68/200\n",
      " - 38s - loss: 0.0236 - accuracy: 0.9997\n",
      "Epoch 69/200\n",
      " - 38s - loss: 0.0176 - accuracy: 0.9999\n",
      "Epoch 70/200\n",
      " - 41s - loss: 0.0145 - accuracy: 0.9999\n",
      "Epoch 71/200\n",
      " - 40s - loss: 0.0123 - accuracy: 0.9999\n",
      "Epoch 72/200\n",
      " - 39s - loss: 0.0108 - accuracy: 0.9997\n",
      "Epoch 73/200\n",
      " - 39s - loss: 0.0096 - accuracy: 0.9997\n",
      "Epoch 00073: early stopping\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 40, 25)            55525     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 400)               681600    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2221)              890621    \n",
      "=================================================================\n",
      "Total params: 1,627,746\n",
      "Trainable params: 1,627,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      " - 41s - loss: 6.3160 - accuracy: 0.1156\n",
      "Epoch 2/200\n",
      " - 40s - loss: 5.8362 - accuracy: 0.1162\n",
      "Epoch 3/200\n",
      " - 39s - loss: 5.5930 - accuracy: 0.1359\n",
      "Epoch 4/200\n",
      " - 41s - loss: 5.3717 - accuracy: 0.1472\n",
      "Epoch 5/200\n",
      " - 50s - loss: 5.2191 - accuracy: 0.1504\n",
      "Epoch 6/200\n",
      " - 49s - loss: 5.0541 - accuracy: 0.1587\n",
      "Epoch 7/200\n",
      " - 50s - loss: 4.8858 - accuracy: 0.1609\n",
      "Epoch 8/200\n",
      " - 50s - loss: 4.6970 - accuracy: 0.1650\n",
      "Epoch 9/200\n",
      " - 50s - loss: 4.4859 - accuracy: 0.1683\n",
      "Epoch 10/200\n",
      " - 49s - loss: 4.2485 - accuracy: 0.1819\n",
      "Epoch 11/200\n",
      " - 49s - loss: 3.9733 - accuracy: 0.1955\n",
      "Epoch 12/200\n",
      " - 50s - loss: 3.6687 - accuracy: 0.2297\n",
      "Epoch 13/200\n",
      " - 46s - loss: 3.3510 - accuracy: 0.2795\n",
      "Epoch 14/200\n",
      " - 49s - loss: 3.0144 - accuracy: 0.3438\n",
      "Epoch 15/200\n",
      " - 50s - loss: 2.6621 - accuracy: 0.4263\n",
      "Epoch 16/200\n",
      " - 49s - loss: 2.3428 - accuracy: 0.4997\n",
      "Epoch 17/200\n",
      " - 49s - loss: 2.0281 - accuracy: 0.5744\n",
      "Epoch 18/200\n",
      " - 49s - loss: 1.7591 - accuracy: 0.6371\n",
      "Epoch 19/200\n",
      " - 49s - loss: 1.5016 - accuracy: 0.6995\n",
      "Epoch 20/200\n",
      " - 49s - loss: 1.2838 - accuracy: 0.7578\n",
      "Epoch 21/200\n",
      " - 50s - loss: 1.0811 - accuracy: 0.8033\n",
      "Epoch 22/200\n",
      " - 50s - loss: 0.8945 - accuracy: 0.8499\n",
      "Epoch 23/200\n",
      " - 49s - loss: 0.7331 - accuracy: 0.8893\n",
      "Epoch 24/200\n",
      " - 49s - loss: 0.5995 - accuracy: 0.9192\n",
      "Epoch 25/200\n",
      " - 49s - loss: 0.4736 - accuracy: 0.9498\n",
      "Epoch 26/200\n",
      " - 50s - loss: 0.3723 - accuracy: 0.9702\n",
      "Epoch 27/200\n",
      " - 49s - loss: 0.2851 - accuracy: 0.9851\n",
      "Epoch 28/200\n",
      " - 43s - loss: 0.2144 - accuracy: 0.9922\n",
      "Epoch 29/200\n",
      " - 50s - loss: 0.1699 - accuracy: 0.9964\n",
      "Epoch 30/200\n",
      " - 50s - loss: 0.1197 - accuracy: 0.9991\n",
      "Epoch 31/200\n",
      " - 49s - loss: 0.0887 - accuracy: 0.9997\n",
      "Epoch 32/200\n",
      " - 50s - loss: 0.0667 - accuracy: 0.9997\n",
      "Epoch 33/200\n",
      " - 49s - loss: 0.0529 - accuracy: 0.9997\n",
      "Epoch 34/200\n",
      " - 50s - loss: 0.0423 - accuracy: 0.9997\n",
      "Epoch 35/200\n",
      " - 49s - loss: 0.0345 - accuracy: 0.9997\n",
      "Epoch 36/200\n",
      " - 50s - loss: 0.0286 - accuracy: 0.9999\n",
      "Epoch 37/200\n",
      " - 50s - loss: 0.0241 - accuracy: 0.9999\n",
      "Epoch 38/200\n",
      " - 50s - loss: 0.0206 - accuracy: 0.9997\n",
      "Epoch 39/200\n",
      " - 50s - loss: 0.0175 - accuracy: 0.9999\n",
      "Epoch 40/200\n",
      " - 45s - loss: 0.0154 - accuracy: 0.9997\n",
      "Epoch 41/200\n",
      " - 49s - loss: 0.0630 - accuracy: 0.9910\n",
      "Epoch 42/200\n",
      " - 50s - loss: 0.9039 - accuracy: 0.7728\n",
      "Epoch 43/200\n",
      " - 50s - loss: 0.1519 - accuracy: 0.9855\n",
      "Epoch 44/200\n",
      " - 51s - loss: 0.0461 - accuracy: 0.9992\n",
      "Epoch 45/200\n",
      " - 50s - loss: 0.0247 - accuracy: 0.9999\n",
      "Epoch 46/200\n",
      " - 50s - loss: 0.0184 - accuracy: 0.9999\n",
      "Epoch 47/200\n",
      " - 50s - loss: 0.0152 - accuracy: 0.9997\n",
      "Epoch 48/200\n",
      " - 50s - loss: 0.0130 - accuracy: 0.9997\n",
      "Epoch 49/200\n",
      " - 50s - loss: 0.0114 - accuracy: 0.9997\n",
      "Epoch 50/200\n",
      " - 50s - loss: 0.0101 - accuracy: 0.9997\n",
      "Epoch 51/200\n",
      " - 50s - loss: 0.0088 - accuracy: 0.9999\n",
      "Epoch 52/200\n",
      " - 50s - loss: 0.0079 - accuracy: 0.9997\n",
      "Epoch 53/200\n",
      " - 50s - loss: 0.0070 - accuracy: 0.9999\n",
      "Epoch 54/200\n",
      " - 50s - loss: 0.0062 - accuracy: 0.9999\n",
      "Epoch 55/200\n",
      " - 50s - loss: 0.0056 - accuracy: 0.9999\n",
      "Epoch 56/200\n",
      " - 50s - loss: 0.0051 - accuracy: 0.9999\n",
      "Epoch 57/200\n",
      " - 50s - loss: 0.0046 - accuracy: 0.9997\n",
      "Epoch 58/200\n",
      " - 50s - loss: 0.0042 - accuracy: 0.9999\n",
      "Epoch 59/200\n",
      " - 50s - loss: 0.0038 - accuracy: 0.9999\n",
      "Epoch 60/200\n",
      " - 50s - loss: 0.0036 - accuracy: 0.9997\n",
      "Epoch 61/200\n",
      " - 50s - loss: 0.0031 - accuracy: 0.9999\n",
      "Epoch 62/200\n",
      " - 49s - loss: 0.0028 - accuracy: 0.9999\n",
      "Epoch 63/200\n",
      " - 51s - loss: 0.0026 - accuracy: 0.9999\n",
      "Epoch 64/200\n",
      " - 51s - loss: 0.0024 - accuracy: 0.9999\n",
      "Epoch 65/200\n",
      " - 50s - loss: 0.0022 - accuracy: 0.9999\n",
      "Epoch 66/200\n",
      " - 50s - loss: 0.0020 - accuracy: 0.9999\n",
      "Epoch 67/200\n",
      " - 50s - loss: 0.6226 - accuracy: 0.8340\n",
      "Epoch 68/200\n",
      " - 50s - loss: 0.4981 - accuracy: 0.8755\n",
      "Epoch 69/200\n",
      " - 49s - loss: 0.0709 - accuracy: 0.9922\n",
      "Epoch 70/200\n",
      " - 50s - loss: 0.0217 - accuracy: 0.9996\n",
      "Epoch 71/200\n",
      " - 50s - loss: 0.0127 - accuracy: 0.9999\n",
      "Epoch 72/200\n",
      " - 50s - loss: 0.0098 - accuracy: 0.9997\n",
      "Epoch 73/200\n",
      " - 50s - loss: 0.0082 - accuracy: 0.9999\n",
      "Epoch 74/200\n",
      " - 50s - loss: 0.0071 - accuracy: 0.9999\n",
      "Epoch 75/200\n",
      " - 50s - loss: 0.0064 - accuracy: 0.9997\n",
      "Epoch 76/200\n",
      " - 50s - loss: 0.0056 - accuracy: 0.9999\n",
      "Epoch 00076: early stopping\n"
     ]
    }
   ],
   "source": [
    "embedSize = [5, 25]\n",
    "for dim in embedSize:\n",
    "    test2 = LSTM_word(LSTM_numUnits = 400)\n",
    "    test2.SonnetLoader('Spenser_v2')\n",
    "    test2.getTrainSeq()\n",
    "    test2.getMapping()\n",
    "    test2.Train(useWordEmbedding = True, embeddingSize = dim, numEpoch = 200, fileName = 'model_Spenser_unit400') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature:  1.5 , embedding size:  5 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now love is thine as he takes from you \n",
      " then can i see what you with your true \n",
      " to love your self with keeps love's still \n",
      " sweet life so much sweet self should from hate \n",
      " that all i swear yet to death a alone \n",
      " but him for her and by well i lose thee \n",
      " those long i'll child for my love not \n",
      " though i in love loves gracious a common kind \n",
      " be as thy proud thy self thy constancy \n",
      " to live thy self thy self i will \n",
      " make thee a heart even when thy graces \n",
      " the perfect of my will \n",
      " a most time jewel out out \n",
      " now in thee \n",
      "\n",
      "temperature:  1 , embedding size:  5 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now love is thine as he takes from you \n",
      " then can i see what you with your true \n",
      " to love your self with keeps love's still \n",
      " sweet life though this nature use doth grow \n",
      " and yet to be it being with be \n",
      " that you thy love even keeps to give \n",
      " that is to outlive long date \n",
      " when to have eyes of upon \n",
      " or time doth have a satire of praise \n",
      " and make make time's spoils despised everywhere \n",
      " give my love fame faster than time wastes life \n",
      " so thou prevent'st black save and my part \n",
      " \n",
      " that i have though not must be hide \n",
      " like tender old to be i \n",
      "\n",
      "temperature:  0.75 , embedding size:  5 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now his beauty shall still you for so \n",
      " and all my best is to this all one \n",
      " and i an see them with my pen \n",
      " therefore all my love and from my life hath end \n",
      " i find sweet love though my love is not \n",
      " so thou no mother's that i am both \n",
      " as high to lose my purpose disgrace \n",
      " till my heart knows all present eye's eye's \n",
      " and for my love for thee is so fair \n",
      " that i am not with present nor to be \n",
      " and he that might i not my love be \n",
      " as if thou not becoming this nor be \n",
      " not thee i not yet i am not \n",
      "\n",
      "temperature:  0.25 , embedding size:  5 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now his beauty shall still you for so \n",
      " and all my best is to this all one \n",
      " and i an accessary needs must be \n",
      " that i will not to be with thee \n",
      " of him thy self that i do thee more \n",
      " then worthy i love my self thou dost be \n",
      " the ten of thine for happier be i whilst \n",
      " thou gav'st the mother's part i am love \n",
      " as those gold candles fixed in thee air \n",
      " lose all of more that i may see \n",
      " and this true love is love still thee and so so \n",
      " for love that love i will in thee \n",
      " by mine eyes thy fair truth wouldst to \n",
      "\n",
      "temperature:  1.5 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now love is thine as he takes from you \n",
      " then can i see what you with your true \n",
      " to love your self with keeps love's still \n",
      " sweet life so much sweet self should from hate \n",
      " that all i swear yet to death a alone \n",
      " but him for her and by well i lose thee \n",
      " those long i'll child for my love not \n",
      " though i in love loves gracious a common kind \n",
      " be as thy proud thy self thy constancy \n",
      " to live thy self thy self i will \n",
      " make thee a heart even when thy graces \n",
      " the perfect of my will \n",
      " a most time jewel out out \n",
      " now in thee \n",
      "\n",
      "temperature:  1 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now love is thine as he takes from you \n",
      " then can i see what you with your true \n",
      " to love your self with keeps love's still \n",
      " sweet life though this nature use doth grow \n",
      " and yet to be it being with be \n",
      " that you thy love even keeps to give \n",
      " that is to outlive long date \n",
      " when to have eyes of upon \n",
      " or time doth have a satire of praise \n",
      " and make make time's spoils despised everywhere \n",
      " give my love fame faster than time wastes life \n",
      " so thou prevent'st black save and my part \n",
      " \n",
      " that i have though not must be hide \n",
      " like tender old to be i \n",
      "\n",
      "temperature:  0.75 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now his beauty shall still you for so \n",
      " and all my best is to this all one \n",
      " and i an see them with my pen \n",
      " therefore all my love and from my life hath end \n",
      " i find sweet love though my love is not \n",
      " so thou no mother's that i am both \n",
      " as high to lose my purpose disgrace \n",
      " till my heart knows all present eye's eye's \n",
      " and for my love for thee is so fair \n",
      " that i am not with present nor to be \n",
      " and he that might i not my love be \n",
      " as if thou not becoming this nor be \n",
      " not thee i not yet i am not \n",
      "\n",
      "temperature:  0.25 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now his beauty shall still you for so \n",
      " and all my best is to this all one \n",
      " and i an accessary needs must be \n",
      " that i will not to be with thee \n",
      " of him thy self that i do thee more \n",
      " then worthy i love my self thou dost be \n",
      " the ten of thine for happier be i whilst \n",
      " thou gav'st the mother's part i am love \n",
      " as those gold candles fixed in thee air \n",
      " lose all of more that i may see \n",
      " and this true love is love still thee and so so \n",
      " for love that love i will in thee \n",
      " by mine eyes thy fair truth wouldst to \n",
      "\n",
      "temperature:  1.5 , embedding size:  25 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now love is thine as he takes from you \n",
      " then can i see what you with your true \n",
      " to love your self with keeps love's still \n",
      " sweet life so much sweet self should from hate \n",
      " that all i swear yet to death a alone \n",
      " but him for her and by well i lose thee \n",
      " those long i'll child for my love not \n",
      " though i in love loves gracious a common kind \n",
      " be as thy proud thy self thy constancy \n",
      " to live thy self thy self i will \n",
      " make thee a heart even when thy graces \n",
      " the perfect of my will \n",
      " a most time jewel out out \n",
      " now in thee \n",
      "\n",
      "temperature:  1 , embedding size:  25 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now love is thine as he takes from you \n",
      " then can i see what you with your true \n",
      " to love your self with keeps love's still \n",
      " sweet life though this nature use doth grow \n",
      " and yet to be it being with be \n",
      " that you thy love even keeps to give \n",
      " that is to outlive long date \n",
      " when to have eyes of upon \n",
      " or time doth have a satire of praise \n",
      " and make make time's spoils despised everywhere \n",
      " give my love fame faster than time wastes life \n",
      " so thou prevent'st black save and my part \n",
      " \n",
      " that i have though not must be hide \n",
      " like tender old to be i \n",
      "\n",
      "temperature:  0.75 , embedding size:  25 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now his beauty shall still you for so \n",
      " and all my best is to this all one \n",
      " and i an see them with my pen \n",
      " therefore all my love and from my life hath end \n",
      " i find sweet love though my love is not \n",
      " so thou no mother's that i am both \n",
      " as high to lose my purpose disgrace \n",
      " till my heart knows all present eye's eye's \n",
      " and for my love for thee is so fair \n",
      " that i am not with present nor to be \n",
      " and he that might i not my love be \n",
      " as if thou not becoming this nor be \n",
      " not thee i not yet i am not \n",
      "\n",
      "temperature:  0.25 , embedding size:  25 \n",
      " shall i compare thee to a summer's day \n",
      " that i see on thee which beauty bring \n",
      " and in our faults by lies we flattered be \n",
      " and make their proud lives of his time \n",
      " now his beauty shall still you for so \n",
      " and all my best is to this all one \n",
      " and i an accessary needs must be \n",
      " that i will not to be with thee \n",
      " of him thy self that i do thee more \n",
      " then worthy i love my self thou dost be \n",
      " the ten of thine for happier be i whilst \n",
      " thou gav'st the mother's part i am love \n",
      " as those gold candles fixed in thee air \n",
      " lose all of more that i may see \n",
      " and this true love is love still thee and so so \n",
      " for love that love i will in thee \n",
      " by mine eyes thy fair truth wouldst to \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with checking pentmeter\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "embedSize = [5, 10, 25]\n",
    "for dim in embedSize:\n",
    "    modelName = \"model_Spenser_withWordEmbedding %d.h5\" % dim\n",
    "    mappingName = \"model_Spenser_mapping_withWordEmbedding %d.pk1\" % dim\n",
    "    test2 = LSTM_word()\n",
    "    test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "    test2.SonnetLoader('shakespeare')\n",
    "    tempList = [1.5, 1, 0.75, 0.25]\n",
    "    predicted = [test2.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=150, temperature = x, checkPentameter = False) for x in tempList]\n",
    "    for i, x in enumerate(predicted):\n",
    "        print('temperature: ', tempList[i], ', embedding size: ', dim, '\\n', x, '\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
