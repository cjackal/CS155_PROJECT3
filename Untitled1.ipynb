{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88840\n",
      "2221\n",
      "16719688\n"
     ]
    }
   ],
   "source": [
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "embedSize = [10]\n",
    "for dim in embedSize:\n",
    "    test2 = LSTM_word()\n",
    "    test2.SonnetLoader('Spenser_v2')\n",
    "    test2.getTrainSeq()\n",
    "    test2.getMapping()\n",
    "#    test2.Train(useWordEmbedding = True, embeddingSize = dim, numEpoch = 200, fileName = 'model_Spenser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 40, 5)             16030     \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 400)               649600    \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 1,951,236\n",
      "Trainable params: 1,951,236\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 37s 3ms/step\n",
      "0.006680879098548909\n",
      "1.0\n",
      "1.006703245953655\n",
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 40, 10)            32060     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 400)               657600    \n",
      "_________________________________________________________________\n",
      "lambda_8 (Lambda)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 1,975,266\n",
      "Trainable params: 1,975,266\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 37s 3ms/step\n",
      "4.1456098875768\n",
      "0.19901303946971893\n",
      "63.156128296312325\n",
      "0.19901303946971893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_16 (Embedding)     (None, 40, 25)            80150     \n",
      "_________________________________________________________________\n",
      "lstm_16 (LSTM)               (None, 400)               681600    \n",
      "_________________________________________________________________\n",
      "lambda_9 (Lambda)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 3206)              1285606   \n",
      "=================================================================\n",
      "Total params: 2,047,356\n",
      "Trainable params: 2,047,356\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13577/13577 [==============================] - 38s 3ms/step\n",
      "0.005212324941954059\n",
      "1.0\n",
      "1.0052259327400865\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "embedSize = [5, 10, 25]\n",
    "for dim in embedSize:\n",
    "#     modelName = \"model_Spenser_withWordEmbedding %d.h5\" % dim\n",
    "#     mappingName = \"model_Spenser_mapping_withWordEmbedding %d.pk1\" % dim\n",
    "    modelName = \"model_unit400_withWordEmbedding %d.h5\" % dim\n",
    "    mappingName = \"model_unit400_mapping_withWordEmbedding %d.pk1\" % dim\n",
    "    test2 = LSTM_word()\n",
    "    #test2.SonnetLoader('Spenser_v2')\n",
    "    test2.SonnetLoader('shakespeare')\n",
    "    test2.getTrainSeq()\n",
    "    test2.getMapping()\n",
    "    test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "    [perplexity, accuracy] = test2.perplexity_train(useWordEmbedding=True)\n",
    "    print(perplexity)\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LSTM_word' object has no attribute 'Y'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-3156da126218>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperplexity_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0museWordEmbedding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive - California Institute of Technology\\class\\Machine Learning and Data Mining\\Project 3\\git\\CS155_PROJECT3\\LSTMforSonnet.py\u001b[0m in \u001b[0;36mperplexity_train\u001b[1;34m(self, useWordEmbedding)\u001b[0m\n\u001b[0;32m    274\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No trained model found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    275\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    277\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'No intput/output data found'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LSTM_word' object has no attribute 'Y'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to C:\\Users\\HyeongChan\n",
      "[nltk_data]     Jo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 5)             11105     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               164800    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 622,326\n",
      "Trainable params: 622,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 5)             11105     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               164800    \n",
      "_________________________________________________________________\n",
      "lambda_2 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 622,326\n",
      "Trainable params: 622,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 5)             11105     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               164800    \n",
      "_________________________________________________________________\n",
      "lambda_3 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 622,326\n",
      "Trainable params: 622,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 5)             11105     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               164800    \n",
      "_________________________________________________________________\n",
      "lambda_4 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 622,326\n",
      "Trainable params: 622,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "temperature:  1.5 , embedding size:  5 \n",
      " shall i compare thee to a summer's day \n",
      " to you your power with silence wit \n",
      " yet every sweet thoughts in me you \n",
      " that they my minute seem did self be \n",
      " than dare her lov'd by men of mean degree \n",
      " so unto the but my leaf i love \n",
      " for all the woes of they still light \n",
      " no skill can stint nor reason can aslake \n",
      " but when in hand my tuneless harp i take \n",
      " then do i more augment my foes' despight \n",
      " and grief renew and passions do awake \n",
      " to battle fresh against myself to fight \n",
      " 'mongst whom the more i seek to settle peace \n",
      " the more i find their malice to increase \n",
      " so for my body ye their meeds \n",
      " so they therewith heart poets' love's honor \n",
      " to sorrow forth doth anguish of his heavenly part \n",
      " and with one with greater entice \n",
      " exceeding \n",
      "\n",
      "temperature:  1 , embedding size:  5 \n",
      " shall i compare thee to a summer's day \n",
      " to you your power with silence wit \n",
      " let her your constant stiffness doth constrain \n",
      " only behold her love as cruelty doth \n",
      " \n",
      " ye sweet as both god and did be \n",
      " but ye the stars for they have purer will \n",
      " and harder grows the harder she is smit \n",
      " with all the plaints of to fair light \n",
      " and from delight the loves with mote sight \n",
      " yet in thy meed and mischief thy reward \n",
      " due to thyself that it to her decay \n",
      " and eek her my borrowed love forth \n",
      " and if in presence or my prince's peer \n",
      " for now your light doth more itself dilate \n",
      " and in my darkness greater doth appear \n",
      " yet since your light hath once enlumined me \n",
      " with my reflex yours shall increased be \n",
      " but seek the world with her eyes \n",
      " drawn with \n",
      "\n",
      "temperature:  0.75 , embedding size:  5 \n",
      " shall i compare thee to a summer's day \n",
      " to you your power with silence wit \n",
      " let her your constant stiffness doth constrain \n",
      " only behold her love as cruelty doth \n",
      " \n",
      " ye sweet as both god and did be \n",
      " but ye the stars for they have purer will \n",
      " and harder grows the harder she is smit \n",
      " with all the plaints of to fair light \n",
      " and from delight the loves with mote sight \n",
      " yet in thy meed and mischief thy reward \n",
      " due to thyself that it to her decay \n",
      " and eek her my borrowed love forth \n",
      " and if in presence or my prince's peer \n",
      " for now your light doth more itself dilate \n",
      " and in my darkness greater doth appear \n",
      " yet since your light hath once enlumined me \n",
      " with my reflex yours shall increased be \n",
      " but seek the world with her me \n",
      " drawn with \n",
      "\n",
      "temperature:  0.25 , embedding size:  5 \n",
      " shall i compare thee to a summer's day \n",
      " to you your power with silence wit \n",
      " let her your constant stiffness doth constrain \n",
      " only behold her love as cruelty doth \n",
      " \n",
      " ye sweet as both god and did be \n",
      " but ye the stars for they have purer will \n",
      " and harder grows the harder she is smit \n",
      " with all the plaints of to fair light \n",
      " and from delight the loves which mote pervert \n",
      " his safe assurance strongly it restrain \n",
      " only let in abstain with cruelty \n",
      " the looser you that stir up lusts impure \n",
      " with such strange terms her eyes she doth inure \n",
      " that with one look she can not in make \n",
      " to spill your power which i too well have tried \n",
      " but yet if in your hardened breast ye hide \n",
      " a close intent at last to show me grace \n",
      " then all the woes and \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 10)            22210     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               168800    \n",
      "_________________________________________________________________\n",
      "lambda_5 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 637,431\n",
      "Trainable params: 637,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 10)            22210     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               168800    \n",
      "_________________________________________________________________\n",
      "lambda_6 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 637,431\n",
      "Trainable params: 637,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 10)            22210     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               168800    \n",
      "_________________________________________________________________\n",
      "lambda_7 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 637,431\n",
      "Trainable params: 637,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 40, 10)            22210     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 200)               168800    \n",
      "_________________________________________________________________\n",
      "lambda_8 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 637,431\n",
      "Trainable params: 637,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "temperature:  1.5 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " and three with not so hard may surcease \n",
      " but when more laugh she mocks and when i cry \n",
      " she laughs and hardens evermore her heart \n",
      " what then can move her if not mirth nor moan \n",
      " she is no woman but senseless stone \n",
      " that fairest true beauty that are to sheen \n",
      " but when ye lour or look on me askew \n",
      " then do i die as one with lightning fired \n",
      " but since that life is more i death desired \n",
      " so bend my force against which my mind \n",
      " in which i may like in me all got \n",
      " but simple truth and mutual good will \n",
      " seeks with sweet peace to salve each others' wound \n",
      " there faith doth fearless dwell in brazen tower \n",
      " and spotless pleasure builds her sacred bower \n",
      " and every rash beholder passing by \n",
      " one of those \n",
      "\n",
      "temperature:  1 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " and three with not that my life \n",
      " such life doth lighten all doth are \n",
      " \n",
      " then to the work is woven all above \n",
      " with woodbine flowers and fragrant eglantine \n",
      " so sweet your prison you in time shall prove \n",
      " with many dear delights bedecked fine \n",
      " and all thenceforth eternal peace shall see \n",
      " between the spider and the gentle bee \n",
      " is her whilst which her will \n",
      " but pain i me gain to me a rest \n",
      " being with the dear blood clean washed from sin \n",
      " may live for ever in felicity \n",
      " and that thy love we weighing worthily \n",
      " may likewise love thee for the same again \n",
      " and for thy sake that all like dear didst buy \n",
      " with love may one another entertain \n",
      " so let us love like as we ought \n",
      " love is the lesson which the \n",
      "\n",
      "temperature:  0.75 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " and three with not that my life \n",
      " such life doth lighten all doth are \n",
      " \n",
      " then to the work is woven all above \n",
      " with woodbine flowers and fragrant eglantine \n",
      " so sweet your prison you in time shall prove \n",
      " with many dear delights bedecked fine \n",
      " and all thenceforth eternal peace shall see \n",
      " between the spider and the gentle bee \n",
      " is her whilst which her will \n",
      " but pain i me gain to me a rest \n",
      " being with the dear blood clean washed from sin \n",
      " may live for ever in felicity \n",
      " and that thy love we weighing worthily \n",
      " may likewise love thee for the same again \n",
      " and for thy sake that all like dear didst buy \n",
      " with love may one another entertain \n",
      " so let us love like as we ought \n",
      " love is the lesson which the \n",
      "\n",
      "temperature:  0.25 , embedding size:  10 \n",
      " shall i compare thee to a summer's day \n",
      " and three with not that my life \n",
      " such life doth lighten all doth are \n",
      " \n",
      " all to the work is woven all above \n",
      " with woodbine flowers and fragrant eglantine \n",
      " so sweet your prison you in time shall prove \n",
      " with many dear delights bedecked fine \n",
      " and all thenceforth eternal peace shall see \n",
      " between the spider and the gentle bee \n",
      " is her whilst which her will \n",
      " but pain i me gain to me a whit \n",
      " being all her words so wise i to sad plights \n",
      " which her deep wit which envy or man or ill \n",
      " so ship that tree and he that would attend \n",
      " mote soften it and to his will allure \n",
      " so do i hope her stubborn heart to bend \n",
      " and that it then more steadfast will endure \n",
      " only my pains will be the more \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 25)            55525     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               180800    \n",
      "_________________________________________________________________\n",
      "lambda_9 (Lambda)            (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 682,746\n",
      "Trainable params: 682,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 25)            55525     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               180800    \n",
      "_________________________________________________________________\n",
      "lambda_10 (Lambda)           (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 682,746\n",
      "Trainable params: 682,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 25)            55525     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               180800    \n",
      "_________________________________________________________________\n",
      "lambda_11 (Lambda)           (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 682,746\n",
      "Trainable params: 682,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 40, 25)            55525     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 200)               180800    \n",
      "_________________________________________________________________\n",
      "lambda_12 (Lambda)           (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2221)              446421    \n",
      "=================================================================\n",
      "Total params: 682,746\n",
      "Trainable params: 682,746\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "temperature:  1.5 , embedding size:  25 \n",
      " shall i compare thee to a summer's day \n",
      " it the woes in her eyes i their meeds \n",
      " and since forth all doth frail late i take \n",
      " but she her last fall may in me \n",
      " and dying one may make of my high \n",
      " that him her all in my life in get \n",
      " for all as out her stubborn wit \n",
      " then think not not in in in never \n",
      " to sport your muse and deformed the virtues part \n",
      " but did another peace in blessed saints upbrought \n",
      " which those archers fair and not are long \n",
      " and every beast with make me ever \n",
      " to lofty desire shall lofty you \n",
      " and of of me by you light do \n",
      " to prove that dust to kindle love shall will \n",
      " but him that life that that the life \n",
      " perhaps with the cause with silence like she \n",
      " she she their faults with \n",
      "\n",
      "temperature:  1 , embedding size:  25 \n",
      " shall i compare thee to a summer's day \n",
      " it the woes in her eyes i their meeds \n",
      " and since forth all doth frail late i take \n",
      " but she her last fall may in me \n",
      " and dying one word was can in to me to \n",
      " to be were pitty but to save were praise \n",
      " but when the closet of my parts entire \n",
      " i when how out your torment of me \n",
      " and is that not from her live to ever \n",
      " not all me not but those and her feet \n",
      " and to meek humbless and afflicted mood \n",
      " pardon for thee and and me more spill \n",
      " but which no weep but wise are from the brook \n",
      " and when i ever like doth argue be \n",
      " but ye high heavens like ye me doth see \n",
      " fed with the time of your life cannot get \n",
      " through all the faults which \n",
      "\n",
      "temperature:  0.75 , embedding size:  25 \n",
      " shall i compare thee to a summer's day \n",
      " it the woes in her eyes i their meeds \n",
      " and since forth all doth frail late i take \n",
      " but she her last fall may in me \n",
      " and dying one word was can in to me to \n",
      " to be were pitty but to save were praise \n",
      " but when the closet of my parts entire \n",
      " i when how out your torment of me \n",
      " and is that not from her live to ever \n",
      " not all me not but those and her feet \n",
      " and to meek humbless and afflicted mood \n",
      " pardon \n",
      " ever for and man for all \n",
      " and do end not a bosom beams be \n",
      " and having i hope to stubborn fair i \n",
      " to turn to cruel and my might \n",
      " and that of have dearest happy needs \n",
      " and oft her thrall for my ill \n",
      " the raging waves \n",
      "\n",
      "temperature:  0.25 , embedding size:  25 \n",
      " shall i compare thee to a summer's day \n",
      " it the woes in her eyes i their meeds \n",
      " and since forth all doth frail late i take \n",
      " but she her last fall may in me \n",
      " and dying one word was can in to me to \n",
      " to be were pitty but to save were praise \n",
      " but when the closet of my parts entire \n",
      " i when how out your torment of me \n",
      " and is that not from her live to ever \n",
      " not all me not but those and her feet \n",
      " and to meek humbless and afflicted mood \n",
      " pardon \n",
      " ever for and man for all \n",
      " and do end not a bosom beams be \n",
      " and having i hope to stubborn fair i \n",
      " to turn to cruel and my might \n",
      " and that of have dearest happy needs \n",
      " and oft her thrall for my ill \n",
      " the raging waves \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with checking pentameter\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "embedSize = [5, 10, 25]\n",
    "for dim in embedSize:\n",
    "    modelName = \"model_Spenser_withWordEmbedding %d.h5\" % dim\n",
    "    mappingName = \"model_Spenser_mapping_withWordEmbedding %d.pk1\" % dim\n",
    "    test2 = LSTM_word()\n",
    "    test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "    test2.SonnetLoader('Spenser_v2')\n",
    "    tempList = [1.5, 1, 0.75, 0.25]\n",
    "    predicted = [test2.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=150, temperature = x, checkPentameter = False, useWordEmbedding = True) for x in tempList]\n",
    "    for i, x in enumerate(predicted):\n",
    "        print('temperature: ', tempList[i], ', embedding size: ', dim, '\\n', x, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 40, 200)           874600    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "lambda_170 (Lambda)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "lambda_171 (Lambda)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4373)              1753573   \n",
      "=================================================================\n",
      "Total params: 3,589,773\n",
      "Trainable params: 3,589,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Completed sentence:  ['when', 'others', 'gaze', 'upon', 'their', 'shadows', 'vain', '\\n']\n",
      "Completed sentence:  ['then', 'him', 'he', 'when', 'from', 'me', 'doth', 'grow', 'away', '\\n']\n",
      "Completed sentence:  ['so', 'long', 'i', 'love', 'was', 'well', 'and', 'love', 'love', 'now', '\\n']\n",
      "Completed sentence:  ['and', 'so', 'profound', 'abysm', 'i', 'throw', 'all', 'care', '\\n']\n",
      "Completed sentence:  ['of', \"others'\", 'voices', 'that', 'my', \"adder's\", 'sense', '\\n']\n",
      "Completed sentence:  ['to', 'critic', 'and', 'to', 'flatterer', 'stopped', 'are', '\\n']\n",
      "Completed sentence:  ['mark', 'how', 'with', 'my', 'neglect', 'i', 'do', 'dispense', '\\n']\n",
      "Completed sentence:  ['you', 'are', 'so', 'strongly', 'in', 'my', 'purpose', 'bred', '\\n']\n",
      "Completed sentence:  ['that', 'all', 'the', 'world', 'besides', 'methinks', 'are', 'dead', '\\n']\n",
      "Completed sentence:  ['for', 'his', 'triumphant', 'prize', 'proud', 'of', 'this', 'pride', '\\n']\n",
      "Completed sentence:  ['he', 'is', 'contented', 'thy', 'poor', 'drudge', 'to', 'be', '\\n']\n",
      "Completed sentence:  ['to', 'stand', 'in', 'thy', 'affairs', 'fall', 'by', 'thy', 'side', '\\n']\n",
      "Completed sentence:  ['no', 'want', 'of', 'conscience', 'hold', 'it', 'that', 'i', 'call', '\\n']\n",
      "Completed sentence:  ['her', 'love', 'for', 'whose', 'dear', 'love', 'i', 'rise', 'and', 'fall', '\\n']\n",
      "Completed sentence:  ['to', 'make', 'him', 'seem', 'long', 'hence', 'as', 'he', 'shows', 'now', '\\n']\n",
      "Completed sentence:  ['and', 'nothing', 'mine', 'thee', 'how', 'mine', 'own', 'desert', '\\n']\n",
      "Model: \"sequential_180\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 40, 200)           874600    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "lambda_332 (Lambda)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "lambda_333 (Lambda)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4373)              1753573   \n",
      "=================================================================\n",
      "Total params: 3,589,773\n",
      "Trainable params: 3,589,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Completed sentence:  ['when', 'in', 'dead', 'night', 'thy', 'fair', 'imperfect', 'shade', '\\n']\n",
      "Completed sentence:  ['through', 'heavy', 'sleep', 'on', 'sightless', 'eyes', 'doth', 'stay', '\\n']\n",
      "Completed sentence:  ['all', 'days', 'are', 'nights', 'to', 'see', 'till', 'i', 'see', 'thee', '\\n']\n",
      "Completed sentence:  ['and', 'nights', 'bright', 'days', 'when', 'dreams', 'do', 'show', 'thee', 'me', '\\n']\n",
      "Completed sentence:  ['when', 'so', 'hold', 'me', 'when', 'i', 'have', 'looked', 'on', 'me', '\\n']\n",
      "Completed sentence:  ['and', 'when', 'i', 'am', 'and', 'water', 'do', 'not', 'be', '\\n']\n",
      "Completed sentence:  ['and', 'to', 'the', 'light', 'lift', 'up', 'their', 'drooping', 'head', '\\n']\n",
      "Completed sentence:  ['so', 'when', 'her', 'me', 'he', 'wise', 'when', 'from', 'me', 'me', '\\n']\n",
      "Completed sentence:  ['and', 'wish', 'that', 'well', 'and', 'well', 'and', 'am', 'well', '\\n']\n",
      "Completed sentence:  ['as', 'i', 'am', 'now', 'what', 'is', 'not', 'be', 'free', '\\n']\n",
      "Completed sentence:  ['as', 'truth', 'as', 'steel', 'the', 'glory', 'where', 'the', 'best', '\\n']\n",
      "Completed sentence:  ['as', 'is', 'the', 'rest', 'how', 'ever', 'fair', 'it', 'be', '\\n']\n",
      "Completed sentence:  ['and', 'lay', 'the', 'fly', 'and', 'lay', 'was', 'well', 'and', 'see', '\\n']\n",
      "Completed sentence:  ['the', 'earth', 'can', 'yield', 'me', 'but', 'a', 'common', 'grave', '\\n']\n",
      "Model: \"sequential_402\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 40, 200)           874600    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "lambda_762 (Lambda)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "lambda_763 (Lambda)          (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4373)              1753573   \n",
      "=================================================================\n",
      "Total params: 3,589,773\n",
      "Trainable params: 3,589,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Completed sentence:  ['when', 'others', 'gaze', 'upon', 'their', 'shadows', 'vain', '\\n']\n",
      "Completed sentence:  ['then', 'when', 'he', 'when', 'love', 'me', 'in', 'me', 'or', 'so', '\\n']\n",
      "Completed sentence:  ['and', 'you', 'are', 'rich', 'and', 'ransom', 'all', 'ill', 'deeds', '\\n']\n",
      "Completed sentence:  ['ne', 'when', 'i', 'sigh', 'she', 'says', 'i', 'know', 'the', 'art', '\\n']\n",
      "Completed sentence:  ['when', 'in', 'thy', 'sweet', 'sweet', 'find', 'of', 'thy', 'sweet', 'skill', '\\n']\n",
      "Completed sentence:  ['would', 'be', 'of', 'my', 'poor', 'love', 'to', 'me', 'subscribes', '\\n']\n",
      "Completed sentence:  ['no', 'neither', 'i', 'have', 'thee', 'but', 'in', 'my', 'mind', '\\n']\n",
      "Completed sentence:  ['to', 'mourn', 'for', 'me', 'since', 'mourning', 'doth', 'thee', 'grace', '\\n']\n",
      "Completed sentence:  ['and', 'suit', 'thy', 'pity', 'like', 'in', 'every', 'part', '\\n']\n",
      "Completed sentence:  ['then', 'he', 'find', 'when', 'he', 'of', 'thy', 'love', 'away', '\\n']\n",
      "Completed sentence:  ['and', 'i', 'may', 'will', 'be', 'so', 'and', 'mine', 'eye', 'be', '\\n']\n",
      "Completed sentence:  ['and', 'in', 'our', 'faults', 'by', 'lies', 'we', 'flattered', 'be', '\\n']\n",
      "Completed sentence:  ['in', 'one', 'short', 'hour', 'thou', 'have', 'much', 'with', 'thee', '\\n']\n",
      "Completed sentence:  ['to', 'make', 'him', 'seem', 'long', 'hence', 'as', 'he', 'shows', 'now', '\\n']\n",
      "temperature:  1 , embedding size:  200 \n",
      " shall i compare thee to a summer's day \n",
      " when others gaze upon their shadows vain \n",
      " then him he when from me doth grow away \n",
      " so long i love was well and love love now \n",
      " and so profound abysm i throw all care \n",
      " of others' voices that my adder's sense \n",
      " to critic and to flatterer stopped are \n",
      " mark how with my neglect i do dispense \n",
      " you are so strongly in my purpose bred \n",
      " that all the world besides methinks are dead \n",
      " for his triumphant prize proud of this pride \n",
      " he is contented thy poor drudge to be \n",
      " to stand in thy affairs fall by thy side \n",
      " no want of conscience hold it that i call \n",
      " her love for whose dear love i rise and fall \n",
      " to make him seem long hence as he shows now \n",
      " and nothing mine thee how mine own desert \n",
      " \n",
      "\n",
      "temperature:  0.75 , embedding size:  200 \n",
      " shall i compare thee to a summer's day \n",
      " when in dead night thy fair imperfect shade \n",
      " through heavy sleep on sightless eyes doth stay \n",
      " all days are nights to see till i see thee \n",
      " and nights bright days when dreams do show thee me \n",
      " when so hold me when i have looked on me \n",
      " and when i am and water do not be \n",
      " and to the light lift up their drooping head \n",
      " so when her me he wise when from me me \n",
      " and wish that well and well and am well \n",
      " as i am now what is not be free \n",
      " as truth as steel the glory where the best \n",
      " as is the rest how ever fair it be \n",
      " and lay the fly and lay was well and see \n",
      " the earth can yield me but a common grave \n",
      " when you entombed in men's eyes shall \n",
      "\n",
      "temperature:  0.25 , embedding size:  200 \n",
      " shall i compare thee to a summer's day \n",
      " when others gaze upon their shadows vain \n",
      " then when he when love me in me or so \n",
      " and you are rich and ransom all ill deeds \n",
      " ne when i sigh she says i know the art \n",
      " when in thy sweet sweet find of thy sweet skill \n",
      " would be of my poor love to me subscribes \n",
      " no neither i have thee but in my mind \n",
      " to mourn for me since mourning doth thee grace \n",
      " and suit thy pity like in every part \n",
      " then he find when he of thy love away \n",
      " and i may will be so and mine eye be \n",
      " and in our faults by lies we flattered be \n",
      " in one short hour thou have much with thee \n",
      " to make him seem long hence as he shows now \n",
      " and having thee of my heart i do \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with checking pentameter\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "## model with minimum perplexity\n",
    "useWordEmbedding = True\n",
    "embedSize = [200]\n",
    "numUnit = [400]\n",
    "data = 'both'\n",
    "\n",
    "if data == 'shakespeare':\n",
    "    fileName_data = ''\n",
    "elif data == 'spenser':\n",
    "    fileName_data = 'Spenser_'\n",
    "elif data == 'both':\n",
    "    fileName_data = 'Both_'\n",
    "\n",
    "for i, dim in enumerate(embedSize):\n",
    "    for j, un in enumerate(numUnit):\n",
    "        if useWordEmbedding:\n",
    "            modelName = \"model_%sunit%d_withWordEmbedding %d.h5\" % (fileName_data, un, dim)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWordEmbedding %d.pk1\" % (fileName_data, un, dim)\n",
    "        test2 = LSTM_word()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "        if data == 'shakespeare':\n",
    "            test2.SonnetLoader('shakespeare')\n",
    "        elif data == 'spenser':\n",
    "            test2.SonnetLoader('Spenser_v2')\n",
    "        elif data == 'both':\n",
    "            test2.SonnetLoader(['shakespeare', 'Spenser_v2'])\n",
    "        \n",
    "        #tempList = [1.5, 1, 0.75, 0.25]\n",
    "        tempList = [1, 0.75, 0.25]\n",
    "        predicted = [test2.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=150, temperature = x, checkPentameter = True, useWordEmbedding = True) for x in tempList]\n",
    "        for i, x in enumerate(predicted):\n",
    "            print('temperature: ', tempList[i], ', embedding size: ', dim, '\\n', x, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HyeongChan Jo\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_650\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 40, 200)           874600    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 400)               961600    \n",
      "_________________________________________________________________\n",
      "lambda_1242 (Lambda)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "lambda_1243 (Lambda)         (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4373)              1753573   \n",
      "=================================================================\n",
      "Total params: 3,589,773\n",
      "Trainable params: 3,589,773\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Completed sentence:  ['when', 'others', 'gaze', 'upon', 'their', 'shadows', 'vain', '\\n']\n",
      "Completed sentence:  ['then', 'when', 'he', 'when', 'love', 'long', 'in', 'me', 'of', 'me', '\\n']\n",
      "Completed sentence:  ['so', 'when', 'i', 'sigh', 'she', 'says', 'i', 'know', 'the', 'art', '\\n']\n",
      "Completed sentence:  ['and', 'see', 'the', 'better', 'with', 'thy', 'late', 'hath', 'end', '\\n']\n",
      "Completed sentence:  ['and', 'i', 'in', 'hand', 'thou', 'hast', 'too', 'grossly', 'dyed', '\\n']\n",
      "Completed sentence:  ['that', 'i', 'am', 'still', 'with', 'them', 'and', 'they', 'with', 'thee', '\\n']\n",
      "Completed sentence:  ['or', 'if', 'they', 'sleep', 'thy', 'picture', 'in', 'my', 'sight', '\\n']\n",
      "Completed sentence:  ['awakes', 'my', 'heart', 'and', 'give', 'far', 'and', 'well', '\\n']\n",
      "Completed sentence:  ['more', 'is', 'so', 'proud', 'thy', 'love', 'thy', 'love', 'to', 'thee', '\\n']\n",
      "Completed sentence:  ['then', 'if', 'i', 'say', 'you', 'look', 'to', 'have', 'years', 'told', '\\n']\n",
      "Completed sentence:  ['i', 'must', 'begin', 'and', 'never', 'bring', 'to', 'end', '\\n']\n",
      "Completed sentence:  ['for', 'with', 'one', 'look', 'she', 'spills', 'that', 'long', 'i', 'spun', '\\n']\n",
      "Completed sentence:  ['and', 'with', 'one', 'word', 'my', 'whole', \"year's\", 'work', 'doth', 'rend', '\\n']\n",
      "Completed sentence:  ['such', 'labour', 'like', 'the', \"spider's\", 'web', 'i', 'find', '\\n']\n",
      "temperature:  1.5 , embedding size:  200 \n",
      " shall i compare thee to a summer's day \n",
      " when others gaze upon their shadows vain \n",
      " then when he when love long in me of me \n",
      " so when i sigh she says i know the art \n",
      " and see the better with thy late hath end \n",
      " and i in hand thou hast too grossly dyed \n",
      " that i am still with them and they with thee \n",
      " or if they sleep thy picture in my sight \n",
      " awakes my heart and give far and well \n",
      " more is so proud thy love thy love to thee \n",
      " then if i say you look to have years told \n",
      " i must begin and never bring to end \n",
      " for with one look she spills that long i spun \n",
      " and with one word my whole year's work doth rend \n",
      " such labour like the spider's web i find \n",
      " whose fruitless work is broken with least wind \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with checking pentameter\n",
    "from LSTMforSonnet import LSTM_char, LSTM_word\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "## model with minimum perplexity\n",
    "useWordEmbedding = True\n",
    "embedSize = [200]\n",
    "numUnit = [400]\n",
    "data = 'both'\n",
    "\n",
    "if data == 'shakespeare':\n",
    "    fileName_data = ''\n",
    "elif data == 'spenser':\n",
    "    fileName_data = 'Spenser_'\n",
    "elif data == 'both':\n",
    "    fileName_data = 'Both_'\n",
    "\n",
    "for i, dim in enumerate(embedSize):\n",
    "    for j, un in enumerate(numUnit):\n",
    "        if useWordEmbedding:\n",
    "            modelName = \"model_%sunit%d_withWordEmbedding %d.h5\" % (fileName_data, un, dim)\n",
    "            mappingName = \"model_%sunit%d_mapping_withWordEmbedding %d.pk1\" % (fileName_data, un, dim)\n",
    "        test2 = LSTM_word()\n",
    "        test2.LoadModel(modelName = modelName, mappingName = mappingName)\n",
    "        if data == 'shakespeare':\n",
    "            test2.SonnetLoader('shakespeare')\n",
    "        elif data == 'spenser':\n",
    "            test2.SonnetLoader('Spenser_v2')\n",
    "        elif data == 'both':\n",
    "            test2.SonnetLoader(['shakespeare', 'Spenser_v2'])\n",
    "        \n",
    "        #tempList = [1.5, 1, 0.75, 0.25]\n",
    "        tempList = [1.5]\n",
    "        predicted = [test2.Predict(\"shall i compare thee to a summer's day?\\n\", outputText_len=150, temperature = x, checkPentameter = True, useWordEmbedding = True) for x in tempList]\n",
    "        for i, x in enumerate(predicted):\n",
    "            print('temperature: ', tempList[i], ', embedding size: ', dim, '\\n', x, '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-610e78f865fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'predicted' is not defined"
     ]
    }
   ],
   "source": [
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_16 (LSTM)               (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/4000\n",
      " - 3s - loss: 2.7865 - accuracy: 0.2494\n",
      "Epoch 2/4000\n",
      " - 3s - loss: 2.2614 - accuracy: 0.3577\n",
      "Epoch 3/4000\n",
      " - 3s - loss: 2.0998 - accuracy: 0.3966\n",
      "Epoch 4/4000\n",
      " - 3s - loss: 1.9974 - accuracy: 0.4155\n",
      "Epoch 5/4000\n",
      " - 3s - loss: 1.9213 - accuracy: 0.4317\n",
      "Epoch 6/4000\n",
      " - 3s - loss: 1.8512 - accuracy: 0.4496\n",
      "Epoch 7/4000\n",
      " - 3s - loss: 1.7957 - accuracy: 0.4613\n",
      "Epoch 8/4000\n",
      " - 3s - loss: 1.7407 - accuracy: 0.4734\n",
      "Epoch 9/4000\n",
      " - 3s - loss: 1.6957 - accuracy: 0.4844\n",
      "Epoch 10/4000\n",
      " - 3s - loss: 1.6478 - accuracy: 0.4934\n",
      "Epoch 11/4000\n",
      " - 3s - loss: 1.6069 - accuracy: 0.5051\n",
      "Epoch 12/4000\n",
      " - 3s - loss: 1.5683 - accuracy: 0.5131\n",
      "Epoch 13/4000\n",
      " - 3s - loss: 1.5286 - accuracy: 0.5224\n",
      "Epoch 14/4000\n",
      " - 3s - loss: 1.4923 - accuracy: 0.5308\n",
      "Epoch 15/4000\n",
      " - 3s - loss: 1.4594 - accuracy: 0.5406\n",
      "Epoch 16/4000\n",
      " - 3s - loss: 1.4253 - accuracy: 0.5493\n",
      "Epoch 17/4000\n",
      " - 3s - loss: 1.3929 - accuracy: 0.5556\n",
      "Epoch 18/4000\n",
      " - 3s - loss: 1.3628 - accuracy: 0.5620\n",
      "Epoch 19/4000\n",
      " - 3s - loss: 1.3352 - accuracy: 0.5688\n",
      "Epoch 20/4000\n",
      " - 3s - loss: 1.3095 - accuracy: 0.5767\n",
      "Epoch 21/4000\n",
      " - 3s - loss: 1.2820 - accuracy: 0.5823\n",
      "Epoch 22/4000\n",
      " - 3s - loss: 1.2579 - accuracy: 0.5878\n",
      "Epoch 23/4000\n",
      " - 3s - loss: 1.2356 - accuracy: 0.5918\n",
      "Epoch 24/4000\n",
      " - 3s - loss: 1.2142 - accuracy: 0.5954\n",
      "Epoch 25/4000\n",
      " - 3s - loss: 1.1942 - accuracy: 0.6011\n",
      "Epoch 26/4000\n",
      " - 3s - loss: 1.1753 - accuracy: 0.6077\n",
      "Epoch 27/4000\n",
      " - 3s - loss: 1.1588 - accuracy: 0.6093\n",
      "Epoch 28/4000\n",
      " - 3s - loss: 1.1410 - accuracy: 0.6124\n",
      "Epoch 29/4000\n",
      " - 3s - loss: 1.1287 - accuracy: 0.6122\n",
      "Epoch 30/4000\n",
      " - 3s - loss: 1.1114 - accuracy: 0.6201\n",
      "Epoch 31/4000\n",
      " - 3s - loss: 1.0975 - accuracy: 0.6227\n",
      "Epoch 32/4000\n",
      " - 3s - loss: 1.0846 - accuracy: 0.6218\n",
      "Epoch 33/4000\n",
      " - 3s - loss: 1.0764 - accuracy: 0.6239\n",
      "Epoch 34/4000\n",
      " - 3s - loss: 1.0627 - accuracy: 0.6280\n",
      "Epoch 35/4000\n",
      " - 3s - loss: 1.0521 - accuracy: 0.6272\n",
      "Epoch 36/4000\n",
      " - 3s - loss: 1.0436 - accuracy: 0.6303\n",
      "Epoch 37/4000\n",
      " - 3s - loss: 1.0348 - accuracy: 0.6329\n",
      "Epoch 38/4000\n",
      " - 3s - loss: 1.0272 - accuracy: 0.6316\n",
      "Epoch 39/4000\n",
      " - 3s - loss: 1.0211 - accuracy: 0.6327\n",
      "Epoch 40/4000\n",
      " - 3s - loss: 1.0103 - accuracy: 0.6339\n",
      "Epoch 41/4000\n",
      " - 3s - loss: 1.0080 - accuracy: 0.6335\n",
      "Epoch 42/4000\n",
      " - 3s - loss: 1.0003 - accuracy: 0.6366\n",
      "Epoch 43/4000\n",
      " - 3s - loss: 0.9929 - accuracy: 0.6390\n",
      "Epoch 44/4000\n",
      " - 3s - loss: 0.9889 - accuracy: 0.6362\n",
      "Epoch 45/4000\n",
      " - 3s - loss: 0.9834 - accuracy: 0.6367\n",
      "Epoch 46/4000\n",
      " - 3s - loss: 0.9796 - accuracy: 0.6367\n",
      "Epoch 47/4000\n",
      " - 3s - loss: 0.9757 - accuracy: 0.6358\n",
      "Epoch 48/4000\n",
      " - 3s - loss: 0.9714 - accuracy: 0.6386\n",
      "Epoch 49/4000\n",
      " - 3s - loss: 0.9653 - accuracy: 0.6399\n",
      "Epoch 50/4000\n",
      " - 3s - loss: 0.9636 - accuracy: 0.6387\n",
      "Epoch 51/4000\n",
      " - 3s - loss: 0.9601 - accuracy: 0.6388\n",
      "Epoch 52/4000\n",
      " - 3s - loss: 0.9580 - accuracy: 0.6381\n",
      "Epoch 53/4000\n",
      " - 3s - loss: 0.9530 - accuracy: 0.6377\n",
      "Epoch 54/4000\n",
      " - 3s - loss: 0.9472 - accuracy: 0.6405\n",
      "Epoch 55/4000\n",
      " - 3s - loss: 0.9484 - accuracy: 0.6384\n",
      "Epoch 56/4000\n",
      " - 3s - loss: 0.9450 - accuracy: 0.6388\n",
      "Epoch 57/4000\n",
      " - 3s - loss: 0.9432 - accuracy: 0.6392\n",
      "Epoch 58/4000\n",
      " - 3s - loss: 0.9396 - accuracy: 0.6381\n",
      "Epoch 59/4000\n",
      " - 3s - loss: 0.9366 - accuracy: 0.6425\n",
      "Epoch 60/4000\n",
      " - 3s - loss: 0.9349 - accuracy: 0.6399\n",
      "Epoch 61/4000\n",
      " - 3s - loss: 0.9330 - accuracy: 0.6388\n",
      "Epoch 62/4000\n",
      " - 3s - loss: 0.9304 - accuracy: 0.6430\n",
      "Epoch 63/4000\n",
      " - 3s - loss: 0.9293 - accuracy: 0.6414\n",
      "Epoch 64/4000\n",
      " - 3s - loss: 0.9263 - accuracy: 0.6417\n",
      "Epoch 65/4000\n",
      " - 3s - loss: 0.9242 - accuracy: 0.6405\n",
      "Epoch 66/4000\n",
      " - 3s - loss: 0.9231 - accuracy: 0.6405\n",
      "Epoch 67/4000\n",
      " - 3s - loss: 0.9201 - accuracy: 0.6395\n",
      "Epoch 68/4000\n",
      " - 3s - loss: 0.9204 - accuracy: 0.6373\n",
      "Epoch 69/4000\n",
      " - 3s - loss: 0.9179 - accuracy: 0.6392\n",
      "Epoch 70/4000\n",
      " - 3s - loss: 0.9168 - accuracy: 0.6404\n",
      "Epoch 71/4000\n",
      " - 3s - loss: 0.9148 - accuracy: 0.6402\n",
      "Epoch 72/4000\n",
      " - 3s - loss: 0.9131 - accuracy: 0.6421\n",
      "Epoch 73/4000\n",
      " - 3s - loss: 0.9121 - accuracy: 0.6420\n",
      "Epoch 74/4000\n",
      " - 3s - loss: 0.9121 - accuracy: 0.6404\n",
      "Epoch 75/4000\n",
      " - 3s - loss: 0.9088 - accuracy: 0.6434\n",
      "Epoch 76/4000\n",
      " - 3s - loss: 0.9079 - accuracy: 0.6417\n",
      "Epoch 77/4000\n",
      " - 3s - loss: 0.9066 - accuracy: 0.6391\n",
      "Epoch 78/4000\n",
      " - 3s - loss: 0.9053 - accuracy: 0.6435\n",
      "Epoch 79/4000\n",
      " - 3s - loss: 0.9056 - accuracy: 0.6392\n",
      "Epoch 80/4000\n",
      " - 3s - loss: 0.9015 - accuracy: 0.6389\n",
      "Epoch 81/4000\n",
      " - 3s - loss: 0.9021 - accuracy: 0.6408\n",
      "Epoch 82/4000\n",
      " - 3s - loss: 0.9010 - accuracy: 0.6400\n",
      "Epoch 83/4000\n",
      " - 3s - loss: 0.9003 - accuracy: 0.6413\n",
      "Epoch 84/4000\n",
      " - 3s - loss: 0.8996 - accuracy: 0.6419\n",
      "Epoch 85/4000\n",
      " - 3s - loss: 0.8981 - accuracy: 0.6413\n",
      "Epoch 86/4000\n",
      " - 3s - loss: 0.8959 - accuracy: 0.6404\n",
      "Epoch 87/4000\n",
      " - 3s - loss: 0.8959 - accuracy: 0.6409\n",
      "Epoch 88/4000\n",
      " - 3s - loss: 0.8955 - accuracy: 0.6404\n",
      "Epoch 89/4000\n",
      " - 3s - loss: 0.8934 - accuracy: 0.6411\n",
      "Epoch 90/4000\n",
      " - 3s - loss: 0.8942 - accuracy: 0.6402\n",
      "Epoch 91/4000\n",
      " - 3s - loss: 0.8910 - accuracy: 0.6411\n",
      "Epoch 92/4000\n",
      " - 3s - loss: 0.8919 - accuracy: 0.6419\n",
      "Epoch 93/4000\n",
      " - 3s - loss: 0.8917 - accuracy: 0.6408\n",
      "Epoch 94/4000\n",
      " - 3s - loss: 0.8901 - accuracy: 0.6409\n",
      "Epoch 95/4000\n",
      " - 3s - loss: 0.8889 - accuracy: 0.6393\n",
      "Epoch 96/4000\n",
      " - 3s - loss: 0.8880 - accuracy: 0.6414\n",
      "Epoch 97/4000\n",
      " - 3s - loss: 0.8877 - accuracy: 0.6408\n",
      "Epoch 98/4000\n",
      " - 3s - loss: 0.8879 - accuracy: 0.6386\n",
      "Epoch 99/4000\n",
      " - 3s - loss: 0.8860 - accuracy: 0.6408\n",
      "Epoch 100/4000\n",
      " - 3s - loss: 0.8856 - accuracy: 0.6436\n",
      "Epoch 101/4000\n",
      " - 3s - loss: 0.8833 - accuracy: 0.6412\n",
      "Epoch 102/4000\n",
      " - 3s - loss: 0.8854 - accuracy: 0.6393\n",
      "Epoch 103/4000\n",
      " - 3s - loss: 0.8827 - accuracy: 0.6386\n",
      "Epoch 104/4000\n",
      " - 3s - loss: 0.8823 - accuracy: 0.6407\n",
      "Epoch 105/4000\n",
      " - 3s - loss: 0.8810 - accuracy: 0.6413\n",
      "Epoch 106/4000\n",
      " - 3s - loss: 0.8811 - accuracy: 0.6422\n",
      "Epoch 107/4000\n",
      " - 3s - loss: 0.8802 - accuracy: 0.6417\n",
      "Epoch 108/4000\n",
      " - 3s - loss: 0.8795 - accuracy: 0.6419\n",
      "Epoch 109/4000\n",
      " - 3s - loss: 0.8780 - accuracy: 0.6406\n",
      "Epoch 110/4000\n",
      " - 3s - loss: 0.8794 - accuracy: 0.6404\n",
      "Epoch 111/4000\n",
      " - 3s - loss: 0.8781 - accuracy: 0.6413\n",
      "Epoch 112/4000\n",
      " - 3s - loss: 0.8770 - accuracy: 0.6411\n",
      "Epoch 113/4000\n",
      " - 3s - loss: 0.8763 - accuracy: 0.6391\n",
      "Epoch 114/4000\n",
      " - 3s - loss: 0.8756 - accuracy: 0.6404\n",
      "Epoch 115/4000\n",
      " - 3s - loss: 0.8747 - accuracy: 0.6442\n",
      "Epoch 116/4000\n",
      " - 3s - loss: 0.8745 - accuracy: 0.6432\n",
      "Epoch 117/4000\n",
      " - 3s - loss: 0.8744 - accuracy: 0.6406\n",
      "Epoch 118/4000\n",
      " - 3s - loss: 0.8729 - accuracy: 0.6436\n",
      "Epoch 119/4000\n",
      " - 3s - loss: 0.8725 - accuracy: 0.6424\n",
      "Epoch 120/4000\n",
      " - 3s - loss: 0.8728 - accuracy: 0.6431\n",
      "Epoch 121/4000\n",
      " - 3s - loss: 0.8720 - accuracy: 0.6436\n",
      "Epoch 122/4000\n",
      " - 3s - loss: 0.8707 - accuracy: 0.6418\n",
      "Epoch 123/4000\n",
      " - 3s - loss: 0.8719 - accuracy: 0.6402\n",
      "Epoch 124/4000\n",
      " - 3s - loss: 0.8707 - accuracy: 0.6410\n",
      "Epoch 125/4000\n",
      " - 3s - loss: 0.8707 - accuracy: 0.6426\n",
      "Epoch 126/4000\n",
      " - 3s - loss: 0.8693 - accuracy: 0.6412\n",
      "Epoch 127/4000\n",
      " - 3s - loss: 0.8700 - accuracy: 0.6443\n",
      "Epoch 128/4000\n",
      " - 3s - loss: 0.8675 - accuracy: 0.6413\n",
      "Epoch 129/4000\n",
      " - 3s - loss: 0.8682 - accuracy: 0.6428\n",
      "Epoch 130/4000\n",
      " - 3s - loss: 0.8675 - accuracy: 0.6434\n",
      "Epoch 131/4000\n",
      " - 3s - loss: 0.8666 - accuracy: 0.6427\n",
      "Epoch 132/4000\n",
      " - 3s - loss: 0.8657 - accuracy: 0.6431\n",
      "Epoch 133/4000\n",
      " - 3s - loss: 0.8662 - accuracy: 0.6417\n",
      "Epoch 134/4000\n",
      " - 3s - loss: 0.8655 - accuracy: 0.6415\n",
      "Epoch 135/4000\n",
      " - 3s - loss: 0.8641 - accuracy: 0.6439\n",
      "Epoch 136/4000\n",
      " - 3s - loss: 0.8650 - accuracy: 0.6425\n",
      "Epoch 137/4000\n",
      " - 3s - loss: 0.8633 - accuracy: 0.6423\n",
      "Epoch 138/4000\n",
      " - 3s - loss: 0.8630 - accuracy: 0.6427\n",
      "Epoch 139/4000\n",
      " - 3s - loss: 0.8643 - accuracy: 0.6410\n",
      "Epoch 140/4000\n",
      " - 3s - loss: 0.8638 - accuracy: 0.6428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/4000\n",
      " - 3s - loss: 0.8614 - accuracy: 0.6436\n",
      "Epoch 142/4000\n",
      " - 3s - loss: 0.8616 - accuracy: 0.6432\n",
      "Epoch 143/4000\n",
      " - 3s - loss: 0.8618 - accuracy: 0.6438\n",
      "Epoch 144/4000\n",
      " - 3s - loss: 0.8602 - accuracy: 0.6419\n",
      "Epoch 145/4000\n",
      " - 3s - loss: 0.8611 - accuracy: 0.6438\n",
      "Epoch 146/4000\n",
      " - 3s - loss: 0.8618 - accuracy: 0.6437\n",
      "Epoch 147/4000\n",
      " - 3s - loss: 0.8603 - accuracy: 0.6447\n",
      "Epoch 148/4000\n",
      " - 3s - loss: 0.8597 - accuracy: 0.6447\n",
      "Epoch 149/4000\n",
      " - 3s - loss: 0.8582 - accuracy: 0.6436\n",
      "Epoch 150/4000\n",
      " - 3s - loss: 0.8586 - accuracy: 0.6451\n",
      "Epoch 151/4000\n",
      " - 3s - loss: 0.8581 - accuracy: 0.6450\n",
      "Epoch 152/4000\n",
      " - 3s - loss: 0.8568 - accuracy: 0.6450\n",
      "Epoch 153/4000\n",
      " - 3s - loss: 0.8567 - accuracy: 0.6471\n",
      "Epoch 154/4000\n",
      " - 3s - loss: 0.8587 - accuracy: 0.6438\n",
      "Epoch 155/4000\n",
      " - 3s - loss: 0.8559 - accuracy: 0.6441\n",
      "Epoch 156/4000\n",
      " - 3s - loss: 0.8559 - accuracy: 0.6417\n",
      "Epoch 157/4000\n",
      " - 3s - loss: 0.8551 - accuracy: 0.6453\n",
      "Epoch 158/4000\n",
      " - 3s - loss: 0.8557 - accuracy: 0.6435\n",
      "Epoch 159/4000\n",
      " - 3s - loss: 0.8551 - accuracy: 0.6439\n",
      "Epoch 160/4000\n",
      " - 3s - loss: 0.8561 - accuracy: 0.6438\n",
      "Epoch 161/4000\n",
      " - 3s - loss: 0.8541 - accuracy: 0.6440\n",
      "Epoch 162/4000\n",
      " - 3s - loss: 0.8535 - accuracy: 0.6456\n",
      "Epoch 163/4000\n",
      " - 3s - loss: 0.8538 - accuracy: 0.6440\n",
      "Epoch 164/4000\n",
      " - 3s - loss: 0.8535 - accuracy: 0.6435\n",
      "Epoch 165/4000\n",
      " - 3s - loss: 0.8530 - accuracy: 0.6454\n",
      "Epoch 166/4000\n",
      " - 3s - loss: 0.8519 - accuracy: 0.6435\n",
      "Epoch 167/4000\n",
      " - 3s - loss: 0.8530 - accuracy: 0.6437\n",
      "Epoch 168/4000\n",
      " - 3s - loss: 0.8516 - accuracy: 0.6468\n",
      "Epoch 169/4000\n",
      " - 3s - loss: 0.8513 - accuracy: 0.6430\n",
      "Epoch 170/4000\n",
      " - 3s - loss: 0.8513 - accuracy: 0.6435\n",
      "Epoch 171/4000\n",
      " - 3s - loss: 0.8518 - accuracy: 0.6446\n",
      "Epoch 172/4000\n",
      " - 3s - loss: 0.8508 - accuracy: 0.6445\n",
      "Epoch 173/4000\n",
      " - 3s - loss: 0.8500 - accuracy: 0.6462\n",
      "Epoch 174/4000\n",
      " - 3s - loss: 0.8509 - accuracy: 0.6445\n",
      "Epoch 175/4000\n",
      " - 3s - loss: 0.8497 - accuracy: 0.6432\n",
      "Epoch 176/4000\n",
      " - 3s - loss: 0.8495 - accuracy: 0.6449\n",
      "Epoch 177/4000\n",
      " - 3s - loss: 0.8494 - accuracy: 0.6455\n",
      "Epoch 178/4000\n",
      " - 3s - loss: 0.8488 - accuracy: 0.6459\n",
      "Epoch 179/4000\n",
      " - 3s - loss: 0.8484 - accuracy: 0.6461\n",
      "Epoch 180/4000\n",
      " - 3s - loss: 0.8489 - accuracy: 0.6436\n",
      "Epoch 181/4000\n",
      " - 3s - loss: 0.8479 - accuracy: 0.6463\n",
      "Epoch 182/4000\n",
      " - 3s - loss: 0.8474 - accuracy: 0.6452\n",
      "Epoch 183/4000\n",
      " - 3s - loss: 0.8480 - accuracy: 0.6419\n",
      "Epoch 184/4000\n",
      " - 3s - loss: 0.8471 - accuracy: 0.6437\n",
      "Epoch 185/4000\n",
      " - 3s - loss: 0.8474 - accuracy: 0.6455\n",
      "Epoch 186/4000\n",
      " - 3s - loss: 0.8471 - accuracy: 0.6447\n",
      "Epoch 187/4000\n",
      " - 3s - loss: 0.8454 - accuracy: 0.6444\n",
      "Epoch 188/4000\n",
      " - 3s - loss: 0.8454 - accuracy: 0.6462\n",
      "Epoch 189/4000\n",
      " - 3s - loss: 0.8447 - accuracy: 0.6454\n",
      "Epoch 190/4000\n",
      " - 3s - loss: 0.8457 - accuracy: 0.6439\n",
      "Epoch 191/4000\n",
      " - 3s - loss: 0.8445 - accuracy: 0.6458\n",
      "Epoch 192/4000\n",
      " - 3s - loss: 0.8460 - accuracy: 0.6440\n",
      "Epoch 193/4000\n",
      " - 3s - loss: 0.8448 - accuracy: 0.6460\n",
      "Epoch 194/4000\n",
      " - 3s - loss: 0.8431 - accuracy: 0.6476\n",
      "Epoch 195/4000\n",
      " - 3s - loss: 0.8457 - accuracy: 0.6432\n",
      "Epoch 196/4000\n",
      " - 3s - loss: 0.8434 - accuracy: 0.6448\n",
      "Epoch 197/4000\n",
      " - 3s - loss: 0.8434 - accuracy: 0.6456\n",
      "Epoch 198/4000\n",
      " - 3s - loss: 0.8439 - accuracy: 0.6446\n",
      "Epoch 199/4000\n",
      " - 3s - loss: 0.8431 - accuracy: 0.6466\n",
      "Epoch 200/4000\n",
      " - 3s - loss: 0.8423 - accuracy: 0.6475\n",
      "Epoch 201/4000\n",
      " - 3s - loss: 0.8433 - accuracy: 0.6459\n",
      "Epoch 202/4000\n",
      " - 3s - loss: 0.8421 - accuracy: 0.6448\n",
      "Epoch 203/4000\n",
      " - 3s - loss: 0.8416 - accuracy: 0.6461\n",
      "Epoch 204/4000\n",
      " - 3s - loss: 0.8412 - accuracy: 0.6467\n",
      "Epoch 205/4000\n",
      " - 3s - loss: 0.8417 - accuracy: 0.6469\n",
      "Epoch 206/4000\n",
      " - 3s - loss: 0.8414 - accuracy: 0.6456\n",
      "Epoch 207/4000\n",
      " - 3s - loss: 0.8413 - accuracy: 0.6463\n",
      "Epoch 208/4000\n",
      " - 3s - loss: 0.8416 - accuracy: 0.6465\n",
      "Epoch 209/4000\n",
      " - 3s - loss: 0.8411 - accuracy: 0.6436\n",
      "Epoch 210/4000\n",
      " - 3s - loss: 0.8402 - accuracy: 0.6465\n",
      "Epoch 211/4000\n",
      " - 3s - loss: 0.8392 - accuracy: 0.6455\n",
      "Epoch 212/4000\n",
      " - 3s - loss: 0.8405 - accuracy: 0.6455\n",
      "Epoch 213/4000\n",
      " - 3s - loss: 0.8392 - accuracy: 0.6484\n",
      "Epoch 214/4000\n",
      " - 3s - loss: 0.8390 - accuracy: 0.6464\n",
      "Epoch 215/4000\n",
      " - 3s - loss: 0.8392 - accuracy: 0.6453\n",
      "Epoch 216/4000\n",
      " - 3s - loss: 0.8391 - accuracy: 0.6480\n",
      "Epoch 217/4000\n",
      " - 3s - loss: 0.8393 - accuracy: 0.6461\n",
      "Epoch 218/4000\n",
      " - 3s - loss: 0.8398 - accuracy: 0.6462\n",
      "Epoch 219/4000\n",
      " - 3s - loss: 0.8382 - accuracy: 0.6453\n",
      "Epoch 220/4000\n",
      " - 3s - loss: 0.8382 - accuracy: 0.6467\n",
      "Epoch 221/4000\n",
      " - 3s - loss: 0.8375 - accuracy: 0.6478\n",
      "Epoch 222/4000\n",
      " - 3s - loss: 0.8382 - accuracy: 0.6472\n",
      "Epoch 223/4000\n",
      " - 3s - loss: 0.8368 - accuracy: 0.6453\n",
      "Epoch 224/4000\n",
      " - 3s - loss: 0.8379 - accuracy: 0.6458\n",
      "Epoch 225/4000\n",
      " - 3s - loss: 0.8368 - accuracy: 0.6449\n",
      "Epoch 226/4000\n",
      " - 3s - loss: 0.8369 - accuracy: 0.6474\n",
      "Epoch 227/4000\n",
      " - 3s - loss: 0.8370 - accuracy: 0.6461\n",
      "Epoch 228/4000\n",
      " - 3s - loss: 0.8361 - accuracy: 0.6481\n",
      "Epoch 229/4000\n",
      " - 3s - loss: 0.8357 - accuracy: 0.6459\n",
      "Epoch 230/4000\n",
      " - 3s - loss: 0.8351 - accuracy: 0.6488\n",
      "Epoch 231/4000\n",
      " - 3s - loss: 0.8360 - accuracy: 0.6461\n",
      "Epoch 232/4000\n",
      " - 3s - loss: 0.8358 - accuracy: 0.6464\n",
      "Epoch 233/4000\n",
      " - 3s - loss: 0.8354 - accuracy: 0.6443\n",
      "Epoch 234/4000\n",
      " - 3s - loss: 0.8360 - accuracy: 0.6464\n",
      "Epoch 235/4000\n",
      " - 3s - loss: 0.8335 - accuracy: 0.6492\n",
      "Epoch 236/4000\n",
      " - 3s - loss: 0.8347 - accuracy: 0.6467\n",
      "Epoch 237/4000\n",
      " - 3s - loss: 0.8346 - accuracy: 0.6456\n",
      "Epoch 238/4000\n",
      " - 3s - loss: 0.8345 - accuracy: 0.6460\n",
      "Epoch 239/4000\n",
      " - 3s - loss: 0.8338 - accuracy: 0.6463\n",
      "Epoch 240/4000\n",
      " - 3s - loss: 0.8339 - accuracy: 0.6485\n",
      "Epoch 241/4000\n",
      " - 3s - loss: 0.8343 - accuracy: 0.6460\n",
      "Epoch 242/4000\n",
      " - 3s - loss: 0.8332 - accuracy: 0.6468\n",
      "Epoch 243/4000\n",
      " - 3s - loss: 0.8344 - accuracy: 0.6466\n",
      "Epoch 244/4000\n",
      " - 3s - loss: 0.8326 - accuracy: 0.6465\n",
      "Epoch 245/4000\n",
      " - 3s - loss: 0.8319 - accuracy: 0.6468\n",
      "Epoch 246/4000\n",
      " - 3s - loss: 0.8331 - accuracy: 0.6468\n",
      "Epoch 247/4000\n",
      " - 3s - loss: 0.8324 - accuracy: 0.6480\n",
      "Epoch 248/4000\n",
      " - 3s - loss: 0.8323 - accuracy: 0.6468\n",
      "Epoch 249/4000\n",
      " - 3s - loss: 0.8320 - accuracy: 0.6474\n",
      "Epoch 250/4000\n",
      " - 3s - loss: 0.8325 - accuracy: 0.6472\n",
      "Epoch 251/4000\n",
      " - 3s - loss: 0.8320 - accuracy: 0.6490\n",
      "Epoch 252/4000\n",
      " - 3s - loss: 0.8311 - accuracy: 0.6471\n",
      "Epoch 253/4000\n",
      " - 3s - loss: 0.8316 - accuracy: 0.6466\n",
      "Epoch 254/4000\n",
      " - 3s - loss: 0.8305 - accuracy: 0.6470\n",
      "Epoch 255/4000\n",
      " - 3s - loss: 0.8311 - accuracy: 0.6458\n",
      "Epoch 256/4000\n",
      " - 3s - loss: 0.8314 - accuracy: 0.6481\n",
      "Epoch 257/4000\n",
      " - 3s - loss: 0.8297 - accuracy: 0.6488\n",
      "Epoch 258/4000\n",
      " - 3s - loss: 0.8309 - accuracy: 0.6479\n",
      "Epoch 259/4000\n",
      " - 3s - loss: 0.8312 - accuracy: 0.6478\n",
      "Epoch 260/4000\n",
      " - 3s - loss: 0.8304 - accuracy: 0.6479\n",
      "Epoch 261/4000\n",
      " - 3s - loss: 0.8300 - accuracy: 0.6487\n",
      "Epoch 262/4000\n",
      " - 3s - loss: 0.8294 - accuracy: 0.6487\n",
      "Epoch 263/4000\n",
      " - 3s - loss: 0.8294 - accuracy: 0.6470\n",
      "Epoch 264/4000\n",
      " - 3s - loss: 0.8292 - accuracy: 0.6490\n",
      "Epoch 265/4000\n",
      " - 3s - loss: 0.8289 - accuracy: 0.6478\n",
      "Epoch 266/4000\n",
      " - 3s - loss: 0.8287 - accuracy: 0.6492\n",
      "Epoch 267/4000\n",
      " - 3s - loss: 0.8288 - accuracy: 0.6474\n",
      "Epoch 268/4000\n",
      " - 3s - loss: 0.8283 - accuracy: 0.6472\n",
      "Epoch 269/4000\n",
      " - 3s - loss: 0.8280 - accuracy: 0.6491\n",
      "Epoch 270/4000\n",
      " - 3s - loss: 0.8285 - accuracy: 0.6483\n",
      "Epoch 271/4000\n",
      " - 3s - loss: 0.8276 - accuracy: 0.6487\n",
      "Epoch 272/4000\n",
      " - 3s - loss: 0.8283 - accuracy: 0.6476\n",
      "Epoch 273/4000\n",
      " - 3s - loss: 0.8284 - accuracy: 0.6464\n",
      "Epoch 274/4000\n",
      " - 3s - loss: 0.8279 - accuracy: 0.6492\n",
      "Epoch 275/4000\n",
      " - 3s - loss: 0.8285 - accuracy: 0.6491\n",
      "Epoch 276/4000\n",
      " - 3s - loss: 0.8272 - accuracy: 0.6481\n",
      "Epoch 277/4000\n",
      " - 3s - loss: 0.8278 - accuracy: 0.6477\n",
      "Epoch 278/4000\n",
      " - 3s - loss: 0.8268 - accuracy: 0.6488\n",
      "Epoch 279/4000\n",
      " - 3s - loss: 0.8270 - accuracy: 0.6490\n",
      "Epoch 280/4000\n",
      " - 3s - loss: 0.8271 - accuracy: 0.6473\n",
      "Epoch 281/4000\n",
      " - 3s - loss: 0.8267 - accuracy: 0.6484\n",
      "Epoch 282/4000\n",
      " - 3s - loss: 0.8263 - accuracy: 0.6477\n",
      "Epoch 283/4000\n",
      " - 3s - loss: 0.8264 - accuracy: 0.6489\n",
      "Epoch 284/4000\n",
      " - 3s - loss: 0.8262 - accuracy: 0.6480\n",
      "Epoch 285/4000\n",
      " - 3s - loss: 0.8259 - accuracy: 0.6496\n",
      "Epoch 286/4000\n",
      " - 3s - loss: 0.8258 - accuracy: 0.6499\n",
      "Epoch 287/4000\n",
      " - 3s - loss: 0.8261 - accuracy: 0.6475\n",
      "Epoch 288/4000\n",
      " - 3s - loss: 0.8254 - accuracy: 0.6463\n",
      "Epoch 289/4000\n",
      " - 3s - loss: 0.8258 - accuracy: 0.6495\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 290/4000\n",
      " - 3s - loss: 0.8252 - accuracy: 0.6487\n",
      "Epoch 291/4000\n",
      " - 3s - loss: 0.8250 - accuracy: 0.6494\n",
      "Epoch 292/4000\n",
      " - 3s - loss: 0.8248 - accuracy: 0.6463\n",
      "Epoch 293/4000\n",
      " - 3s - loss: 0.8250 - accuracy: 0.6467\n",
      "Epoch 294/4000\n",
      " - 3s - loss: 0.8242 - accuracy: 0.6505\n",
      "Epoch 295/4000\n",
      " - 3s - loss: 0.8239 - accuracy: 0.6492\n",
      "Epoch 296/4000\n",
      " - 3s - loss: 0.8252 - accuracy: 0.6480\n",
      "Epoch 297/4000\n",
      " - 3s - loss: 0.8239 - accuracy: 0.6505\n",
      "Epoch 298/4000\n",
      " - 3s - loss: 0.8242 - accuracy: 0.6484\n",
      "Epoch 299/4000\n",
      " - 3s - loss: 0.8235 - accuracy: 0.6492\n",
      "Epoch 300/4000\n",
      " - 3s - loss: 0.8226 - accuracy: 0.6493\n",
      "Epoch 301/4000\n",
      " - 3s - loss: 0.8236 - accuracy: 0.6487\n",
      "Epoch 302/4000\n",
      " - 3s - loss: 0.8237 - accuracy: 0.6488\n",
      "Epoch 303/4000\n",
      " - 3s - loss: 0.8230 - accuracy: 0.6495\n",
      "Epoch 304/4000\n",
      " - 3s - loss: 0.8240 - accuracy: 0.6495\n",
      "Epoch 305/4000\n",
      " - 3s - loss: 0.8223 - accuracy: 0.6477\n",
      "Epoch 306/4000\n",
      " - 3s - loss: 0.8231 - accuracy: 0.6491\n",
      "Epoch 307/4000\n",
      " - 3s - loss: 0.8232 - accuracy: 0.6485\n",
      "Epoch 308/4000\n",
      " - 3s - loss: 0.8224 - accuracy: 0.6499\n",
      "Epoch 309/4000\n",
      " - 3s - loss: 0.8230 - accuracy: 0.6494\n",
      "Epoch 310/4000\n",
      " - 3s - loss: 0.8227 - accuracy: 0.6505\n",
      "Epoch 311/4000\n",
      " - 3s - loss: 0.8221 - accuracy: 0.6503\n",
      "Epoch 312/4000\n",
      " - 3s - loss: 0.8216 - accuracy: 0.6504\n",
      "Epoch 313/4000\n",
      " - 3s - loss: 0.8219 - accuracy: 0.6490\n",
      "Epoch 314/4000\n",
      " - 3s - loss: 0.8224 - accuracy: 0.6481\n",
      "Epoch 315/4000\n",
      " - 3s - loss: 0.8216 - accuracy: 0.6502\n",
      "Epoch 316/4000\n",
      " - 3s - loss: 0.8225 - accuracy: 0.6489\n",
      "Epoch 317/4000\n",
      " - 3s - loss: 0.8216 - accuracy: 0.6492\n",
      "Epoch 318/4000\n",
      " - 3s - loss: 0.8211 - accuracy: 0.6503\n",
      "Epoch 319/4000\n",
      " - 3s - loss: 0.8209 - accuracy: 0.6484\n",
      "Epoch 320/4000\n",
      " - 3s - loss: 0.8214 - accuracy: 0.6503\n",
      "Epoch 321/4000\n",
      " - 3s - loss: 0.8210 - accuracy: 0.6489\n",
      "Epoch 322/4000\n",
      " - 3s - loss: 0.8214 - accuracy: 0.6507\n",
      "Epoch 323/4000\n",
      " - 3s - loss: 0.8197 - accuracy: 0.6489\n",
      "Epoch 324/4000\n",
      " - 3s - loss: 0.8212 - accuracy: 0.6493\n",
      "Epoch 325/4000\n",
      " - 3s - loss: 0.8201 - accuracy: 0.6502\n",
      "Epoch 326/4000\n",
      " - 3s - loss: 0.8197 - accuracy: 0.6512\n",
      "Epoch 327/4000\n",
      " - 3s - loss: 0.8201 - accuracy: 0.6489\n",
      "Epoch 328/4000\n",
      " - 3s - loss: 0.8201 - accuracy: 0.6494\n",
      "Epoch 329/4000\n",
      " - 3s - loss: 0.8202 - accuracy: 0.6489\n",
      "Epoch 330/4000\n",
      " - 3s - loss: 0.8203 - accuracy: 0.6473\n",
      "Epoch 331/4000\n",
      " - 3s - loss: 0.8199 - accuracy: 0.6500\n",
      "Epoch 332/4000\n",
      " - 3s - loss: 0.8198 - accuracy: 0.6488\n",
      "Epoch 333/4000\n",
      " - 3s - loss: 0.8201 - accuracy: 0.6494\n",
      "Epoch 334/4000\n",
      " - 3s - loss: 0.8197 - accuracy: 0.6501\n",
      "Epoch 335/4000\n",
      " - 3s - loss: 0.8186 - accuracy: 0.6503\n",
      "Epoch 336/4000\n",
      " - 3s - loss: 0.8192 - accuracy: 0.6512\n",
      "Epoch 337/4000\n",
      " - 3s - loss: 0.8187 - accuracy: 0.6505\n",
      "Epoch 338/4000\n",
      " - 3s - loss: 0.8189 - accuracy: 0.6532\n",
      "Epoch 339/4000\n",
      " - 3s - loss: 0.8186 - accuracy: 0.6493\n",
      "Epoch 340/4000\n",
      " - 3s - loss: 0.8183 - accuracy: 0.6505\n",
      "Epoch 341/4000\n",
      " - 3s - loss: 0.8186 - accuracy: 0.6488\n",
      "Epoch 342/4000\n",
      " - 3s - loss: 0.8178 - accuracy: 0.6502\n",
      "Epoch 343/4000\n",
      " - 3s - loss: 0.8178 - accuracy: 0.6496\n",
      "Epoch 344/4000\n",
      " - 3s - loss: 0.8181 - accuracy: 0.6513\n",
      "Epoch 345/4000\n",
      " - 3s - loss: 0.8179 - accuracy: 0.6493\n",
      "Epoch 346/4000\n",
      " - 3s - loss: 0.8185 - accuracy: 0.6474\n",
      "Epoch 347/4000\n",
      " - 3s - loss: 0.8179 - accuracy: 0.6510\n",
      "Epoch 348/4000\n",
      " - 3s - loss: 0.8175 - accuracy: 0.6493\n",
      "Epoch 349/4000\n",
      " - 3s - loss: 0.8180 - accuracy: 0.6495\n",
      "Epoch 350/4000\n",
      " - 3s - loss: 0.8163 - accuracy: 0.6501\n",
      "Epoch 351/4000\n",
      " - 3s - loss: 0.8175 - accuracy: 0.6490\n",
      "Epoch 352/4000\n",
      " - 3s - loss: 0.8166 - accuracy: 0.6507\n",
      "Epoch 353/4000\n",
      " - 3s - loss: 0.8175 - accuracy: 0.6489\n",
      "Epoch 354/4000\n",
      " - 3s - loss: 0.8170 - accuracy: 0.6526\n",
      "Epoch 355/4000\n",
      " - 3s - loss: 0.8164 - accuracy: 0.6511\n",
      "Epoch 356/4000\n",
      " - 3s - loss: 0.8163 - accuracy: 0.6519\n",
      "Epoch 357/4000\n",
      " - 3s - loss: 0.8173 - accuracy: 0.6512\n",
      "Epoch 358/4000\n",
      " - 3s - loss: 0.8160 - accuracy: 0.6520\n",
      "Epoch 359/4000\n",
      " - 3s - loss: 0.8167 - accuracy: 0.6489\n",
      "Epoch 360/4000\n",
      " - 3s - loss: 0.8157 - accuracy: 0.6524\n",
      "Epoch 361/4000\n",
      " - 3s - loss: 0.8165 - accuracy: 0.6537\n",
      "Epoch 362/4000\n",
      " - 3s - loss: 0.8162 - accuracy: 0.6510\n",
      "Epoch 363/4000\n",
      " - 3s - loss: 0.8161 - accuracy: 0.6501\n",
      "Epoch 364/4000\n",
      " - 3s - loss: 0.8158 - accuracy: 0.6504\n",
      "Epoch 365/4000\n",
      " - 3s - loss: 0.8161 - accuracy: 0.6507\n",
      "Epoch 366/4000\n",
      " - 3s - loss: 0.8151 - accuracy: 0.6512\n",
      "Epoch 367/4000\n",
      " - 3s - loss: 0.8159 - accuracy: 0.6511\n",
      "Epoch 368/4000\n",
      " - 3s - loss: 0.8156 - accuracy: 0.6523\n",
      "Epoch 369/4000\n",
      " - 3s - loss: 0.8149 - accuracy: 0.6509\n",
      "Epoch 370/4000\n",
      " - 3s - loss: 0.8156 - accuracy: 0.6510\n",
      "Epoch 371/4000\n",
      " - 3s - loss: 0.8149 - accuracy: 0.6516\n",
      "Epoch 372/4000\n",
      " - 3s - loss: 0.8160 - accuracy: 0.6504\n",
      "Epoch 373/4000\n",
      " - 3s - loss: 0.8143 - accuracy: 0.6514\n",
      "Epoch 374/4000\n",
      " - 3s - loss: 0.8147 - accuracy: 0.6513\n",
      "Epoch 375/4000\n",
      " - 3s - loss: 0.8148 - accuracy: 0.6513\n",
      "Epoch 376/4000\n",
      " - 3s - loss: 0.8149 - accuracy: 0.6506\n",
      "Epoch 377/4000\n",
      " - 3s - loss: 0.8145 - accuracy: 0.6499\n",
      "Epoch 378/4000\n",
      " - 3s - loss: 0.8150 - accuracy: 0.6502\n",
      "Epoch 379/4000\n",
      " - 3s - loss: 0.8137 - accuracy: 0.6502\n",
      "Epoch 380/4000\n",
      " - 3s - loss: 0.8143 - accuracy: 0.6528\n",
      "Epoch 381/4000\n",
      " - 3s - loss: 0.8141 - accuracy: 0.6522\n",
      "Epoch 382/4000\n",
      " - 3s - loss: 0.8141 - accuracy: 0.6507\n",
      "Epoch 383/4000\n",
      " - 3s - loss: 0.8145 - accuracy: 0.6520\n",
      "Epoch 384/4000\n",
      " - 3s - loss: 0.8135 - accuracy: 0.6522\n",
      "Epoch 385/4000\n",
      " - 3s - loss: 0.8127 - accuracy: 0.6534\n",
      "Epoch 386/4000\n",
      " - 3s - loss: 0.8135 - accuracy: 0.6511\n",
      "Epoch 387/4000\n",
      " - 3s - loss: 0.8137 - accuracy: 0.6481\n",
      "Epoch 388/4000\n",
      " - 3s - loss: 0.8134 - accuracy: 0.6538\n",
      "Epoch 389/4000\n",
      " - 3s - loss: 0.8133 - accuracy: 0.6518\n",
      "Epoch 390/4000\n",
      " - 3s - loss: 0.8132 - accuracy: 0.6514\n",
      "Epoch 391/4000\n",
      " - 3s - loss: 0.8133 - accuracy: 0.6496\n",
      "Epoch 392/4000\n",
      " - 3s - loss: 0.8133 - accuracy: 0.6508\n",
      "Epoch 393/4000\n",
      " - 3s - loss: 0.8120 - accuracy: 0.6525\n",
      "Epoch 394/4000\n",
      " - 3s - loss: 0.8131 - accuracy: 0.6513\n",
      "Epoch 395/4000\n",
      " - 3s - loss: 0.8129 - accuracy: 0.6533\n",
      "Epoch 396/4000\n",
      " - 3s - loss: 0.8127 - accuracy: 0.6509\n",
      "Epoch 397/4000\n",
      " - 3s - loss: 0.8118 - accuracy: 0.6508\n",
      "Epoch 398/4000\n",
      " - 3s - loss: 0.8122 - accuracy: 0.6520\n",
      "Epoch 399/4000\n",
      " - 3s - loss: 0.8125 - accuracy: 0.6532\n",
      "Epoch 400/4000\n",
      " - 3s - loss: 0.8125 - accuracy: 0.6502\n",
      "Epoch 401/4000\n",
      " - 3s - loss: 0.8123 - accuracy: 0.6524\n",
      "Epoch 402/4000\n",
      " - 3s - loss: 0.8113 - accuracy: 0.6518\n",
      "Epoch 403/4000\n",
      " - 3s - loss: 0.8122 - accuracy: 0.6517\n",
      "Epoch 404/4000\n",
      " - 3s - loss: 0.8118 - accuracy: 0.6523\n",
      "Epoch 405/4000\n",
      " - 3s - loss: 0.8115 - accuracy: 0.6507\n",
      "Epoch 406/4000\n",
      " - 3s - loss: 0.8115 - accuracy: 0.6517\n",
      "Epoch 407/4000\n",
      " - 3s - loss: 0.8118 - accuracy: 0.6514\n",
      "Epoch 408/4000\n",
      " - 3s - loss: 0.8119 - accuracy: 0.6524\n",
      "Epoch 409/4000\n",
      " - 3s - loss: 0.8110 - accuracy: 0.6534\n",
      "Epoch 410/4000\n",
      " - 4s - loss: 0.8110 - accuracy: 0.6520\n",
      "Epoch 411/4000\n",
      " - 5s - loss: 0.8107 - accuracy: 0.6555\n",
      "Epoch 412/4000\n",
      " - 5s - loss: 0.8112 - accuracy: 0.6502\n",
      "Epoch 413/4000\n",
      " - 5s - loss: 0.8116 - accuracy: 0.6489\n",
      "Epoch 414/4000\n",
      " - 5s - loss: 0.8109 - accuracy: 0.6511\n",
      "Epoch 415/4000\n",
      " - 5s - loss: 0.8112 - accuracy: 0.6507\n",
      "Epoch 416/4000\n",
      " - 5s - loss: 0.8119 - accuracy: 0.6520\n",
      "Epoch 417/4000\n",
      " - 5s - loss: 0.8104 - accuracy: 0.6511\n",
      "Epoch 418/4000\n",
      " - 5s - loss: 0.8102 - accuracy: 0.6522\n",
      "Epoch 419/4000\n",
      " - 5s - loss: 0.8099 - accuracy: 0.6523\n",
      "Epoch 420/4000\n",
      " - 5s - loss: 0.8105 - accuracy: 0.6524\n",
      "Epoch 421/4000\n",
      " - 5s - loss: 0.8104 - accuracy: 0.6532\n",
      "Epoch 422/4000\n",
      " - 5s - loss: 0.8106 - accuracy: 0.6527\n",
      "Epoch 423/4000\n",
      " - 5s - loss: 0.8106 - accuracy: 0.6515\n",
      "Epoch 424/4000\n",
      " - 5s - loss: 0.8097 - accuracy: 0.6526\n",
      "Epoch 425/4000\n",
      " - 5s - loss: 0.8106 - accuracy: 0.6522\n",
      "Epoch 426/4000\n",
      " - 5s - loss: 0.8105 - accuracy: 0.6515\n",
      "Epoch 427/4000\n",
      " - 5s - loss: 0.8094 - accuracy: 0.6534\n",
      "Epoch 428/4000\n",
      " - 5s - loss: 0.8091 - accuracy: 0.6511\n",
      "Epoch 429/4000\n",
      " - 5s - loss: 0.8098 - accuracy: 0.6536\n",
      "Epoch 430/4000\n",
      " - 5s - loss: 0.8093 - accuracy: 0.6529\n",
      "Epoch 431/4000\n",
      " - 5s - loss: 0.8102 - accuracy: 0.6507\n",
      "Epoch 432/4000\n",
      " - 5s - loss: 0.8091 - accuracy: 0.6516\n",
      "Epoch 433/4000\n",
      " - 5s - loss: 0.8094 - accuracy: 0.6519\n",
      "Epoch 434/4000\n",
      " - 5s - loss: 0.8090 - accuracy: 0.6524\n",
      "Epoch 435/4000\n",
      " - 5s - loss: 0.8092 - accuracy: 0.6511\n",
      "Epoch 436/4000\n",
      " - 5s - loss: 0.8094 - accuracy: 0.6497\n",
      "Epoch 437/4000\n",
      " - 5s - loss: 0.8088 - accuracy: 0.6531\n",
      "Epoch 438/4000\n",
      " - 5s - loss: 0.8088 - accuracy: 0.6519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 439/4000\n",
      " - 5s - loss: 0.8091 - accuracy: 0.6525\n",
      "Epoch 440/4000\n",
      " - 5s - loss: 0.8087 - accuracy: 0.6522\n",
      "Epoch 441/4000\n",
      " - 5s - loss: 0.8092 - accuracy: 0.6497\n",
      "Epoch 442/4000\n",
      " - 5s - loss: 0.8088 - accuracy: 0.6509\n",
      "Epoch 443/4000\n",
      " - 5s - loss: 0.8078 - accuracy: 0.6522\n",
      "Epoch 444/4000\n",
      " - 5s - loss: 0.8081 - accuracy: 0.6524\n",
      "Epoch 445/4000\n",
      " - 5s - loss: 0.8088 - accuracy: 0.6517\n",
      "Epoch 446/4000\n",
      " - 5s - loss: 0.8084 - accuracy: 0.6515\n",
      "Epoch 447/4000\n",
      " - 5s - loss: 0.8077 - accuracy: 0.6533\n",
      "Epoch 448/4000\n",
      " - 5s - loss: 0.8084 - accuracy: 0.6516\n",
      "Epoch 449/4000\n",
      " - 5s - loss: 0.8089 - accuracy: 0.6525\n",
      "Epoch 450/4000\n",
      " - 5s - loss: 0.8078 - accuracy: 0.6531\n",
      "Epoch 451/4000\n",
      " - 5s - loss: 0.8077 - accuracy: 0.6523\n",
      "Epoch 452/4000\n",
      " - 5s - loss: 0.8079 - accuracy: 0.6517\n",
      "Epoch 453/4000\n",
      " - 5s - loss: 0.8080 - accuracy: 0.6530\n",
      "Epoch 454/4000\n",
      " - 5s - loss: 0.8080 - accuracy: 0.6525\n",
      "Epoch 455/4000\n",
      " - 5s - loss: 0.8081 - accuracy: 0.6518\n",
      "Epoch 456/4000\n",
      " - 5s - loss: 0.8079 - accuracy: 0.6527\n",
      "Epoch 457/4000\n",
      " - 5s - loss: 0.8078 - accuracy: 0.6511\n",
      "Epoch 458/4000\n",
      " - 5s - loss: 0.8068 - accuracy: 0.6539\n",
      "Epoch 459/4000\n",
      " - 5s - loss: 0.8070 - accuracy: 0.6554\n",
      "Epoch 460/4000\n",
      " - 5s - loss: 0.8080 - accuracy: 0.6523\n",
      "Epoch 461/4000\n",
      " - 5s - loss: 0.8071 - accuracy: 0.6525\n",
      "Epoch 462/4000\n",
      " - 5s - loss: 0.8073 - accuracy: 0.6525\n",
      "Epoch 463/4000\n",
      " - 5s - loss: 0.8072 - accuracy: 0.6527\n",
      "Epoch 464/4000\n",
      " - 5s - loss: 0.8065 - accuracy: 0.6533\n",
      "Epoch 465/4000\n",
      " - 5s - loss: 0.8065 - accuracy: 0.6537\n",
      "Epoch 466/4000\n",
      " - 5s - loss: 0.8061 - accuracy: 0.6533\n",
      "Epoch 467/4000\n",
      " - 5s - loss: 0.8068 - accuracy: 0.6524\n",
      "Epoch 468/4000\n",
      " - 5s - loss: 0.8063 - accuracy: 0.6535\n",
      "Epoch 469/4000\n",
      " - 5s - loss: 0.8064 - accuracy: 0.6528\n",
      "Epoch 470/4000\n",
      " - 5s - loss: 0.8070 - accuracy: 0.6508\n",
      "Epoch 471/4000\n",
      " - 5s - loss: 0.8057 - accuracy: 0.6536\n",
      "Epoch 472/4000\n",
      " - 5s - loss: 0.8057 - accuracy: 0.6537\n",
      "Epoch 473/4000\n",
      " - 5s - loss: 0.8066 - accuracy: 0.6525\n",
      "Epoch 474/4000\n",
      " - 5s - loss: 0.8063 - accuracy: 0.6545\n",
      "Epoch 475/4000\n",
      " - 5s - loss: 0.8061 - accuracy: 0.6526\n",
      "Epoch 476/4000\n",
      " - 5s - loss: 0.8070 - accuracy: 0.6542\n",
      "Epoch 477/4000\n",
      " - 5s - loss: 0.8056 - accuracy: 0.6529\n",
      "Epoch 478/4000\n",
      " - 5s - loss: 0.8058 - accuracy: 0.6523\n",
      "Epoch 479/4000\n",
      " - 5s - loss: 0.8056 - accuracy: 0.6520\n",
      "Epoch 480/4000\n",
      " - 5s - loss: 0.8051 - accuracy: 0.6525\n",
      "Epoch 481/4000\n",
      " - 5s - loss: 0.8061 - accuracy: 0.6543\n",
      "Epoch 482/4000\n",
      " - 5s - loss: 0.8051 - accuracy: 0.6527\n",
      "Epoch 483/4000\n",
      " - 5s - loss: 0.8056 - accuracy: 0.6532\n",
      "Epoch 484/4000\n",
      " - 5s - loss: 0.8058 - accuracy: 0.6537\n",
      "Epoch 485/4000\n",
      " - 5s - loss: 0.8050 - accuracy: 0.6535\n",
      "Epoch 486/4000\n",
      " - 5s - loss: 0.8051 - accuracy: 0.6533\n",
      "Epoch 487/4000\n",
      " - 5s - loss: 0.8050 - accuracy: 0.6538\n",
      "Epoch 488/4000\n",
      " - 5s - loss: 0.8052 - accuracy: 0.6530\n",
      "Epoch 489/4000\n",
      " - 5s - loss: 0.8046 - accuracy: 0.6546\n",
      "Epoch 490/4000\n",
      " - 5s - loss: 0.8045 - accuracy: 0.6540\n",
      "Epoch 491/4000\n",
      " - 5s - loss: 0.8053 - accuracy: 0.6536\n",
      "Epoch 492/4000\n",
      " - 5s - loss: 0.8046 - accuracy: 0.6534\n",
      "Epoch 493/4000\n",
      " - 5s - loss: 0.8045 - accuracy: 0.6536\n",
      "Epoch 494/4000\n",
      " - 5s - loss: 0.8041 - accuracy: 0.6542\n",
      "Epoch 495/4000\n",
      " - 5s - loss: 0.8053 - accuracy: 0.6540\n",
      "Epoch 496/4000\n",
      " - 5s - loss: 0.8045 - accuracy: 0.6528\n",
      "Epoch 497/4000\n",
      " - 5s - loss: 0.8041 - accuracy: 0.6546\n",
      "Epoch 498/4000\n",
      " - 5s - loss: 0.8048 - accuracy: 0.6550\n",
      "Epoch 499/4000\n",
      " - 5s - loss: 0.8040 - accuracy: 0.6546\n",
      "Epoch 500/4000\n",
      " - 5s - loss: 0.8048 - accuracy: 0.6543\n",
      "Epoch 501/4000\n",
      " - 5s - loss: 0.8036 - accuracy: 0.6538\n",
      "Epoch 502/4000\n",
      " - 5s - loss: 0.8040 - accuracy: 0.6547\n",
      "Epoch 503/4000\n",
      " - 5s - loss: 0.8035 - accuracy: 0.6549\n",
      "Epoch 504/4000\n",
      " - 5s - loss: 0.8034 - accuracy: 0.6553\n",
      "Epoch 505/4000\n",
      " - 5s - loss: 0.8038 - accuracy: 0.6535\n",
      "Epoch 506/4000\n",
      " - 5s - loss: 0.8043 - accuracy: 0.6538\n",
      "Epoch 507/4000\n",
      " - 5s - loss: 0.8037 - accuracy: 0.6552\n",
      "Epoch 508/4000\n",
      " - 5s - loss: 0.8035 - accuracy: 0.6541\n",
      "Epoch 509/4000\n",
      " - 5s - loss: 0.8042 - accuracy: 0.6531\n",
      "Epoch 510/4000\n",
      " - 5s - loss: 0.8037 - accuracy: 0.6550\n",
      "Epoch 511/4000\n",
      " - 5s - loss: 0.8027 - accuracy: 0.6557\n",
      "Epoch 512/4000\n",
      " - 5s - loss: 0.8027 - accuracy: 0.6537\n",
      "Epoch 513/4000\n",
      " - 5s - loss: 0.8029 - accuracy: 0.6526\n",
      "Epoch 514/4000\n",
      " - 5s - loss: 0.8036 - accuracy: 0.6538\n",
      "Epoch 515/4000\n",
      " - 5s - loss: 0.8033 - accuracy: 0.6538\n",
      "Epoch 516/4000\n",
      " - 5s - loss: 0.8038 - accuracy: 0.6516\n",
      "Epoch 517/4000\n",
      " - 5s - loss: 0.8036 - accuracy: 0.6533\n",
      "Epoch 518/4000\n",
      " - 5s - loss: 0.8030 - accuracy: 0.6538\n",
      "Epoch 519/4000\n",
      " - 5s - loss: 0.8024 - accuracy: 0.6545\n",
      "Epoch 520/4000\n",
      " - 5s - loss: 0.8029 - accuracy: 0.6541\n",
      "Epoch 521/4000\n",
      " - 5s - loss: 0.8023 - accuracy: 0.6546\n",
      "Epoch 522/4000\n",
      " - 5s - loss: 0.8024 - accuracy: 0.6543\n",
      "Epoch 523/4000\n",
      " - 5s - loss: 0.8029 - accuracy: 0.6514\n",
      "Epoch 524/4000\n",
      " - 5s - loss: 0.8027 - accuracy: 0.6543\n",
      "Epoch 525/4000\n",
      " - 5s - loss: 0.8022 - accuracy: 0.6531\n",
      "Epoch 526/4000\n",
      " - 5s - loss: 0.8028 - accuracy: 0.6528\n",
      "Epoch 527/4000\n",
      " - 5s - loss: 0.8027 - accuracy: 0.6526\n",
      "Epoch 528/4000\n",
      " - 5s - loss: 0.8020 - accuracy: 0.6546\n",
      "Epoch 529/4000\n",
      " - 5s - loss: 0.8033 - accuracy: 0.6532\n",
      "Epoch 530/4000\n",
      " - 5s - loss: 0.8028 - accuracy: 0.6545\n",
      "Epoch 531/4000\n",
      " - 5s - loss: 0.8021 - accuracy: 0.6544\n",
      "Epoch 532/4000\n",
      " - 5s - loss: 0.8027 - accuracy: 0.6529\n",
      "Epoch 533/4000\n",
      " - 5s - loss: 0.8017 - accuracy: 0.6526\n",
      "Epoch 534/4000\n",
      " - 5s - loss: 0.8026 - accuracy: 0.6523\n",
      "Epoch 535/4000\n",
      " - 5s - loss: 0.8014 - accuracy: 0.6551\n",
      "Epoch 536/4000\n",
      " - 5s - loss: 0.8018 - accuracy: 0.6538\n",
      "Epoch 537/4000\n",
      " - 5s - loss: 0.8019 - accuracy: 0.6536\n",
      "Epoch 538/4000\n",
      " - 5s - loss: 0.8017 - accuracy: 0.6536\n",
      "Epoch 539/4000\n",
      " - 5s - loss: 0.8018 - accuracy: 0.6522\n",
      "Epoch 540/4000\n",
      " - 5s - loss: 0.8018 - accuracy: 0.6530\n",
      "Epoch 541/4000\n",
      " - 5s - loss: 0.8017 - accuracy: 0.6545\n",
      "Epoch 542/4000\n",
      " - 5s - loss: 0.8013 - accuracy: 0.6559\n",
      "Epoch 543/4000\n",
      " - 5s - loss: 0.8014 - accuracy: 0.6537\n",
      "Epoch 544/4000\n",
      " - 5s - loss: 0.8021 - accuracy: 0.6539\n",
      "Epoch 545/4000\n",
      " - 6s - loss: 0.8016 - accuracy: 0.6546\n",
      "Epoch 546/4000\n",
      " - 5s - loss: 0.8010 - accuracy: 0.6532\n",
      "Epoch 547/4000\n",
      " - 5s - loss: 0.8008 - accuracy: 0.6561\n",
      "Epoch 548/4000\n",
      " - 5s - loss: 0.8014 - accuracy: 0.6531\n",
      "Epoch 549/4000\n",
      " - 5s - loss: 0.8009 - accuracy: 0.6545\n",
      "Epoch 550/4000\n",
      " - 5s - loss: 0.8015 - accuracy: 0.6520\n",
      "Epoch 551/4000\n",
      " - 5s - loss: 0.8013 - accuracy: 0.6547\n",
      "Epoch 552/4000\n",
      " - 5s - loss: 0.8007 - accuracy: 0.6524\n",
      "Epoch 553/4000\n",
      " - 5s - loss: 0.8006 - accuracy: 0.6540\n",
      "Epoch 554/4000\n",
      " - 5s - loss: 0.8006 - accuracy: 0.6526\n",
      "Epoch 555/4000\n",
      " - 5s - loss: 0.8003 - accuracy: 0.6556\n",
      "Epoch 556/4000\n",
      " - 6s - loss: 0.8011 - accuracy: 0.6554\n",
      "Epoch 557/4000\n",
      " - 5s - loss: 0.8011 - accuracy: 0.6541\n",
      "Epoch 558/4000\n",
      " - 6s - loss: 0.8007 - accuracy: 0.6553\n",
      "Epoch 559/4000\n",
      " - 6s - loss: 0.8010 - accuracy: 0.6537\n",
      "Epoch 560/4000\n",
      " - 5s - loss: 0.8000 - accuracy: 0.6544\n",
      "Epoch 561/4000\n",
      " - 6s - loss: 0.8005 - accuracy: 0.6536\n",
      "Epoch 562/4000\n",
      " - 5s - loss: 0.8006 - accuracy: 0.6533\n",
      "Epoch 563/4000\n",
      " - 5s - loss: 0.8008 - accuracy: 0.6565\n",
      "Epoch 564/4000\n",
      " - 5s - loss: 0.8003 - accuracy: 0.6539\n",
      "Epoch 565/4000\n",
      " - 5s - loss: 0.8000 - accuracy: 0.6535\n",
      "Epoch 566/4000\n",
      " - 5s - loss: 0.8003 - accuracy: 0.6533\n",
      "Epoch 567/4000\n",
      " - 5s - loss: 0.8001 - accuracy: 0.6529\n",
      "Epoch 568/4000\n",
      " - 5s - loss: 0.8000 - accuracy: 0.6516\n",
      "Epoch 569/4000\n",
      " - 5s - loss: 0.8002 - accuracy: 0.6557\n",
      "Epoch 570/4000\n",
      " - 5s - loss: 0.8001 - accuracy: 0.6528\n",
      "Epoch 00570: early stopping\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_17 (LSTM)               (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/4000\n",
      " - 6s - loss: 2.7630 - accuracy: 0.2578\n",
      "Epoch 2/4000\n",
      " - 3s - loss: 2.2472 - accuracy: 0.3596\n",
      "Epoch 3/4000\n",
      " - 5s - loss: 2.0899 - accuracy: 0.3973\n",
      "Epoch 4/4000\n",
      " - 5s - loss: 1.9943 - accuracy: 0.4163\n",
      "Epoch 5/4000\n",
      " - 5s - loss: 1.9208 - accuracy: 0.4297\n",
      "Epoch 6/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 1.8538 - accuracy: 0.4475\n",
      "Epoch 7/4000\n",
      " - 5s - loss: 1.7984 - accuracy: 0.4610\n",
      "Epoch 8/4000\n",
      " - 5s - loss: 1.7440 - accuracy: 0.4723\n",
      "Epoch 9/4000\n",
      " - 5s - loss: 1.6972 - accuracy: 0.4848\n",
      "Epoch 10/4000\n",
      " - 5s - loss: 1.6532 - accuracy: 0.4928\n",
      "Epoch 11/4000\n",
      " - 5s - loss: 1.6111 - accuracy: 0.5048\n",
      "Epoch 12/4000\n",
      " - 5s - loss: 1.5702 - accuracy: 0.5142\n",
      "Epoch 13/4000\n",
      " - 5s - loss: 1.5319 - accuracy: 0.5232\n",
      "Epoch 14/4000\n",
      " - 5s - loss: 1.4927 - accuracy: 0.5330\n",
      "Epoch 15/4000\n",
      " - 5s - loss: 1.4592 - accuracy: 0.5407\n",
      "Epoch 16/4000\n",
      " - 5s - loss: 1.4254 - accuracy: 0.5508\n",
      "Epoch 17/4000\n",
      " - 5s - loss: 1.3928 - accuracy: 0.5560\n",
      "Epoch 18/4000\n",
      " - 5s - loss: 1.3609 - accuracy: 0.5642\n",
      "Epoch 19/4000\n",
      " - 5s - loss: 1.3317 - accuracy: 0.5704\n",
      "Epoch 20/4000\n",
      " - 5s - loss: 1.3052 - accuracy: 0.5770\n",
      "Epoch 21/4000\n",
      " - 5s - loss: 1.2786 - accuracy: 0.5806\n",
      "Epoch 22/4000\n",
      " - 5s - loss: 1.2553 - accuracy: 0.5855\n",
      "Epoch 23/4000\n",
      " - 5s - loss: 1.2311 - accuracy: 0.5926\n",
      "Epoch 24/4000\n",
      " - 5s - loss: 1.2101 - accuracy: 0.5975\n",
      "Epoch 25/4000\n",
      " - 5s - loss: 1.1897 - accuracy: 0.6008\n",
      "Epoch 26/4000\n",
      " - 5s - loss: 1.1710 - accuracy: 0.6057\n",
      "Epoch 27/4000\n",
      " - 5s - loss: 1.1550 - accuracy: 0.6100\n",
      "Epoch 28/4000\n",
      " - 5s - loss: 1.1362 - accuracy: 0.6132\n",
      "Epoch 29/4000\n",
      " - 5s - loss: 1.1211 - accuracy: 0.6180\n",
      "Epoch 30/4000\n",
      " - 5s - loss: 1.1092 - accuracy: 0.6168\n",
      "Epoch 31/4000\n",
      " - 5s - loss: 1.0954 - accuracy: 0.6219\n",
      "Epoch 32/4000\n",
      " - 5s - loss: 1.0833 - accuracy: 0.6223\n",
      "Epoch 33/4000\n",
      " - 5s - loss: 1.0700 - accuracy: 0.6277\n",
      "Epoch 34/4000\n",
      " - 5s - loss: 1.0608 - accuracy: 0.6261\n",
      "Epoch 35/4000\n",
      " - 5s - loss: 1.0507 - accuracy: 0.6280\n",
      "Epoch 36/4000\n",
      " - 5s - loss: 1.0415 - accuracy: 0.6316\n",
      "Epoch 37/4000\n",
      " - 5s - loss: 1.0328 - accuracy: 0.6334\n",
      "Epoch 38/4000\n",
      " - 5s - loss: 1.0249 - accuracy: 0.6337\n",
      "Epoch 39/4000\n",
      " - 5s - loss: 1.0183 - accuracy: 0.6334\n",
      "Epoch 40/4000\n",
      " - 5s - loss: 1.0100 - accuracy: 0.6374\n",
      "Epoch 41/4000\n",
      " - 5s - loss: 1.0038 - accuracy: 0.6367\n",
      "Epoch 42/4000\n",
      " - 5s - loss: 1.0004 - accuracy: 0.6348\n",
      "Epoch 43/4000\n",
      " - 5s - loss: 0.9924 - accuracy: 0.6368\n",
      "Epoch 44/4000\n",
      " - 5s - loss: 0.9882 - accuracy: 0.6375\n",
      "Epoch 45/4000\n",
      " - 5s - loss: 0.9846 - accuracy: 0.6356\n",
      "Epoch 46/4000\n",
      " - 5s - loss: 0.9770 - accuracy: 0.6366\n",
      "Epoch 47/4000\n",
      " - 5s - loss: 0.9745 - accuracy: 0.6392\n",
      "Epoch 48/4000\n",
      " - 5s - loss: 0.9715 - accuracy: 0.6374\n",
      "Epoch 49/4000\n",
      " - 5s - loss: 0.9655 - accuracy: 0.6407\n",
      "Epoch 50/4000\n",
      " - 5s - loss: 0.9625 - accuracy: 0.6374\n",
      "Epoch 51/4000\n",
      " - 5s - loss: 0.9593 - accuracy: 0.6365\n",
      "Epoch 52/4000\n",
      " - 5s - loss: 0.9549 - accuracy: 0.6380\n",
      "Epoch 53/4000\n",
      " - 5s - loss: 0.9520 - accuracy: 0.6390\n",
      "Epoch 54/4000\n",
      " - 5s - loss: 0.9492 - accuracy: 0.6399\n",
      "Epoch 55/4000\n",
      " - 4s - loss: 0.9459 - accuracy: 0.6414\n",
      "Epoch 56/4000\n",
      " - 3s - loss: 0.9452 - accuracy: 0.6399\n",
      "Epoch 57/4000\n",
      " - 3s - loss: 0.9396 - accuracy: 0.6415\n",
      "Epoch 58/4000\n",
      " - 3s - loss: 0.9385 - accuracy: 0.6403\n",
      "Epoch 59/4000\n",
      " - 3s - loss: 0.9349 - accuracy: 0.6420\n",
      "Epoch 60/4000\n",
      " - 3s - loss: 0.9341 - accuracy: 0.6396\n",
      "Epoch 61/4000\n",
      " - 3s - loss: 0.9346 - accuracy: 0.6395\n",
      "Epoch 62/4000\n",
      " - 3s - loss: 0.9291 - accuracy: 0.6397\n",
      "Epoch 63/4000\n",
      " - 3s - loss: 0.9298 - accuracy: 0.6391\n",
      "Epoch 64/4000\n",
      " - 3s - loss: 0.9250 - accuracy: 0.6412\n",
      "Epoch 65/4000\n",
      " - 3s - loss: 0.9256 - accuracy: 0.6381\n",
      "Epoch 66/4000\n",
      " - 3s - loss: 0.9227 - accuracy: 0.6401\n",
      "Epoch 67/4000\n",
      " - 3s - loss: 0.9223 - accuracy: 0.6384\n",
      "Epoch 68/4000\n",
      " - 3s - loss: 0.9193 - accuracy: 0.6404\n",
      "Epoch 69/4000\n",
      " - 3s - loss: 0.9176 - accuracy: 0.6436\n",
      "Epoch 70/4000\n",
      " - 3s - loss: 0.9171 - accuracy: 0.6383\n",
      "Epoch 71/4000\n",
      " - 3s - loss: 0.9141 - accuracy: 0.6410\n",
      "Epoch 72/4000\n",
      " - 3s - loss: 0.9138 - accuracy: 0.6427\n",
      "Epoch 73/4000\n",
      " - 3s - loss: 0.9137 - accuracy: 0.6385\n",
      "Epoch 74/4000\n",
      " - 3s - loss: 0.9120 - accuracy: 0.6389\n",
      "Epoch 75/4000\n",
      " - 3s - loss: 0.9082 - accuracy: 0.6387\n",
      "Epoch 76/4000\n",
      " - 3s - loss: 0.9080 - accuracy: 0.6421\n",
      "Epoch 77/4000\n",
      " - 3s - loss: 0.9076 - accuracy: 0.6380\n",
      "Epoch 78/4000\n",
      " - 3s - loss: 0.9059 - accuracy: 0.6391\n",
      "Epoch 79/4000\n",
      " - 3s - loss: 0.9064 - accuracy: 0.6378\n",
      "Epoch 80/4000\n",
      " - 3s - loss: 0.9038 - accuracy: 0.6396\n",
      "Epoch 81/4000\n",
      " - 3s - loss: 0.9026 - accuracy: 0.6411\n",
      "Epoch 82/4000\n",
      " - 3s - loss: 0.9003 - accuracy: 0.6428\n",
      "Epoch 83/4000\n",
      " - 3s - loss: 0.9001 - accuracy: 0.6417\n",
      "Epoch 84/4000\n",
      " - 3s - loss: 0.8992 - accuracy: 0.6409\n",
      "Epoch 85/4000\n",
      " - 3s - loss: 0.8998 - accuracy: 0.6397\n",
      "Epoch 86/4000\n",
      " - 3s - loss: 0.8963 - accuracy: 0.6425\n",
      "Epoch 87/4000\n",
      " - 3s - loss: 0.8960 - accuracy: 0.6396\n",
      "Epoch 88/4000\n",
      " - 3s - loss: 0.8958 - accuracy: 0.6381\n",
      "Epoch 89/4000\n",
      " - 3s - loss: 0.8935 - accuracy: 0.6420\n",
      "Epoch 90/4000\n",
      " - 3s - loss: 0.8943 - accuracy: 0.6420\n",
      "Epoch 91/4000\n",
      " - 3s - loss: 0.8953 - accuracy: 0.6380\n",
      "Epoch 92/4000\n",
      " - 3s - loss: 0.8925 - accuracy: 0.6392\n",
      "Epoch 93/4000\n",
      " - 3s - loss: 0.8912 - accuracy: 0.6425\n",
      "Epoch 94/4000\n",
      " - 3s - loss: 0.8908 - accuracy: 0.6392\n",
      "Epoch 95/4000\n",
      " - 3s - loss: 0.8897 - accuracy: 0.6412\n",
      "Epoch 96/4000\n",
      " - 3s - loss: 0.8899 - accuracy: 0.6394\n",
      "Epoch 97/4000\n",
      " - 3s - loss: 0.8879 - accuracy: 0.6397\n",
      "Epoch 98/4000\n",
      " - 3s - loss: 0.8878 - accuracy: 0.6422\n",
      "Epoch 99/4000\n",
      " - 3s - loss: 0.8863 - accuracy: 0.6410\n",
      "Epoch 100/4000\n",
      " - 3s - loss: 0.8854 - accuracy: 0.6400\n",
      "Epoch 101/4000\n",
      " - 3s - loss: 0.8846 - accuracy: 0.6444\n",
      "Epoch 102/4000\n",
      " - 3s - loss: 0.8840 - accuracy: 0.6416\n",
      "Epoch 103/4000\n",
      " - 3s - loss: 0.8852 - accuracy: 0.6421\n",
      "Epoch 104/4000\n",
      " - 3s - loss: 0.8826 - accuracy: 0.6421\n",
      "Epoch 105/4000\n",
      " - 3s - loss: 0.8826 - accuracy: 0.6409\n",
      "Epoch 106/4000\n",
      " - 3s - loss: 0.8811 - accuracy: 0.6394\n",
      "Epoch 107/4000\n",
      " - 3s - loss: 0.8796 - accuracy: 0.6412\n",
      "Epoch 108/4000\n",
      " - 3s - loss: 0.8803 - accuracy: 0.6415\n",
      "Epoch 109/4000\n",
      " - 3s - loss: 0.8784 - accuracy: 0.6410\n",
      "Epoch 110/4000\n",
      " - 3s - loss: 0.8780 - accuracy: 0.6439\n",
      "Epoch 111/4000\n",
      " - 5s - loss: 0.8792 - accuracy: 0.6412\n",
      "Epoch 112/4000\n",
      " - 5s - loss: 0.8776 - accuracy: 0.6410\n",
      "Epoch 113/4000\n",
      " - 5s - loss: 0.8766 - accuracy: 0.6404\n",
      "Epoch 114/4000\n",
      " - 5s - loss: 0.8769 - accuracy: 0.6419\n",
      "Epoch 115/4000\n",
      " - 5s - loss: 0.8764 - accuracy: 0.6384\n",
      "Epoch 116/4000\n",
      " - 5s - loss: 0.8748 - accuracy: 0.6417\n",
      "Epoch 117/4000\n",
      " - 5s - loss: 0.8735 - accuracy: 0.6420\n",
      "Epoch 118/4000\n",
      " - 5s - loss: 0.8733 - accuracy: 0.6425\n",
      "Epoch 119/4000\n",
      " - 5s - loss: 0.8738 - accuracy: 0.6412\n",
      "Epoch 120/4000\n",
      " - 5s - loss: 0.8727 - accuracy: 0.6409\n",
      "Epoch 121/4000\n",
      " - 5s - loss: 0.8730 - accuracy: 0.6404\n",
      "Epoch 122/4000\n",
      " - 5s - loss: 0.8721 - accuracy: 0.6401\n",
      "Epoch 123/4000\n",
      " - 5s - loss: 0.8706 - accuracy: 0.6414\n",
      "Epoch 124/4000\n",
      " - 5s - loss: 0.8709 - accuracy: 0.6430\n",
      "Epoch 125/4000\n",
      " - 5s - loss: 0.8709 - accuracy: 0.6411\n",
      "Epoch 126/4000\n",
      " - 5s - loss: 0.8699 - accuracy: 0.6435\n",
      "Epoch 127/4000\n",
      " - 5s - loss: 0.8679 - accuracy: 0.6429\n",
      "Epoch 128/4000\n",
      " - 5s - loss: 0.8703 - accuracy: 0.6397\n",
      "Epoch 129/4000\n",
      " - 5s - loss: 0.8663 - accuracy: 0.6445\n",
      "Epoch 130/4000\n",
      " - 5s - loss: 0.8683 - accuracy: 0.6406\n",
      "Epoch 131/4000\n",
      " - 5s - loss: 0.8657 - accuracy: 0.6445\n",
      "Epoch 132/4000\n",
      " - 5s - loss: 0.8671 - accuracy: 0.6404\n",
      "Epoch 133/4000\n",
      " - 5s - loss: 0.8660 - accuracy: 0.6422\n",
      "Epoch 134/4000\n",
      " - 5s - loss: 0.8656 - accuracy: 0.6434\n",
      "Epoch 135/4000\n",
      " - 5s - loss: 0.8651 - accuracy: 0.6437\n",
      "Epoch 136/4000\n",
      " - 5s - loss: 0.8641 - accuracy: 0.6430\n",
      "Epoch 137/4000\n",
      " - 5s - loss: 0.8652 - accuracy: 0.6411\n",
      "Epoch 138/4000\n",
      " - 5s - loss: 0.8637 - accuracy: 0.6413\n",
      "Epoch 139/4000\n",
      " - 5s - loss: 0.8625 - accuracy: 0.6432\n",
      "Epoch 140/4000\n",
      " - 5s - loss: 0.8621 - accuracy: 0.6430\n",
      "Epoch 141/4000\n",
      " - 5s - loss: 0.8639 - accuracy: 0.6440\n",
      "Epoch 142/4000\n",
      " - 5s - loss: 0.8610 - accuracy: 0.6437\n",
      "Epoch 143/4000\n",
      " - 5s - loss: 0.8612 - accuracy: 0.6439\n",
      "Epoch 144/4000\n",
      " - 5s - loss: 0.8631 - accuracy: 0.6436\n",
      "Epoch 145/4000\n",
      " - 5s - loss: 0.8603 - accuracy: 0.6446\n",
      "Epoch 146/4000\n",
      " - 5s - loss: 0.8597 - accuracy: 0.6453\n",
      "Epoch 147/4000\n",
      " - 5s - loss: 0.8589 - accuracy: 0.6432\n",
      "Epoch 148/4000\n",
      " - 5s - loss: 0.8611 - accuracy: 0.6416\n",
      "Epoch 149/4000\n",
      " - 5s - loss: 0.8596 - accuracy: 0.6432\n",
      "Epoch 150/4000\n",
      " - 5s - loss: 0.8593 - accuracy: 0.6418\n",
      "Epoch 151/4000\n",
      " - 5s - loss: 0.8583 - accuracy: 0.6433\n",
      "Epoch 152/4000\n",
      " - 5s - loss: 0.8584 - accuracy: 0.6436\n",
      "Epoch 153/4000\n",
      " - 5s - loss: 0.8577 - accuracy: 0.6445\n",
      "Epoch 154/4000\n",
      " - 5s - loss: 0.8571 - accuracy: 0.6456\n",
      "Epoch 155/4000\n",
      " - 5s - loss: 0.8558 - accuracy: 0.6438\n",
      "Epoch 156/4000\n",
      " - 5s - loss: 0.8569 - accuracy: 0.6440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 157/4000\n",
      " - 5s - loss: 0.8566 - accuracy: 0.6435\n",
      "Epoch 158/4000\n",
      " - 5s - loss: 0.8568 - accuracy: 0.6436\n",
      "Epoch 159/4000\n",
      " - 5s - loss: 0.8543 - accuracy: 0.6456\n",
      "Epoch 160/4000\n",
      " - 5s - loss: 0.8553 - accuracy: 0.6461\n",
      "Epoch 161/4000\n",
      " - 5s - loss: 0.8557 - accuracy: 0.6461\n",
      "Epoch 162/4000\n",
      " - 5s - loss: 0.8547 - accuracy: 0.6440\n",
      "Epoch 163/4000\n",
      " - 5s - loss: 0.8543 - accuracy: 0.6446\n",
      "Epoch 164/4000\n",
      " - 5s - loss: 0.8545 - accuracy: 0.6428\n",
      "Epoch 165/4000\n",
      " - 5s - loss: 0.8529 - accuracy: 0.6433\n",
      "Epoch 166/4000\n",
      " - 5s - loss: 0.8531 - accuracy: 0.6442\n",
      "Epoch 167/4000\n",
      " - 5s - loss: 0.8535 - accuracy: 0.6417\n",
      "Epoch 168/4000\n",
      " - 5s - loss: 0.8536 - accuracy: 0.6448\n",
      "Epoch 169/4000\n",
      " - 5s - loss: 0.8519 - accuracy: 0.6432\n",
      "Epoch 170/4000\n",
      " - 5s - loss: 0.8508 - accuracy: 0.6460\n",
      "Epoch 171/4000\n",
      " - 5s - loss: 0.8513 - accuracy: 0.6466\n",
      "Epoch 172/4000\n",
      " - 5s - loss: 0.8505 - accuracy: 0.6445\n",
      "Epoch 173/4000\n",
      " - 5s - loss: 0.8519 - accuracy: 0.6454\n",
      "Epoch 174/4000\n",
      " - 5s - loss: 0.8509 - accuracy: 0.6440\n",
      "Epoch 175/4000\n",
      " - 5s - loss: 0.8482 - accuracy: 0.6449\n",
      "Epoch 176/4000\n",
      " - 5s - loss: 0.8495 - accuracy: 0.6449\n",
      "Epoch 177/4000\n",
      " - 5s - loss: 0.8502 - accuracy: 0.6440\n",
      "Epoch 178/4000\n",
      " - 5s - loss: 0.8495 - accuracy: 0.6439\n",
      "Epoch 179/4000\n",
      " - 5s - loss: 0.8486 - accuracy: 0.6450\n",
      "Epoch 180/4000\n",
      " - 5s - loss: 0.8493 - accuracy: 0.6441\n",
      "Epoch 181/4000\n",
      " - 5s - loss: 0.8488 - accuracy: 0.6437\n",
      "Epoch 182/4000\n",
      " - 5s - loss: 0.8480 - accuracy: 0.6466\n",
      "Epoch 183/4000\n",
      " - 5s - loss: 0.8479 - accuracy: 0.6439\n",
      "Epoch 184/4000\n",
      " - 5s - loss: 0.8484 - accuracy: 0.6435\n",
      "Epoch 185/4000\n",
      " - 5s - loss: 0.8472 - accuracy: 0.6455\n",
      "Epoch 186/4000\n",
      " - 5s - loss: 0.8470 - accuracy: 0.6437\n",
      "Epoch 187/4000\n",
      " - 5s - loss: 0.8469 - accuracy: 0.6437\n",
      "Epoch 188/4000\n",
      " - 5s - loss: 0.8469 - accuracy: 0.6475\n",
      "Epoch 189/4000\n",
      " - 5s - loss: 0.8457 - accuracy: 0.6462\n",
      "Epoch 190/4000\n",
      " - 5s - loss: 0.8450 - accuracy: 0.6463\n",
      "Epoch 191/4000\n",
      " - 5s - loss: 0.8462 - accuracy: 0.6450\n",
      "Epoch 192/4000\n",
      " - 5s - loss: 0.8456 - accuracy: 0.6471\n",
      "Epoch 193/4000\n",
      " - 5s - loss: 0.8446 - accuracy: 0.6458\n",
      "Epoch 194/4000\n",
      " - 5s - loss: 0.8446 - accuracy: 0.6430\n",
      "Epoch 195/4000\n",
      " - 5s - loss: 0.8438 - accuracy: 0.6458\n",
      "Epoch 196/4000\n",
      " - 5s - loss: 0.8450 - accuracy: 0.6438\n",
      "Epoch 197/4000\n",
      " - 5s - loss: 0.8447 - accuracy: 0.6444\n",
      "Epoch 198/4000\n",
      " - 5s - loss: 0.8441 - accuracy: 0.6450\n",
      "Epoch 199/4000\n",
      " - 5s - loss: 0.8432 - accuracy: 0.6465\n",
      "Epoch 200/4000\n",
      " - 5s - loss: 0.8430 - accuracy: 0.6439\n",
      "Epoch 201/4000\n",
      " - 5s - loss: 0.8423 - accuracy: 0.6441\n",
      "Epoch 202/4000\n",
      " - 5s - loss: 0.8427 - accuracy: 0.6472\n",
      "Epoch 203/4000\n",
      " - 5s - loss: 0.8419 - accuracy: 0.6459\n",
      "Epoch 204/4000\n",
      " - 5s - loss: 0.8415 - accuracy: 0.6460\n",
      "Epoch 205/4000\n",
      " - 5s - loss: 0.8416 - accuracy: 0.6472\n",
      "Epoch 206/4000\n",
      " - 5s - loss: 0.8416 - accuracy: 0.6451\n",
      "Epoch 207/4000\n",
      " - 5s - loss: 0.8419 - accuracy: 0.6463\n",
      "Epoch 208/4000\n",
      " - 5s - loss: 0.8403 - accuracy: 0.6459\n",
      "Epoch 209/4000\n",
      " - 5s - loss: 0.8407 - accuracy: 0.6463\n",
      "Epoch 210/4000\n",
      " - 5s - loss: 0.8403 - accuracy: 0.6476\n",
      "Epoch 211/4000\n",
      " - 5s - loss: 0.8410 - accuracy: 0.6468\n",
      "Epoch 212/4000\n",
      " - 5s - loss: 0.8391 - accuracy: 0.6461\n",
      "Epoch 213/4000\n",
      " - 5s - loss: 0.8394 - accuracy: 0.6463\n",
      "Epoch 214/4000\n",
      " - 5s - loss: 0.8400 - accuracy: 0.6448\n",
      "Epoch 215/4000\n",
      " - 5s - loss: 0.8394 - accuracy: 0.6461\n",
      "Epoch 216/4000\n",
      " - 5s - loss: 0.8380 - accuracy: 0.6462\n",
      "Epoch 217/4000\n",
      " - 5s - loss: 0.8395 - accuracy: 0.6462\n",
      "Epoch 218/4000\n",
      " - 5s - loss: 0.8387 - accuracy: 0.6449\n",
      "Epoch 219/4000\n",
      " - 5s - loss: 0.8393 - accuracy: 0.6470\n",
      "Epoch 220/4000\n",
      " - 5s - loss: 0.8390 - accuracy: 0.6489\n",
      "Epoch 221/4000\n",
      " - 5s - loss: 0.8378 - accuracy: 0.6473\n",
      "Epoch 222/4000\n",
      " - 5s - loss: 0.8390 - accuracy: 0.6440\n",
      "Epoch 223/4000\n",
      " - 5s - loss: 0.8384 - accuracy: 0.6461\n",
      "Epoch 224/4000\n",
      " - 5s - loss: 0.8380 - accuracy: 0.6453\n",
      "Epoch 225/4000\n",
      " - 5s - loss: 0.8371 - accuracy: 0.6470\n",
      "Epoch 226/4000\n",
      " - 5s - loss: 0.8371 - accuracy: 0.6487\n",
      "Epoch 227/4000\n",
      " - 5s - loss: 0.8374 - accuracy: 0.6461\n",
      "Epoch 228/4000\n",
      " - 5s - loss: 0.8375 - accuracy: 0.6458\n",
      "Epoch 229/4000\n",
      " - 5s - loss: 0.8367 - accuracy: 0.6465\n",
      "Epoch 230/4000\n",
      " - 5s - loss: 0.8360 - accuracy: 0.6475\n",
      "Epoch 231/4000\n",
      " - 5s - loss: 0.8359 - accuracy: 0.6461\n",
      "Epoch 232/4000\n",
      " - 5s - loss: 0.8366 - accuracy: 0.6441\n",
      "Epoch 233/4000\n",
      " - 5s - loss: 0.8350 - accuracy: 0.6471\n",
      "Epoch 234/4000\n",
      " - 5s - loss: 0.8355 - accuracy: 0.6463\n",
      "Epoch 235/4000\n",
      " - 5s - loss: 0.8343 - accuracy: 0.6482\n",
      "Epoch 236/4000\n",
      " - 5s - loss: 0.8351 - accuracy: 0.6470\n",
      "Epoch 237/4000\n",
      " - 5s - loss: 0.8347 - accuracy: 0.6486\n",
      "Epoch 238/4000\n",
      " - 5s - loss: 0.8354 - accuracy: 0.6450\n",
      "Epoch 239/4000\n",
      " - 5s - loss: 0.8341 - accuracy: 0.6470\n",
      "Epoch 240/4000\n",
      " - 5s - loss: 0.8339 - accuracy: 0.6476\n",
      "Epoch 241/4000\n",
      " - 5s - loss: 0.8345 - accuracy: 0.6479\n",
      "Epoch 242/4000\n",
      " - 5s - loss: 0.8342 - accuracy: 0.6469\n",
      "Epoch 243/4000\n",
      " - 5s - loss: 0.8337 - accuracy: 0.6475\n",
      "Epoch 244/4000\n",
      " - 5s - loss: 0.8336 - accuracy: 0.6494\n",
      "Epoch 245/4000\n",
      " - 5s - loss: 0.8337 - accuracy: 0.6450\n",
      "Epoch 246/4000\n",
      " - 5s - loss: 0.8327 - accuracy: 0.6484\n",
      "Epoch 247/4000\n",
      " - 5s - loss: 0.8326 - accuracy: 0.6489\n",
      "Epoch 248/4000\n",
      " - 5s - loss: 0.8333 - accuracy: 0.6469\n",
      "Epoch 249/4000\n",
      " - 5s - loss: 0.8323 - accuracy: 0.6471\n",
      "Epoch 250/4000\n",
      " - 5s - loss: 0.8320 - accuracy: 0.6483\n",
      "Epoch 251/4000\n",
      " - 5s - loss: 0.8329 - accuracy: 0.6471\n",
      "Epoch 252/4000\n",
      " - 5s - loss: 0.8318 - accuracy: 0.6476\n",
      "Epoch 253/4000\n",
      " - 5s - loss: 0.8324 - accuracy: 0.6460\n",
      "Epoch 254/4000\n",
      " - 5s - loss: 0.8314 - accuracy: 0.6475\n",
      "Epoch 255/4000\n",
      " - 5s - loss: 0.8315 - accuracy: 0.6488\n",
      "Epoch 256/4000\n",
      " - 5s - loss: 0.8306 - accuracy: 0.6484\n",
      "Epoch 257/4000\n",
      " - 5s - loss: 0.8303 - accuracy: 0.6515\n",
      "Epoch 258/4000\n",
      " - 5s - loss: 0.8308 - accuracy: 0.6490\n",
      "Epoch 259/4000\n",
      " - 5s - loss: 0.8310 - accuracy: 0.6471\n",
      "Epoch 260/4000\n",
      " - 5s - loss: 0.8304 - accuracy: 0.6495\n",
      "Epoch 261/4000\n",
      " - 5s - loss: 0.8304 - accuracy: 0.6490\n",
      "Epoch 262/4000\n",
      " - 5s - loss: 0.8307 - accuracy: 0.6467\n",
      "Epoch 263/4000\n",
      " - 5s - loss: 0.8298 - accuracy: 0.6474\n",
      "Epoch 264/4000\n",
      " - 5s - loss: 0.8294 - accuracy: 0.6474\n",
      "Epoch 265/4000\n",
      " - 5s - loss: 0.8301 - accuracy: 0.6490\n",
      "Epoch 266/4000\n",
      " - 5s - loss: 0.8289 - accuracy: 0.6476\n",
      "Epoch 267/4000\n",
      " - 5s - loss: 0.8298 - accuracy: 0.6475\n",
      "Epoch 268/4000\n",
      " - 5s - loss: 0.8287 - accuracy: 0.6494\n",
      "Epoch 269/4000\n",
      " - 5s - loss: 0.8293 - accuracy: 0.6490\n",
      "Epoch 270/4000\n",
      " - 5s - loss: 0.8289 - accuracy: 0.6482\n",
      "Epoch 271/4000\n",
      " - 5s - loss: 0.8290 - accuracy: 0.6466\n",
      "Epoch 272/4000\n",
      " - 5s - loss: 0.8279 - accuracy: 0.6491\n",
      "Epoch 273/4000\n",
      " - 5s - loss: 0.8274 - accuracy: 0.6480\n",
      "Epoch 274/4000\n",
      " - 5s - loss: 0.8276 - accuracy: 0.6495\n",
      "Epoch 275/4000\n",
      " - 5s - loss: 0.8282 - accuracy: 0.6472\n",
      "Epoch 276/4000\n",
      " - 5s - loss: 0.8281 - accuracy: 0.6489\n",
      "Epoch 277/4000\n",
      " - 5s - loss: 0.8279 - accuracy: 0.6481\n",
      "Epoch 278/4000\n",
      " - 5s - loss: 0.8276 - accuracy: 0.6519\n",
      "Epoch 279/4000\n",
      " - 5s - loss: 0.8269 - accuracy: 0.6493\n",
      "Epoch 280/4000\n",
      " - 5s - loss: 0.8261 - accuracy: 0.6479\n",
      "Epoch 281/4000\n",
      " - 5s - loss: 0.8265 - accuracy: 0.6501\n",
      "Epoch 282/4000\n",
      " - 5s - loss: 0.8273 - accuracy: 0.6483\n",
      "Epoch 283/4000\n",
      " - 5s - loss: 0.8265 - accuracy: 0.6474\n",
      "Epoch 284/4000\n",
      " - 5s - loss: 0.8268 - accuracy: 0.6485\n",
      "Epoch 285/4000\n",
      " - 5s - loss: 0.8264 - accuracy: 0.6492\n",
      "Epoch 286/4000\n",
      " - 5s - loss: 0.8270 - accuracy: 0.6471\n",
      "Epoch 287/4000\n",
      " - 5s - loss: 0.8262 - accuracy: 0.6499\n",
      "Epoch 288/4000\n",
      " - 5s - loss: 0.8261 - accuracy: 0.6479\n",
      "Epoch 289/4000\n",
      " - 5s - loss: 0.8248 - accuracy: 0.6479\n",
      "Epoch 290/4000\n",
      " - 5s - loss: 0.8245 - accuracy: 0.6483\n",
      "Epoch 291/4000\n",
      " - 5s - loss: 0.8255 - accuracy: 0.6493\n",
      "Epoch 292/4000\n",
      " - 5s - loss: 0.8261 - accuracy: 0.6494\n",
      "Epoch 293/4000\n",
      " - 5s - loss: 0.8248 - accuracy: 0.6492\n",
      "Epoch 294/4000\n",
      " - 5s - loss: 0.8253 - accuracy: 0.6485\n",
      "Epoch 295/4000\n",
      " - 5s - loss: 0.8254 - accuracy: 0.6490\n",
      "Epoch 296/4000\n",
      " - 5s - loss: 0.8252 - accuracy: 0.6487\n",
      "Epoch 297/4000\n",
      " - 5s - loss: 0.8236 - accuracy: 0.6501\n",
      "Epoch 298/4000\n",
      " - 5s - loss: 0.8242 - accuracy: 0.6497\n",
      "Epoch 299/4000\n",
      " - 5s - loss: 0.8248 - accuracy: 0.6505\n",
      "Epoch 300/4000\n",
      " - 5s - loss: 0.8251 - accuracy: 0.6488\n",
      "Epoch 301/4000\n",
      " - 5s - loss: 0.8237 - accuracy: 0.6503\n",
      "Epoch 302/4000\n",
      " - 5s - loss: 0.8243 - accuracy: 0.6481\n",
      "Epoch 303/4000\n",
      " - 5s - loss: 0.8230 - accuracy: 0.6491\n",
      "Epoch 304/4000\n",
      " - 5s - loss: 0.8239 - accuracy: 0.6509\n",
      "Epoch 305/4000\n",
      " - 5s - loss: 0.8240 - accuracy: 0.6485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 306/4000\n",
      " - 5s - loss: 0.8235 - accuracy: 0.6486\n",
      "Epoch 307/4000\n",
      " - 5s - loss: 0.8226 - accuracy: 0.6525\n",
      "Epoch 308/4000\n",
      " - 5s - loss: 0.8233 - accuracy: 0.6485\n",
      "Epoch 309/4000\n",
      " - 5s - loss: 0.8224 - accuracy: 0.6493\n",
      "Epoch 310/4000\n",
      " - 5s - loss: 0.8227 - accuracy: 0.6507\n",
      "Epoch 311/4000\n",
      " - 5s - loss: 0.8218 - accuracy: 0.6509\n",
      "Epoch 312/4000\n",
      " - 5s - loss: 0.8222 - accuracy: 0.6514\n",
      "Epoch 313/4000\n",
      " - 5s - loss: 0.8222 - accuracy: 0.6499\n",
      "Epoch 314/4000\n",
      " - 5s - loss: 0.8222 - accuracy: 0.6486\n",
      "Epoch 315/4000\n",
      " - 5s - loss: 0.8225 - accuracy: 0.6475\n",
      "Epoch 316/4000\n",
      " - 5s - loss: 0.8219 - accuracy: 0.6481\n",
      "Epoch 317/4000\n",
      " - 5s - loss: 0.8223 - accuracy: 0.6515\n",
      "Epoch 318/4000\n",
      " - 5s - loss: 0.8209 - accuracy: 0.6493\n",
      "Epoch 319/4000\n",
      " - 5s - loss: 0.8221 - accuracy: 0.6484\n",
      "Epoch 320/4000\n",
      " - 5s - loss: 0.8217 - accuracy: 0.6489\n",
      "Epoch 321/4000\n",
      " - 5s - loss: 0.8207 - accuracy: 0.6483\n",
      "Epoch 322/4000\n",
      " - 5s - loss: 0.8210 - accuracy: 0.6501\n",
      "Epoch 323/4000\n",
      " - 5s - loss: 0.8200 - accuracy: 0.6501\n",
      "Epoch 324/4000\n",
      " - 5s - loss: 0.8209 - accuracy: 0.6479\n",
      "Epoch 325/4000\n",
      " - 5s - loss: 0.8216 - accuracy: 0.6496\n",
      "Epoch 326/4000\n",
      " - 5s - loss: 0.8202 - accuracy: 0.6518\n",
      "Epoch 327/4000\n",
      " - 5s - loss: 0.8207 - accuracy: 0.6478\n",
      "Epoch 328/4000\n",
      " - 5s - loss: 0.8200 - accuracy: 0.6489\n",
      "Epoch 329/4000\n",
      " - 5s - loss: 0.8209 - accuracy: 0.6480\n",
      "Epoch 330/4000\n",
      " - 5s - loss: 0.8199 - accuracy: 0.6482\n",
      "Epoch 331/4000\n",
      " - 5s - loss: 0.8194 - accuracy: 0.6519\n",
      "Epoch 332/4000\n",
      " - 5s - loss: 0.8200 - accuracy: 0.6479\n",
      "Epoch 333/4000\n",
      " - 5s - loss: 0.8202 - accuracy: 0.6497\n",
      "Epoch 334/4000\n",
      " - 5s - loss: 0.8205 - accuracy: 0.6506\n",
      "Epoch 335/4000\n",
      " - 5s - loss: 0.8202 - accuracy: 0.6503\n",
      "Epoch 336/4000\n",
      " - 5s - loss: 0.8194 - accuracy: 0.6496\n",
      "Epoch 337/4000\n",
      " - 5s - loss: 0.8190 - accuracy: 0.6510\n",
      "Epoch 338/4000\n",
      " - 5s - loss: 0.8196 - accuracy: 0.6465\n",
      "Epoch 339/4000\n",
      " - 5s - loss: 0.8192 - accuracy: 0.6501\n",
      "Epoch 340/4000\n",
      " - 5s - loss: 0.8199 - accuracy: 0.6478\n",
      "Epoch 341/4000\n",
      " - 5s - loss: 0.8190 - accuracy: 0.6523\n",
      "Epoch 342/4000\n",
      " - 5s - loss: 0.8187 - accuracy: 0.6506\n",
      "Epoch 343/4000\n",
      " - 5s - loss: 0.8188 - accuracy: 0.6511\n",
      "Epoch 344/4000\n",
      " - 5s - loss: 0.8187 - accuracy: 0.6503\n",
      "Epoch 345/4000\n",
      " - 5s - loss: 0.8187 - accuracy: 0.6502\n",
      "Epoch 346/4000\n",
      " - 5s - loss: 0.8188 - accuracy: 0.6477\n",
      "Epoch 347/4000\n",
      " - 5s - loss: 0.8170 - accuracy: 0.6516\n",
      "Epoch 348/4000\n",
      " - 5s - loss: 0.8178 - accuracy: 0.6520\n",
      "Epoch 349/4000\n",
      " - 5s - loss: 0.8181 - accuracy: 0.6511\n",
      "Epoch 350/4000\n",
      " - 5s - loss: 0.8180 - accuracy: 0.6505\n",
      "Epoch 351/4000\n",
      " - 5s - loss: 0.8174 - accuracy: 0.6512\n",
      "Epoch 352/4000\n",
      " - 5s - loss: 0.8174 - accuracy: 0.6507\n",
      "Epoch 353/4000\n",
      " - 5s - loss: 0.8169 - accuracy: 0.6523\n",
      "Epoch 354/4000\n",
      " - 5s - loss: 0.8171 - accuracy: 0.6512\n",
      "Epoch 355/4000\n",
      " - 5s - loss: 0.8178 - accuracy: 0.6479\n",
      "Epoch 356/4000\n",
      " - 5s - loss: 0.8171 - accuracy: 0.6505\n",
      "Epoch 357/4000\n",
      " - 5s - loss: 0.8165 - accuracy: 0.6510\n",
      "Epoch 358/4000\n",
      " - 5s - loss: 0.8170 - accuracy: 0.6496\n",
      "Epoch 359/4000\n",
      " - 5s - loss: 0.8170 - accuracy: 0.6519\n",
      "Epoch 360/4000\n",
      " - 5s - loss: 0.8164 - accuracy: 0.6509\n",
      "Epoch 361/4000\n",
      " - 5s - loss: 0.8167 - accuracy: 0.6493\n",
      "Epoch 362/4000\n",
      " - 5s - loss: 0.8168 - accuracy: 0.6490\n",
      "Epoch 363/4000\n",
      " - 5s - loss: 0.8161 - accuracy: 0.6494\n",
      "Epoch 364/4000\n",
      " - 5s - loss: 0.8168 - accuracy: 0.6506\n",
      "Epoch 365/4000\n",
      " - 5s - loss: 0.8161 - accuracy: 0.6505\n",
      "Epoch 366/4000\n",
      " - 5s - loss: 0.8154 - accuracy: 0.6510\n",
      "Epoch 367/4000\n",
      " - 5s - loss: 0.8160 - accuracy: 0.6523\n",
      "Epoch 368/4000\n",
      " - 5s - loss: 0.8166 - accuracy: 0.6505\n",
      "Epoch 369/4000\n",
      " - 5s - loss: 0.8156 - accuracy: 0.6514\n",
      "Epoch 370/4000\n",
      " - 5s - loss: 0.8159 - accuracy: 0.6507\n",
      "Epoch 371/4000\n",
      " - 5s - loss: 0.8160 - accuracy: 0.6502\n",
      "Epoch 372/4000\n",
      " - 5s - loss: 0.8148 - accuracy: 0.6504\n",
      "Epoch 373/4000\n",
      " - 5s - loss: 0.8153 - accuracy: 0.6524\n",
      "Epoch 374/4000\n",
      " - 5s - loss: 0.8157 - accuracy: 0.6497\n",
      "Epoch 375/4000\n",
      " - 5s - loss: 0.8150 - accuracy: 0.6527\n",
      "Epoch 376/4000\n",
      " - 5s - loss: 0.8149 - accuracy: 0.6518\n",
      "Epoch 377/4000\n",
      " - 5s - loss: 0.8145 - accuracy: 0.6521\n",
      "Epoch 378/4000\n",
      " - 5s - loss: 0.8151 - accuracy: 0.6515\n",
      "Epoch 379/4000\n",
      " - 5s - loss: 0.8151 - accuracy: 0.6499\n",
      "Epoch 380/4000\n",
      " - 5s - loss: 0.8141 - accuracy: 0.6508\n",
      "Epoch 381/4000\n",
      " - 5s - loss: 0.8155 - accuracy: 0.6507\n",
      "Epoch 382/4000\n",
      " - 5s - loss: 0.8143 - accuracy: 0.6523\n",
      "Epoch 383/4000\n",
      " - 5s - loss: 0.8142 - accuracy: 0.6508\n",
      "Epoch 384/4000\n",
      " - 5s - loss: 0.8134 - accuracy: 0.6524\n",
      "Epoch 385/4000\n",
      " - 5s - loss: 0.8142 - accuracy: 0.6496\n",
      "Epoch 386/4000\n",
      " - 5s - loss: 0.8142 - accuracy: 0.6518\n",
      "Epoch 387/4000\n",
      " - 5s - loss: 0.8139 - accuracy: 0.6499\n",
      "Epoch 388/4000\n",
      " - 5s - loss: 0.8137 - accuracy: 0.6521\n",
      "Epoch 389/4000\n",
      " - 5s - loss: 0.8146 - accuracy: 0.6510\n",
      "Epoch 390/4000\n",
      " - 5s - loss: 0.8141 - accuracy: 0.6524\n",
      "Epoch 391/4000\n",
      " - 5s - loss: 0.8137 - accuracy: 0.6523\n",
      "Epoch 392/4000\n",
      " - 5s - loss: 0.8135 - accuracy: 0.6499\n",
      "Epoch 393/4000\n",
      " - 5s - loss: 0.8137 - accuracy: 0.6522\n",
      "Epoch 394/4000\n",
      " - 5s - loss: 0.8135 - accuracy: 0.6520\n",
      "Epoch 00394: early stopping\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_18 (LSTM)               (None, 200)               217600    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 71)                14271     \n",
      "=================================================================\n",
      "Total params: 231,871\n",
      "Trainable params: 231,871\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/4000\n",
      " - 6s - loss: 2.7517 - accuracy: 0.2578\n",
      "Epoch 2/4000\n",
      " - 5s - loss: 2.2480 - accuracy: 0.3617\n",
      "Epoch 3/4000\n",
      " - 5s - loss: 2.0915 - accuracy: 0.3971\n",
      "Epoch 4/4000\n",
      " - 5s - loss: 1.9910 - accuracy: 0.4170\n",
      "Epoch 5/4000\n",
      " - 5s - loss: 1.9164 - accuracy: 0.4315\n",
      "Epoch 6/4000\n",
      " - 5s - loss: 1.8494 - accuracy: 0.4467\n",
      "Epoch 7/4000\n",
      " - 5s - loss: 1.7909 - accuracy: 0.4625\n",
      "Epoch 8/4000\n",
      " - 5s - loss: 1.7394 - accuracy: 0.4705\n",
      "Epoch 9/4000\n",
      " - 5s - loss: 1.6910 - accuracy: 0.4856\n",
      "Epoch 10/4000\n",
      " - 5s - loss: 1.6439 - accuracy: 0.4942\n",
      "Epoch 11/4000\n",
      " - 5s - loss: 1.6015 - accuracy: 0.5060\n",
      "Epoch 12/4000\n",
      " - 5s - loss: 1.5621 - accuracy: 0.5146\n",
      "Epoch 13/4000\n",
      " - 5s - loss: 1.5228 - accuracy: 0.5215\n",
      "Epoch 14/4000\n",
      " - 5s - loss: 1.4884 - accuracy: 0.5339\n",
      "Epoch 15/4000\n",
      " - 5s - loss: 1.4525 - accuracy: 0.5404\n",
      "Epoch 16/4000\n",
      " - 5s - loss: 1.4203 - accuracy: 0.5501\n",
      "Epoch 17/4000\n",
      " - 5s - loss: 1.3887 - accuracy: 0.5569\n",
      "Epoch 18/4000\n",
      " - 5s - loss: 1.3611 - accuracy: 0.5636\n",
      "Epoch 19/4000\n",
      " - 5s - loss: 1.3333 - accuracy: 0.5679\n",
      "Epoch 20/4000\n",
      " - 5s - loss: 1.3050 - accuracy: 0.5756\n",
      "Epoch 21/4000\n",
      " - 5s - loss: 1.2823 - accuracy: 0.5809\n",
      "Epoch 22/4000\n",
      " - 5s - loss: 1.2568 - accuracy: 0.5886\n",
      "Epoch 23/4000\n",
      " - 5s - loss: 1.2338 - accuracy: 0.5931\n",
      "Epoch 24/4000\n",
      " - 5s - loss: 1.2131 - accuracy: 0.5971\n",
      "Epoch 25/4000\n",
      " - 5s - loss: 1.1929 - accuracy: 0.6033\n",
      "Epoch 26/4000\n",
      " - 5s - loss: 1.1741 - accuracy: 0.6045\n",
      "Epoch 27/4000\n",
      " - 5s - loss: 1.1566 - accuracy: 0.6099\n",
      "Epoch 28/4000\n",
      " - 5s - loss: 1.1402 - accuracy: 0.6136\n",
      "Epoch 29/4000\n",
      " - 5s - loss: 1.1239 - accuracy: 0.6181\n",
      "Epoch 30/4000\n",
      " - 5s - loss: 1.1095 - accuracy: 0.6177\n",
      "Epoch 31/4000\n",
      " - 5s - loss: 1.0970 - accuracy: 0.6210\n",
      "Epoch 32/4000\n",
      " - 5s - loss: 1.0843 - accuracy: 0.6240\n",
      "Epoch 33/4000\n",
      " - 5s - loss: 1.0737 - accuracy: 0.6243\n",
      "Epoch 34/4000\n",
      " - 5s - loss: 1.0643 - accuracy: 0.6250\n",
      "Epoch 35/4000\n",
      " - 5s - loss: 1.0530 - accuracy: 0.6290\n",
      "Epoch 36/4000\n",
      " - 5s - loss: 1.0437 - accuracy: 0.6288\n",
      "Epoch 37/4000\n",
      " - 5s - loss: 1.0366 - accuracy: 0.6316\n",
      "Epoch 38/4000\n",
      " - 5s - loss: 1.0269 - accuracy: 0.6329\n",
      "Epoch 39/4000\n",
      " - 5s - loss: 1.0187 - accuracy: 0.6344\n",
      "Epoch 40/4000\n",
      " - 5s - loss: 1.0125 - accuracy: 0.6342\n",
      "Epoch 41/4000\n",
      " - 5s - loss: 1.0047 - accuracy: 0.6347\n",
      "Epoch 42/4000\n",
      " - 5s - loss: 0.9981 - accuracy: 0.6375\n",
      "Epoch 43/4000\n",
      " - 5s - loss: 0.9933 - accuracy: 0.6356\n",
      "Epoch 44/4000\n",
      " - 5s - loss: 0.9871 - accuracy: 0.6363\n",
      "Epoch 45/4000\n",
      " - 5s - loss: 0.9832 - accuracy: 0.6372\n",
      "Epoch 46/4000\n",
      " - 5s - loss: 0.9788 - accuracy: 0.6377\n",
      "Epoch 47/4000\n",
      " - 5s - loss: 0.9755 - accuracy: 0.6389\n",
      "Epoch 48/4000\n",
      " - 5s - loss: 0.9693 - accuracy: 0.6374\n",
      "Epoch 49/4000\n",
      " - 5s - loss: 0.9663 - accuracy: 0.6369\n",
      "Epoch 50/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 3s - loss: 0.9599 - accuracy: 0.6381\n",
      "Epoch 51/4000\n",
      " - 5s - loss: 0.9597 - accuracy: 0.6402\n",
      "Epoch 52/4000\n",
      " - 5s - loss: 0.9547 - accuracy: 0.6405\n",
      "Epoch 53/4000\n",
      " - 5s - loss: 0.9520 - accuracy: 0.6388\n",
      "Epoch 54/4000\n",
      " - 5s - loss: 0.9500 - accuracy: 0.6405\n",
      "Epoch 55/4000\n",
      " - 5s - loss: 0.9477 - accuracy: 0.6365\n",
      "Epoch 56/4000\n",
      " - 5s - loss: 0.9433 - accuracy: 0.6391\n",
      "Epoch 57/4000\n",
      " - 5s - loss: 0.9414 - accuracy: 0.6410\n",
      "Epoch 58/4000\n",
      " - 5s - loss: 0.9382 - accuracy: 0.6380\n",
      "Epoch 59/4000\n",
      " - 5s - loss: 0.9376 - accuracy: 0.6383\n",
      "Epoch 60/4000\n",
      " - 5s - loss: 0.9338 - accuracy: 0.6414\n",
      "Epoch 61/4000\n",
      " - 5s - loss: 0.9316 - accuracy: 0.6391\n",
      "Epoch 62/4000\n",
      " - 5s - loss: 0.9309 - accuracy: 0.6388\n",
      "Epoch 63/4000\n",
      " - 5s - loss: 0.9278 - accuracy: 0.6401\n",
      "Epoch 64/4000\n",
      " - 5s - loss: 0.9267 - accuracy: 0.6381\n",
      "Epoch 65/4000\n",
      " - 5s - loss: 0.9234 - accuracy: 0.6384\n",
      "Epoch 66/4000\n",
      " - 5s - loss: 0.9232 - accuracy: 0.6379\n",
      "Epoch 67/4000\n",
      " - 5s - loss: 0.9219 - accuracy: 0.6388\n",
      "Epoch 68/4000\n",
      " - 5s - loss: 0.9199 - accuracy: 0.6386\n",
      "Epoch 69/4000\n",
      " - 5s - loss: 0.9162 - accuracy: 0.6404\n",
      "Epoch 70/4000\n",
      " - 5s - loss: 0.9176 - accuracy: 0.6404\n",
      "Epoch 71/4000\n",
      " - 5s - loss: 0.9152 - accuracy: 0.6393\n",
      "Epoch 72/4000\n",
      " - 5s - loss: 0.9144 - accuracy: 0.6384\n",
      "Epoch 73/4000\n",
      " - 5s - loss: 0.9120 - accuracy: 0.6383\n",
      "Epoch 74/4000\n",
      " - 5s - loss: 0.9111 - accuracy: 0.6384\n",
      "Epoch 75/4000\n",
      " - 5s - loss: 0.9084 - accuracy: 0.6413\n",
      "Epoch 76/4000\n",
      " - 5s - loss: 0.9088 - accuracy: 0.6389\n",
      "Epoch 77/4000\n",
      " - 5s - loss: 0.9076 - accuracy: 0.6384\n",
      "Epoch 78/4000\n",
      " - 5s - loss: 0.9053 - accuracy: 0.6378\n",
      "Epoch 79/4000\n",
      " - 5s - loss: 0.9049 - accuracy: 0.6398\n",
      "Epoch 80/4000\n",
      " - 5s - loss: 0.9040 - accuracy: 0.6402\n",
      "Epoch 81/4000\n",
      " - 5s - loss: 0.9018 - accuracy: 0.6400\n",
      "Epoch 82/4000\n",
      " - 5s - loss: 0.8991 - accuracy: 0.6397\n",
      "Epoch 83/4000\n",
      " - 5s - loss: 0.9024 - accuracy: 0.6392\n",
      "Epoch 84/4000\n",
      " - 5s - loss: 0.8971 - accuracy: 0.6411\n",
      "Epoch 85/4000\n",
      " - 5s - loss: 0.8972 - accuracy: 0.6399\n",
      "Epoch 86/4000\n",
      " - 5s - loss: 0.8969 - accuracy: 0.6376\n",
      "Epoch 87/4000\n",
      " - 5s - loss: 0.8961 - accuracy: 0.6401\n",
      "Epoch 88/4000\n",
      " - 5s - loss: 0.8960 - accuracy: 0.6405\n",
      "Epoch 89/4000\n",
      " - 5s - loss: 0.8941 - accuracy: 0.6378\n",
      "Epoch 90/4000\n",
      " - 5s - loss: 0.8941 - accuracy: 0.6417\n",
      "Epoch 91/4000\n",
      " - 5s - loss: 0.8932 - accuracy: 0.6424\n",
      "Epoch 92/4000\n",
      " - 5s - loss: 0.8911 - accuracy: 0.6426\n",
      "Epoch 93/4000\n",
      " - 5s - loss: 0.8892 - accuracy: 0.6404\n",
      "Epoch 94/4000\n",
      " - 5s - loss: 0.8908 - accuracy: 0.6404\n",
      "Epoch 95/4000\n",
      " - 5s - loss: 0.8903 - accuracy: 0.6415\n",
      "Epoch 96/4000\n",
      " - 5s - loss: 0.8890 - accuracy: 0.6409\n",
      "Epoch 97/4000\n",
      " - 5s - loss: 0.8871 - accuracy: 0.6405\n",
      "Epoch 98/4000\n",
      " - 5s - loss: 0.8865 - accuracy: 0.6420\n",
      "Epoch 99/4000\n",
      " - 5s - loss: 0.8853 - accuracy: 0.6391\n",
      "Epoch 100/4000\n",
      " - 5s - loss: 0.8849 - accuracy: 0.6427\n",
      "Epoch 101/4000\n",
      " - 5s - loss: 0.8846 - accuracy: 0.6416\n",
      "Epoch 102/4000\n",
      " - 5s - loss: 0.8837 - accuracy: 0.6421\n",
      "Epoch 103/4000\n",
      " - 5s - loss: 0.8824 - accuracy: 0.6406\n",
      "Epoch 104/4000\n",
      " - 5s - loss: 0.8826 - accuracy: 0.6433\n",
      "Epoch 105/4000\n",
      " - 5s - loss: 0.8832 - accuracy: 0.6412\n",
      "Epoch 106/4000\n",
      " - 5s - loss: 0.8817 - accuracy: 0.6399\n",
      "Epoch 107/4000\n",
      " - 5s - loss: 0.8794 - accuracy: 0.6417\n",
      "Epoch 108/4000\n",
      " - 5s - loss: 0.8794 - accuracy: 0.6425\n",
      "Epoch 109/4000\n",
      " - 5s - loss: 0.8796 - accuracy: 0.6407\n",
      "Epoch 110/4000\n",
      " - 5s - loss: 0.8784 - accuracy: 0.6435\n",
      "Epoch 111/4000\n",
      " - 5s - loss: 0.8772 - accuracy: 0.6411\n",
      "Epoch 112/4000\n",
      " - 5s - loss: 0.8762 - accuracy: 0.6431\n",
      "Epoch 113/4000\n",
      " - 5s - loss: 0.8777 - accuracy: 0.6412\n",
      "Epoch 114/4000\n",
      " - 5s - loss: 0.8762 - accuracy: 0.6446\n",
      "Epoch 115/4000\n",
      " - 5s - loss: 0.8746 - accuracy: 0.6412\n",
      "Epoch 116/4000\n",
      " - 5s - loss: 0.8753 - accuracy: 0.6418\n",
      "Epoch 117/4000\n",
      " - 5s - loss: 0.8742 - accuracy: 0.6411\n",
      "Epoch 118/4000\n",
      " - 5s - loss: 0.8737 - accuracy: 0.6432\n",
      "Epoch 119/4000\n",
      " - 5s - loss: 0.8728 - accuracy: 0.6408\n",
      "Epoch 120/4000\n",
      " - 5s - loss: 0.8730 - accuracy: 0.6442\n",
      "Epoch 121/4000\n",
      " - 5s - loss: 0.8725 - accuracy: 0.6412\n",
      "Epoch 122/4000\n",
      " - 5s - loss: 0.8719 - accuracy: 0.6414\n",
      "Epoch 123/4000\n",
      " - 5s - loss: 0.8710 - accuracy: 0.6429\n",
      "Epoch 124/4000\n",
      " - 5s - loss: 0.8702 - accuracy: 0.6425\n",
      "Epoch 125/4000\n",
      " - 5s - loss: 0.8692 - accuracy: 0.6437\n",
      "Epoch 126/4000\n",
      " - 5s - loss: 0.8690 - accuracy: 0.6414\n",
      "Epoch 127/4000\n",
      " - 5s - loss: 0.8677 - accuracy: 0.6430\n",
      "Epoch 128/4000\n",
      " - 5s - loss: 0.8686 - accuracy: 0.6396\n",
      "Epoch 129/4000\n",
      " - 5s - loss: 0.8690 - accuracy: 0.6414\n",
      "Epoch 130/4000\n",
      " - 5s - loss: 0.8667 - accuracy: 0.6428\n",
      "Epoch 131/4000\n",
      " - 5s - loss: 0.8681 - accuracy: 0.6405\n",
      "Epoch 132/4000\n",
      " - 5s - loss: 0.8669 - accuracy: 0.6409\n",
      "Epoch 133/4000\n",
      " - 5s - loss: 0.8663 - accuracy: 0.6435\n",
      "Epoch 134/4000\n",
      " - 5s - loss: 0.8657 - accuracy: 0.6425\n",
      "Epoch 135/4000\n",
      " - 5s - loss: 0.8648 - accuracy: 0.6416\n",
      "Epoch 136/4000\n",
      " - 5s - loss: 0.8647 - accuracy: 0.6419\n",
      "Epoch 137/4000\n",
      " - 5s - loss: 0.8643 - accuracy: 0.6463\n",
      "Epoch 138/4000\n",
      " - 5s - loss: 0.8621 - accuracy: 0.6435\n",
      "Epoch 139/4000\n",
      " - 5s - loss: 0.8639 - accuracy: 0.6417\n",
      "Epoch 140/4000\n",
      " - 5s - loss: 0.8624 - accuracy: 0.6434\n",
      "Epoch 141/4000\n",
      " - 5s - loss: 0.8621 - accuracy: 0.6428\n",
      "Epoch 142/4000\n",
      " - 5s - loss: 0.8624 - accuracy: 0.6443\n",
      "Epoch 143/4000\n",
      " - 5s - loss: 0.8597 - accuracy: 0.6450\n",
      "Epoch 144/4000\n",
      " - 5s - loss: 0.8619 - accuracy: 0.6440\n",
      "Epoch 145/4000\n",
      " - 5s - loss: 0.8615 - accuracy: 0.6423\n",
      "Epoch 146/4000\n",
      " - 5s - loss: 0.8591 - accuracy: 0.6443\n",
      "Epoch 147/4000\n",
      " - 5s - loss: 0.8615 - accuracy: 0.6432\n",
      "Epoch 148/4000\n",
      " - 5s - loss: 0.8593 - accuracy: 0.6414\n",
      "Epoch 149/4000\n",
      " - 5s - loss: 0.8590 - accuracy: 0.6439\n",
      "Epoch 150/4000\n",
      " - 5s - loss: 0.8587 - accuracy: 0.6450\n",
      "Epoch 151/4000\n",
      " - 5s - loss: 0.8602 - accuracy: 0.6427\n",
      "Epoch 152/4000\n",
      " - 5s - loss: 0.8583 - accuracy: 0.6447\n",
      "Epoch 153/4000\n",
      " - 5s - loss: 0.8574 - accuracy: 0.6443\n",
      "Epoch 154/4000\n",
      " - 5s - loss: 0.8564 - accuracy: 0.6437\n",
      "Epoch 155/4000\n",
      " - 5s - loss: 0.8565 - accuracy: 0.6448\n",
      "Epoch 156/4000\n",
      " - 5s - loss: 0.8570 - accuracy: 0.6432\n",
      "Epoch 157/4000\n",
      " - 5s - loss: 0.8558 - accuracy: 0.6430\n",
      "Epoch 158/4000\n",
      " - 5s - loss: 0.8555 - accuracy: 0.6452\n",
      "Epoch 159/4000\n",
      " - 5s - loss: 0.8553 - accuracy: 0.6449\n",
      "Epoch 160/4000\n",
      " - 5s - loss: 0.8543 - accuracy: 0.6431\n",
      "Epoch 161/4000\n",
      " - 6s - loss: 0.8553 - accuracy: 0.6452\n",
      "Epoch 162/4000\n",
      " - 5s - loss: 0.8555 - accuracy: 0.6430\n",
      "Epoch 163/4000\n",
      " - 5s - loss: 0.8537 - accuracy: 0.6426\n",
      "Epoch 164/4000\n",
      " - 5s - loss: 0.8537 - accuracy: 0.6429\n",
      "Epoch 165/4000\n",
      " - 5s - loss: 0.8532 - accuracy: 0.6428\n",
      "Epoch 166/4000\n",
      " - 5s - loss: 0.8532 - accuracy: 0.6444\n",
      "Epoch 167/4000\n",
      " - 5s - loss: 0.8537 - accuracy: 0.6440\n",
      "Epoch 168/4000\n",
      " - 5s - loss: 0.8520 - accuracy: 0.6448\n",
      "Epoch 169/4000\n",
      " - 5s - loss: 0.8527 - accuracy: 0.6435\n",
      "Epoch 170/4000\n",
      " - 5s - loss: 0.8524 - accuracy: 0.6437\n",
      "Epoch 171/4000\n",
      " - 5s - loss: 0.8505 - accuracy: 0.6431\n",
      "Epoch 172/4000\n",
      " - 5s - loss: 0.8506 - accuracy: 0.6450\n",
      "Epoch 173/4000\n",
      " - 5s - loss: 0.8514 - accuracy: 0.6441\n",
      "Epoch 174/4000\n",
      " - 5s - loss: 0.8500 - accuracy: 0.6455\n",
      "Epoch 175/4000\n",
      " - 5s - loss: 0.8504 - accuracy: 0.6440\n",
      "Epoch 176/4000\n",
      " - 5s - loss: 0.8490 - accuracy: 0.6438\n",
      "Epoch 177/4000\n",
      " - 5s - loss: 0.8498 - accuracy: 0.6445\n",
      "Epoch 178/4000\n",
      " - 5s - loss: 0.8489 - accuracy: 0.6445\n",
      "Epoch 179/4000\n",
      " - 5s - loss: 0.8489 - accuracy: 0.6436\n",
      "Epoch 180/4000\n",
      " - 5s - loss: 0.8492 - accuracy: 0.6432\n",
      "Epoch 181/4000\n",
      " - 5s - loss: 0.8496 - accuracy: 0.6441\n",
      "Epoch 182/4000\n",
      " - 5s - loss: 0.8476 - accuracy: 0.6451\n",
      "Epoch 183/4000\n",
      " - 5s - loss: 0.8476 - accuracy: 0.6465\n",
      "Epoch 184/4000\n",
      " - 5s - loss: 0.8466 - accuracy: 0.6473\n",
      "Epoch 185/4000\n",
      " - 5s - loss: 0.8467 - accuracy: 0.6454\n",
      "Epoch 186/4000\n",
      " - 5s - loss: 0.8472 - accuracy: 0.6458\n",
      "Epoch 187/4000\n",
      " - 5s - loss: 0.8463 - accuracy: 0.6439\n",
      "Epoch 188/4000\n",
      " - 5s - loss: 0.8470 - accuracy: 0.6440\n",
      "Epoch 189/4000\n",
      " - 5s - loss: 0.8460 - accuracy: 0.6454\n",
      "Epoch 190/4000\n",
      " - 5s - loss: 0.8453 - accuracy: 0.6449\n",
      "Epoch 191/4000\n",
      " - 5s - loss: 0.8448 - accuracy: 0.6456\n",
      "Epoch 192/4000\n",
      " - 5s - loss: 0.8457 - accuracy: 0.6453\n",
      "Epoch 193/4000\n",
      " - 5s - loss: 0.8450 - accuracy: 0.6462\n",
      "Epoch 194/4000\n",
      " - 5s - loss: 0.8452 - accuracy: 0.6454\n",
      "Epoch 195/4000\n",
      " - 5s - loss: 0.8431 - accuracy: 0.6453\n",
      "Epoch 196/4000\n",
      " - 5s - loss: 0.8445 - accuracy: 0.6474\n",
      "Epoch 197/4000\n",
      " - 5s - loss: 0.8433 - accuracy: 0.6439\n",
      "Epoch 198/4000\n",
      " - 5s - loss: 0.8436 - accuracy: 0.6422\n",
      "Epoch 199/4000\n",
      " - 5s - loss: 0.8429 - accuracy: 0.6473\n",
      "Epoch 200/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 0.8424 - accuracy: 0.6459\n",
      "Epoch 201/4000\n",
      " - 5s - loss: 0.8426 - accuracy: 0.6445\n",
      "Epoch 202/4000\n",
      " - 5s - loss: 0.8428 - accuracy: 0.6453\n",
      "Epoch 203/4000\n",
      " - 5s - loss: 0.8410 - accuracy: 0.6461\n",
      "Epoch 204/4000\n",
      " - 5s - loss: 0.8419 - accuracy: 0.6458\n",
      "Epoch 205/4000\n",
      " - 5s - loss: 0.8416 - accuracy: 0.6458\n",
      "Epoch 206/4000\n",
      " - 5s - loss: 0.8421 - accuracy: 0.6466\n",
      "Epoch 207/4000\n",
      " - 5s - loss: 0.8429 - accuracy: 0.6462\n",
      "Epoch 208/4000\n",
      " - 5s - loss: 0.8402 - accuracy: 0.6474\n",
      "Epoch 209/4000\n",
      " - 5s - loss: 0.8416 - accuracy: 0.6463\n",
      "Epoch 210/4000\n",
      " - 5s - loss: 0.8404 - accuracy: 0.6463\n",
      "Epoch 211/4000\n",
      " - 5s - loss: 0.8391 - accuracy: 0.6474\n",
      "Epoch 212/4000\n",
      " - 5s - loss: 0.8394 - accuracy: 0.6468\n",
      "Epoch 213/4000\n",
      " - 5s - loss: 0.8404 - accuracy: 0.6468\n",
      "Epoch 214/4000\n",
      " - 5s - loss: 0.8397 - accuracy: 0.6448\n",
      "Epoch 215/4000\n",
      " - 5s - loss: 0.8389 - accuracy: 0.6479\n",
      "Epoch 216/4000\n",
      " - 5s - loss: 0.8383 - accuracy: 0.6462\n",
      "Epoch 217/4000\n",
      " - 5s - loss: 0.8388 - accuracy: 0.6459\n",
      "Epoch 218/4000\n",
      " - 5s - loss: 0.8394 - accuracy: 0.6471\n",
      "Epoch 219/4000\n",
      " - 5s - loss: 0.8371 - accuracy: 0.6472\n",
      "Epoch 220/4000\n",
      " - 5s - loss: 0.8382 - accuracy: 0.6461\n",
      "Epoch 221/4000\n",
      " - 5s - loss: 0.8382 - accuracy: 0.6455\n",
      "Epoch 222/4000\n",
      " - 5s - loss: 0.8383 - accuracy: 0.6467\n",
      "Epoch 223/4000\n",
      " - 5s - loss: 0.8382 - accuracy: 0.6469\n",
      "Epoch 224/4000\n",
      " - 5s - loss: 0.8374 - accuracy: 0.6450\n",
      "Epoch 225/4000\n",
      " - 5s - loss: 0.8370 - accuracy: 0.6481\n",
      "Epoch 226/4000\n",
      " - 5s - loss: 0.8377 - accuracy: 0.6474\n",
      "Epoch 227/4000\n",
      " - 5s - loss: 0.8371 - accuracy: 0.6457\n",
      "Epoch 228/4000\n",
      " - 5s - loss: 0.8360 - accuracy: 0.6474\n",
      "Epoch 229/4000\n",
      " - 5s - loss: 0.8358 - accuracy: 0.6468\n",
      "Epoch 230/4000\n",
      " - 5s - loss: 0.8353 - accuracy: 0.6478\n",
      "Epoch 231/4000\n",
      " - 5s - loss: 0.8364 - accuracy: 0.6483\n",
      "Epoch 232/4000\n",
      " - 5s - loss: 0.8340 - accuracy: 0.6507\n",
      "Epoch 233/4000\n",
      " - 5s - loss: 0.8362 - accuracy: 0.6471\n",
      "Epoch 234/4000\n",
      " - 5s - loss: 0.8348 - accuracy: 0.6474\n",
      "Epoch 235/4000\n",
      " - 5s - loss: 0.8353 - accuracy: 0.6466\n",
      "Epoch 236/4000\n",
      " - 5s - loss: 0.8341 - accuracy: 0.6483\n",
      "Epoch 237/4000\n",
      " - 5s - loss: 0.8348 - accuracy: 0.6463\n",
      "Epoch 238/4000\n",
      " - 5s - loss: 0.8354 - accuracy: 0.6479\n",
      "Epoch 239/4000\n",
      " - 5s - loss: 0.8341 - accuracy: 0.6471\n",
      "Epoch 240/4000\n",
      " - 5s - loss: 0.8333 - accuracy: 0.6476\n",
      "Epoch 241/4000\n",
      " - 5s - loss: 0.8342 - accuracy: 0.6476\n",
      "Epoch 242/4000\n",
      " - 5s - loss: 0.8330 - accuracy: 0.6485\n",
      "Epoch 243/4000\n",
      " - 5s - loss: 0.8323 - accuracy: 0.6473\n",
      "Epoch 244/4000\n",
      " - 5s - loss: 0.8339 - accuracy: 0.6466\n",
      "Epoch 245/4000\n",
      " - 5s - loss: 0.8325 - accuracy: 0.6473\n",
      "Epoch 246/4000\n",
      " - 5s - loss: 0.8329 - accuracy: 0.6462\n",
      "Epoch 247/4000\n",
      " - 5s - loss: 0.8321 - accuracy: 0.6482\n",
      "Epoch 248/4000\n",
      " - 5s - loss: 0.8320 - accuracy: 0.6467\n",
      "Epoch 249/4000\n",
      " - 5s - loss: 0.8323 - accuracy: 0.6479\n",
      "Epoch 250/4000\n",
      " - 5s - loss: 0.8316 - accuracy: 0.6470\n",
      "Epoch 251/4000\n",
      " - 5s - loss: 0.8325 - accuracy: 0.6466\n",
      "Epoch 252/4000\n",
      " - 5s - loss: 0.8312 - accuracy: 0.6479\n",
      "Epoch 253/4000\n",
      " - 5s - loss: 0.8310 - accuracy: 0.6478\n",
      "Epoch 254/4000\n",
      " - 5s - loss: 0.8318 - accuracy: 0.6468\n",
      "Epoch 255/4000\n",
      " - 5s - loss: 0.8309 - accuracy: 0.6492\n",
      "Epoch 256/4000\n",
      " - 5s - loss: 0.8300 - accuracy: 0.6494\n",
      "Epoch 257/4000\n",
      " - 5s - loss: 0.8310 - accuracy: 0.6495\n",
      "Epoch 258/4000\n",
      " - 5s - loss: 0.8305 - accuracy: 0.6465\n",
      "Epoch 259/4000\n",
      " - 5s - loss: 0.8305 - accuracy: 0.6481\n",
      "Epoch 260/4000\n",
      " - 5s - loss: 0.8303 - accuracy: 0.6478\n",
      "Epoch 261/4000\n",
      " - 5s - loss: 0.8304 - accuracy: 0.6461\n",
      "Epoch 262/4000\n",
      " - 5s - loss: 0.8302 - accuracy: 0.6475\n",
      "Epoch 263/4000\n",
      " - 5s - loss: 0.8298 - accuracy: 0.6473\n",
      "Epoch 264/4000\n",
      " - 5s - loss: 0.8303 - accuracy: 0.6458\n",
      "Epoch 265/4000\n",
      " - 5s - loss: 0.8296 - accuracy: 0.6495\n",
      "Epoch 266/4000\n",
      " - 5s - loss: 0.8283 - accuracy: 0.6482\n",
      "Epoch 267/4000\n",
      " - 5s - loss: 0.8291 - accuracy: 0.6484\n",
      "Epoch 268/4000\n",
      " - 5s - loss: 0.8284 - accuracy: 0.6484\n",
      "Epoch 269/4000\n",
      " - 5s - loss: 0.8287 - accuracy: 0.6498\n",
      "Epoch 270/4000\n",
      " - 5s - loss: 0.8286 - accuracy: 0.6484\n",
      "Epoch 271/4000\n",
      " - 5s - loss: 0.8283 - accuracy: 0.6492\n",
      "Epoch 272/4000\n",
      " - 5s - loss: 0.8285 - accuracy: 0.6479\n",
      "Epoch 273/4000\n",
      " - 5s - loss: 0.8279 - accuracy: 0.6475\n",
      "Epoch 274/4000\n",
      " - 5s - loss: 0.8283 - accuracy: 0.6495\n",
      "Epoch 275/4000\n",
      " - 5s - loss: 0.8280 - accuracy: 0.6468\n",
      "Epoch 276/4000\n",
      " - 5s - loss: 0.8273 - accuracy: 0.6481\n",
      "Epoch 277/4000\n",
      " - 5s - loss: 0.8265 - accuracy: 0.6497\n",
      "Epoch 278/4000\n",
      " - 5s - loss: 0.8275 - accuracy: 0.6477\n",
      "Epoch 279/4000\n",
      " - 5s - loss: 0.8275 - accuracy: 0.6487\n",
      "Epoch 280/4000\n",
      " - 5s - loss: 0.8263 - accuracy: 0.6500\n",
      "Epoch 281/4000\n",
      " - 5s - loss: 0.8271 - accuracy: 0.6488\n",
      "Epoch 282/4000\n",
      " - 5s - loss: 0.8267 - accuracy: 0.6489\n",
      "Epoch 283/4000\n",
      " - 5s - loss: 0.8260 - accuracy: 0.6480\n",
      "Epoch 284/4000\n",
      " - 5s - loss: 0.8271 - accuracy: 0.6474\n",
      "Epoch 285/4000\n",
      " - 5s - loss: 0.8258 - accuracy: 0.6493\n",
      "Epoch 286/4000\n",
      " - 5s - loss: 0.8253 - accuracy: 0.6476\n",
      "Epoch 287/4000\n",
      " - 5s - loss: 0.8264 - accuracy: 0.6479\n",
      "Epoch 288/4000\n",
      " - 5s - loss: 0.8263 - accuracy: 0.6480\n",
      "Epoch 289/4000\n",
      " - 5s - loss: 0.8256 - accuracy: 0.6476\n",
      "Epoch 290/4000\n",
      " - 5s - loss: 0.8262 - accuracy: 0.6492\n",
      "Epoch 291/4000\n",
      " - 5s - loss: 0.8252 - accuracy: 0.6497\n",
      "Epoch 292/4000\n",
      " - 5s - loss: 0.8238 - accuracy: 0.6497\n",
      "Epoch 293/4000\n",
      " - 5s - loss: 0.8257 - accuracy: 0.6471\n",
      "Epoch 294/4000\n",
      " - 5s - loss: 0.8253 - accuracy: 0.6478\n",
      "Epoch 295/4000\n",
      " - 5s - loss: 0.8255 - accuracy: 0.6481\n",
      "Epoch 296/4000\n",
      " - 5s - loss: 0.8236 - accuracy: 0.6489\n",
      "Epoch 297/4000\n",
      " - 5s - loss: 0.8249 - accuracy: 0.6490\n",
      "Epoch 298/4000\n",
      " - 5s - loss: 0.8232 - accuracy: 0.6509\n",
      "Epoch 299/4000\n",
      " - 5s - loss: 0.8232 - accuracy: 0.6494\n",
      "Epoch 300/4000\n",
      " - 5s - loss: 0.8239 - accuracy: 0.6499\n",
      "Epoch 301/4000\n",
      " - 5s - loss: 0.8239 - accuracy: 0.6471\n",
      "Epoch 302/4000\n",
      " - 5s - loss: 0.8241 - accuracy: 0.6500\n",
      "Epoch 303/4000\n",
      " - 5s - loss: 0.8237 - accuracy: 0.6487\n",
      "Epoch 304/4000\n",
      " - 5s - loss: 0.8231 - accuracy: 0.6502\n",
      "Epoch 305/4000\n",
      " - 5s - loss: 0.8234 - accuracy: 0.6489\n",
      "Epoch 306/4000\n",
      " - 5s - loss: 0.8238 - accuracy: 0.6475\n",
      "Epoch 307/4000\n",
      " - 5s - loss: 0.8228 - accuracy: 0.6520\n",
      "Epoch 308/4000\n",
      " - 5s - loss: 0.8229 - accuracy: 0.6468\n",
      "Epoch 309/4000\n",
      " - 5s - loss: 0.8226 - accuracy: 0.6487\n",
      "Epoch 310/4000\n",
      " - 5s - loss: 0.8227 - accuracy: 0.6481\n",
      "Epoch 311/4000\n",
      " - 5s - loss: 0.8228 - accuracy: 0.6484\n",
      "Epoch 312/4000\n",
      " - 5s - loss: 0.8228 - accuracy: 0.6496\n",
      "Epoch 313/4000\n",
      " - 5s - loss: 0.8225 - accuracy: 0.6476\n",
      "Epoch 314/4000\n",
      " - 5s - loss: 0.8218 - accuracy: 0.6500\n",
      "Epoch 315/4000\n",
      " - 5s - loss: 0.8219 - accuracy: 0.6484\n",
      "Epoch 316/4000\n",
      " - 5s - loss: 0.8219 - accuracy: 0.6499\n",
      "Epoch 317/4000\n",
      " - 5s - loss: 0.8214 - accuracy: 0.6495\n",
      "Epoch 318/4000\n",
      " - 5s - loss: 0.8220 - accuracy: 0.6489\n",
      "Epoch 319/4000\n",
      " - 5s - loss: 0.8213 - accuracy: 0.6500\n",
      "Epoch 320/4000\n",
      " - 5s - loss: 0.8216 - accuracy: 0.6512\n",
      "Epoch 321/4000\n",
      " - 5s - loss: 0.8215 - accuracy: 0.6502\n",
      "Epoch 322/4000\n",
      " - 5s - loss: 0.8210 - accuracy: 0.6496\n",
      "Epoch 323/4000\n",
      " - 5s - loss: 0.8208 - accuracy: 0.6489\n",
      "Epoch 324/4000\n",
      " - 5s - loss: 0.8207 - accuracy: 0.6498\n",
      "Epoch 325/4000\n",
      " - 5s - loss: 0.8207 - accuracy: 0.6512\n",
      "Epoch 326/4000\n",
      " - 5s - loss: 0.8203 - accuracy: 0.6489\n",
      "Epoch 327/4000\n",
      " - 5s - loss: 0.8200 - accuracy: 0.6476\n",
      "Epoch 328/4000\n",
      " - 5s - loss: 0.8203 - accuracy: 0.6478\n",
      "Epoch 329/4000\n",
      " - 5s - loss: 0.8206 - accuracy: 0.6487\n",
      "Epoch 330/4000\n",
      " - 6s - loss: 0.8203 - accuracy: 0.6501\n",
      "Epoch 331/4000\n",
      " - 5s - loss: 0.8201 - accuracy: 0.6489\n",
      "Epoch 332/4000\n",
      " - 5s - loss: 0.8209 - accuracy: 0.6503\n",
      "Epoch 333/4000\n",
      " - 5s - loss: 0.8195 - accuracy: 0.6491\n",
      "Epoch 334/4000\n",
      " - 5s - loss: 0.8199 - accuracy: 0.6493\n",
      "Epoch 335/4000\n",
      " - 5s - loss: 0.8193 - accuracy: 0.6501\n",
      "Epoch 336/4000\n",
      " - 5s - loss: 0.8194 - accuracy: 0.6502\n",
      "Epoch 337/4000\n",
      " - 5s - loss: 0.8195 - accuracy: 0.6487\n",
      "Epoch 338/4000\n",
      " - 5s - loss: 0.8193 - accuracy: 0.6502\n",
      "Epoch 339/4000\n",
      " - 5s - loss: 0.8180 - accuracy: 0.6513\n",
      "Epoch 340/4000\n",
      " - 5s - loss: 0.8188 - accuracy: 0.6518\n",
      "Epoch 341/4000\n",
      " - 5s - loss: 0.8193 - accuracy: 0.6501\n",
      "Epoch 342/4000\n",
      " - 5s - loss: 0.8179 - accuracy: 0.6498\n",
      "Epoch 343/4000\n",
      " - 5s - loss: 0.8178 - accuracy: 0.6521\n",
      "Epoch 344/4000\n",
      " - 5s - loss: 0.8183 - accuracy: 0.6506\n",
      "Epoch 345/4000\n",
      " - 5s - loss: 0.8193 - accuracy: 0.6487\n",
      "Epoch 346/4000\n",
      " - 5s - loss: 0.8177 - accuracy: 0.6487\n",
      "Epoch 347/4000\n",
      " - 5s - loss: 0.8177 - accuracy: 0.6516\n",
      "Epoch 348/4000\n",
      " - 5s - loss: 0.8179 - accuracy: 0.6486\n",
      "Epoch 349/4000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 5s - loss: 0.8178 - accuracy: 0.6528\n",
      "Epoch 350/4000\n",
      " - 5s - loss: 0.8179 - accuracy: 0.6488\n",
      "Epoch 351/4000\n",
      " - 5s - loss: 0.8179 - accuracy: 0.6502\n",
      "Epoch 352/4000\n",
      " - 5s - loss: 0.8166 - accuracy: 0.6508\n",
      "Epoch 353/4000\n",
      " - 5s - loss: 0.8180 - accuracy: 0.6495\n",
      "Epoch 354/4000\n",
      " - 5s - loss: 0.8178 - accuracy: 0.6495\n",
      "Epoch 355/4000\n",
      " - 5s - loss: 0.8173 - accuracy: 0.6510\n",
      "Epoch 356/4000\n",
      " - 5s - loss: 0.8162 - accuracy: 0.6492\n",
      "Epoch 357/4000\n",
      " - 5s - loss: 0.8170 - accuracy: 0.6507\n",
      "Epoch 358/4000\n",
      " - 5s - loss: 0.8171 - accuracy: 0.6508\n",
      "Epoch 359/4000\n",
      " - 5s - loss: 0.8165 - accuracy: 0.6509\n",
      "Epoch 360/4000\n",
      " - 5s - loss: 0.8168 - accuracy: 0.6524\n",
      "Epoch 361/4000\n",
      " - 5s - loss: 0.8164 - accuracy: 0.6523\n",
      "Epoch 362/4000\n",
      " - 5s - loss: 0.8162 - accuracy: 0.6494\n",
      "Epoch 363/4000\n",
      " - 5s - loss: 0.8161 - accuracy: 0.6499\n",
      "Epoch 364/4000\n",
      " - 5s - loss: 0.8161 - accuracy: 0.6525\n",
      "Epoch 365/4000\n",
      " - 5s - loss: 0.8154 - accuracy: 0.6521\n",
      "Epoch 366/4000\n",
      " - 5s - loss: 0.8167 - accuracy: 0.6494\n",
      "Epoch 367/4000\n",
      " - 5s - loss: 0.8160 - accuracy: 0.6510\n",
      "Epoch 368/4000\n",
      " - 5s - loss: 0.8154 - accuracy: 0.6516\n",
      "Epoch 369/4000\n",
      " - 5s - loss: 0.8149 - accuracy: 0.6498\n",
      "Epoch 370/4000\n",
      " - 5s - loss: 0.8166 - accuracy: 0.6496\n",
      "Epoch 371/4000\n",
      " - 5s - loss: 0.8152 - accuracy: 0.6498\n",
      "Epoch 372/4000\n",
      " - 5s - loss: 0.8150 - accuracy: 0.6505\n",
      "Epoch 373/4000\n",
      " - 5s - loss: 0.8152 - accuracy: 0.6518\n",
      "Epoch 374/4000\n",
      " - 5s - loss: 0.8157 - accuracy: 0.6509\n",
      "Epoch 375/4000\n",
      " - 5s - loss: 0.8148 - accuracy: 0.6536\n",
      "Epoch 376/4000\n",
      " - 5s - loss: 0.8146 - accuracy: 0.6501\n",
      "Epoch 377/4000\n",
      " - 5s - loss: 0.8148 - accuracy: 0.6520\n",
      "Epoch 378/4000\n",
      " - 5s - loss: 0.8151 - accuracy: 0.6507\n",
      "Epoch 379/4000\n",
      " - 5s - loss: 0.8145 - accuracy: 0.6498\n",
      "Epoch 380/4000\n",
      " - 5s - loss: 0.8142 - accuracy: 0.6494\n",
      "Epoch 381/4000\n",
      " - 5s - loss: 0.8144 - accuracy: 0.6496\n",
      "Epoch 382/4000\n",
      " - 5s - loss: 0.8141 - accuracy: 0.6527\n",
      "Epoch 383/4000\n",
      " - 5s - loss: 0.8138 - accuracy: 0.6511\n",
      "Epoch 384/4000\n",
      " - 5s - loss: 0.8138 - accuracy: 0.6536\n",
      "Epoch 385/4000\n",
      " - 5s - loss: 0.8137 - accuracy: 0.6527\n",
      "Epoch 386/4000\n",
      " - 5s - loss: 0.8140 - accuracy: 0.6508\n",
      "Epoch 387/4000\n",
      " - 5s - loss: 0.8133 - accuracy: 0.6520\n",
      "Epoch 388/4000\n",
      " - 5s - loss: 0.8130 - accuracy: 0.6506\n",
      "Epoch 389/4000\n",
      " - 5s - loss: 0.8138 - accuracy: 0.6519\n",
      "Epoch 390/4000\n",
      " - 5s - loss: 0.8132 - accuracy: 0.6513\n",
      "Epoch 391/4000\n",
      " - 5s - loss: 0.8131 - accuracy: 0.6509\n",
      "Epoch 392/4000\n",
      " - 5s - loss: 0.8131 - accuracy: 0.6502\n",
      "Epoch 393/4000\n",
      " - 5s - loss: 0.8129 - accuracy: 0.6534\n",
      "Epoch 394/4000\n",
      " - 5s - loss: 0.8139 - accuracy: 0.6496\n",
      "Epoch 395/4000\n",
      " - 5s - loss: 0.8132 - accuracy: 0.6516\n",
      "Epoch 396/4000\n",
      " - 5s - loss: 0.8126 - accuracy: 0.6523\n",
      "Epoch 397/4000\n",
      " - 5s - loss: 0.8122 - accuracy: 0.6533\n",
      "Epoch 398/4000\n",
      " - 5s - loss: 0.8132 - accuracy: 0.6512\n",
      "Epoch 399/4000\n",
      " - 5s - loss: 0.8125 - accuracy: 0.6511\n",
      "Epoch 400/4000\n",
      " - 5s - loss: 0.8127 - accuracy: 0.6494\n",
      "Epoch 401/4000\n",
      " - 5s - loss: 0.8117 - accuracy: 0.6533\n",
      "Epoch 402/4000\n",
      " - 5s - loss: 0.8121 - accuracy: 0.6510\n",
      "Epoch 403/4000\n",
      " - 5s - loss: 0.8127 - accuracy: 0.6512\n",
      "Epoch 404/4000\n",
      " - 5s - loss: 0.8113 - accuracy: 0.6536\n",
      "Epoch 405/4000\n",
      " - 5s - loss: 0.8125 - accuracy: 0.6504\n",
      "Epoch 406/4000\n",
      " - 5s - loss: 0.8125 - accuracy: 0.6532\n",
      "Epoch 407/4000\n",
      " - 5s - loss: 0.8119 - accuracy: 0.6510\n",
      "Epoch 408/4000\n",
      " - 5s - loss: 0.8121 - accuracy: 0.6509\n",
      "Epoch 409/4000\n",
      " - 5s - loss: 0.8112 - accuracy: 0.6511\n",
      "Epoch 410/4000\n",
      " - 5s - loss: 0.8118 - accuracy: 0.6510\n",
      "Epoch 411/4000\n",
      " - 5s - loss: 0.8115 - accuracy: 0.6511\n",
      "Epoch 412/4000\n",
      " - 5s - loss: 0.8115 - accuracy: 0.6507\n",
      "Epoch 413/4000\n",
      " - 5s - loss: 0.8110 - accuracy: 0.6533\n",
      "Epoch 414/4000\n",
      " - 5s - loss: 0.8101 - accuracy: 0.6527\n",
      "Epoch 415/4000\n",
      " - 5s - loss: 0.8113 - accuracy: 0.6535\n",
      "Epoch 416/4000\n",
      " - 5s - loss: 0.8109 - accuracy: 0.6533\n",
      "Epoch 417/4000\n",
      " - 5s - loss: 0.8111 - accuracy: 0.6512\n",
      "Epoch 418/4000\n",
      " - 5s - loss: 0.8101 - accuracy: 0.6519\n",
      "Epoch 419/4000\n",
      " - 5s - loss: 0.8111 - accuracy: 0.6543\n",
      "Epoch 420/4000\n",
      " - 5s - loss: 0.8105 - accuracy: 0.6510\n",
      "Epoch 421/4000\n",
      " - 5s - loss: 0.8103 - accuracy: 0.6515\n",
      "Epoch 422/4000\n",
      " - 5s - loss: 0.8109 - accuracy: 0.6526\n",
      "Epoch 423/4000\n",
      " - 5s - loss: 0.8109 - accuracy: 0.6489\n",
      "Epoch 424/4000\n",
      " - 5s - loss: 0.8109 - accuracy: 0.6507\n",
      "Epoch 425/4000\n",
      " - 5s - loss: 0.8106 - accuracy: 0.6501\n",
      "Epoch 426/4000\n",
      " - 5s - loss: 0.8105 - accuracy: 0.6507\n",
      "Epoch 427/4000\n",
      " - 5s - loss: 0.8095 - accuracy: 0.6528\n",
      "Epoch 428/4000\n",
      " - 5s - loss: 0.8098 - accuracy: 0.6508\n",
      "Epoch 429/4000\n",
      " - 5s - loss: 0.8103 - accuracy: 0.6520\n",
      "Epoch 430/4000\n",
      " - 5s - loss: 0.8099 - accuracy: 0.6515\n",
      "Epoch 431/4000\n",
      " - 5s - loss: 0.8101 - accuracy: 0.6533\n",
      "Epoch 432/4000\n",
      " - 5s - loss: 0.8094 - accuracy: 0.6527\n",
      "Epoch 433/4000\n",
      " - 5s - loss: 0.8100 - accuracy: 0.6537\n",
      "Epoch 434/4000\n",
      " - 5s - loss: 0.8095 - accuracy: 0.6536\n",
      "Epoch 435/4000\n",
      " - 5s - loss: 0.8082 - accuracy: 0.6528\n",
      "Epoch 436/4000\n",
      " - 5s - loss: 0.8085 - accuracy: 0.6522\n",
      "Epoch 437/4000\n",
      " - 5s - loss: 0.8089 - accuracy: 0.6520\n",
      "Epoch 438/4000\n",
      " - 5s - loss: 0.8093 - accuracy: 0.6502\n",
      "Epoch 439/4000\n",
      " - 5s - loss: 0.8094 - accuracy: 0.6525\n",
      "Epoch 440/4000\n",
      " - 5s - loss: 0.8091 - accuracy: 0.6521\n",
      "Epoch 441/4000\n",
      " - 5s - loss: 0.8086 - accuracy: 0.6539\n",
      "Epoch 442/4000\n",
      " - 5s - loss: 0.8093 - accuracy: 0.6517\n",
      "Epoch 443/4000\n",
      " - 5s - loss: 0.8083 - accuracy: 0.6528\n",
      "Epoch 444/4000\n",
      " - 5s - loss: 0.8091 - accuracy: 0.6515\n",
      "Epoch 445/4000\n",
      " - 5s - loss: 0.8088 - accuracy: 0.6498\n",
      "Epoch 00445: early stopping\n"
     ]
    }
   ],
   "source": [
    "# training with char - with different patience value for early stopping, and step size for training seq generation\n",
    "from LSTMforSonnet import LSTM_char\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "numEpoch = 4000\n",
    "stepList = [4]\n",
    "patienceList = [5, 10, 15]\n",
    "for step in stepList:\n",
    "    for patience in patienceList:\n",
    "        test = LSTM_char(step)\n",
    "        test.SonnetLoader('shakespeare')\n",
    "        test.getTrainSeq()\n",
    "        test.getMapping()\n",
    "        test.Train(numEpoch = numEpoch, patience = patience, fileName = \"model_step%d_patience%d_maxEpoch%d\" % (step, patience, numEpoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
